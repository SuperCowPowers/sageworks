{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to SageWorks","text":"<p>The SageWorks framework makes AWS\u00ae both easier to use and more powerful. SageWorks handles all the details around updating and managing a complex set of AWS Services. With a simple-to-use Python API and a beautiful set of web interfaces, SageWorks makes creating AWS ML pipelines a snap. It also dramatically improves both the usability and visibility across the entire spectrum of services: Glue Jobs, Athena, Feature Store, Models, and Endpoints. SageWorks makes it easy to build production ready, AWS powered, machine learning pipelines.</p> SageWorks Dashboard: AWS Pipelines in a Whole New Light!"},{"location":"#full-aws-overview","title":"Full AWS OverView","text":"<ul> <li>Health Monitoring \ud83d\udfe2</li> <li>Dynamic Updates</li> <li>High Level Summary</li> </ul>"},{"location":"#drill-down-views","title":"Drill-Down Views","text":"<ul> <li>Glue Jobs</li> <li>DataSources</li> <li>FeatureSets</li> <li>Models</li> <li>Endpoints</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Setting up SageWorks on your AWS Account: AWS Setup</li> <li>Using SageWorks for ML Pipelines: SageWorks API Classes</li> </ul>"},{"location":"#additional-resources","title":"Additional Resources","text":"<ul> <li>SageWorks Core Classes: Core Classes</li> <li>Consulting Available: SuperCowPowers LLC</li> </ul>"},{"location":"admin/docker_push/","title":"Docker Build and Push","text":"<p>Notes and information on how to do the Docker Builds and Push to AWS ECR.</p> <p>The following instructions should work, but things change :)</p>"},{"location":"admin/docker_push/#update-sageworks-version","title":"Update SageWorks Version","text":"<pre><code>cd applications/aws_dashboard\nvi Dockerfile\n# Install Sageworks (changes often)\nRUN pip install --no-cache-dir sageworks==0.2.28 &lt;-- change this\n</code></pre>"},{"location":"admin/docker_push/#build-the-docker-image","title":"Build the Docker Image","text":"<pre><code>docker build -t sageworks_dashboard:v0_1_9_amd64 --platform linux/amd64 .\n</code></pre>"},{"location":"admin/docker_push/#test-the-image-locally","title":"Test the Image Locally","text":"<p>You have a <code>docker_local_dashboard</code> alias in your <code>~/.zshrc</code> :)</p>"},{"location":"admin/docker_push/#login-to-ecr","title":"Login to ECR","text":"<pre><code>aws ecr-public get-login-password --region us-east-1 --profile \\\nscp_sandbox_admin | docker login --username AWS \\\n--password-stdin public.ecr.aws\n</code></pre>"},{"location":"admin/docker_push/#tagpush-the-image-to-aws-ecr","title":"Tag/Push the Image to AWS ECR","text":"<p><pre><code>docker tag sageworks_dashboard:v0_2_28_amd64 \\\npublic.ecr.aws/m6i5k1r2/sageworks_dashboard:v0_2_28_amd64\n</code></pre> <pre><code>docker push public.ecr.aws/m6i5k1r2/sageworks_dashboard:v0_2_28_amd64\n</code></pre></p>"},{"location":"admin/docker_push/#update-the-latest-tag","title":"Update the 'latest' tag","text":"<p><pre><code>docker tag public.ecr.aws/m6i5k1r2/sageworks_dashboard:v0_2_28_amd64 \\\npublic.ecr.aws/m6i5k1r2/sageworks_dashboard:latest\n</code></pre> <pre><code>docker push public.ecr.aws/m6i5k1r2/sageworks_dashboard:latest\n</code></pre></p>"},{"location":"admin/docker_push/#test-the-ecr-image","title":"Test the ECR Image","text":"<p>You have a <code>docker_ecr_dashboard</code> alias in your <code>~/.zshrc</code> :)</p>"},{"location":"admin/pypi_release/","title":"PyPI Release Notes","text":"<p>Notes and information on how to do the PyPI release for the SageMaker project. For full details on packaging you can reference this page Packaging</p> <p>The following instructions should work, but things change :)</p>"},{"location":"admin/pypi_release/#package-requirements","title":"Package Requirements","text":"<ul> <li>pip install tox</li> <li>pip install --upgrade setuptools wheel</li> <li>pip install twine</li> </ul>"},{"location":"admin/pypi_release/#setup-pypirc","title":"Setup pypirc","text":"<p>The easiest thing to do is setup a \\~/.pypirc file with the following contents</p> <pre><code>[distutils]\nindex-servers =\n  pypi\n  testpypi\n\n[pypi]\nusername = __token__\npassword = pypi-AgEIcH...\n\n[testpypi]\nusername = __token__\npassword = pypi-AgENdG...\n</code></pre>"},{"location":"admin/pypi_release/#tox-background","title":"Tox Background","text":"<p>Tox will install the SageMaker Sandbox package into a blank virtualenv and then execute all the tests against the newly installed package. So if everything goes okay, you know the pypi package installed fine and the tests (which puls from the installed <code>sageworks</code> package) also ran okay.</p>"},{"location":"admin/pypi_release/#make-sure-all-tests-pass","title":"Make sure ALL tests pass","text":"<pre><code>$ cd sageworks\n$ tox \n</code></pre> <p>If ALL the test above pass...</p>"},{"location":"admin/pypi_release/#clean-any-previous-distribution-files","title":"Clean any previous distribution files","text":"<pre><code>make clean\n</code></pre>"},{"location":"admin/pypi_release/#tag-the-new-version","title":"Tag the New Version","text":"<pre><code>git tag v0.1.8 (or whatever)\ngit push --tags\n</code></pre>"},{"location":"admin/pypi_release/#create-the-test-pypi-release","title":"Create the TEST PyPI Release","text":"<pre><code>python setup.py sdist bdist_wheel\ntwine upload dist/* -r testpypi\n</code></pre>"},{"location":"admin/pypi_release/#install-the-test-pypi-release","title":"Install the TEST PyPI Release","text":"<pre><code>pip install --index-url https://test.pypi.org/simple sageworks\n</code></pre>"},{"location":"admin/pypi_release/#create-the-real-pypi-release","title":"Create the REAL PyPI Release","text":"<pre><code>twine upload dist/* -r pypi\n</code></pre>"},{"location":"admin/pypi_release/#push-any-possible-changes-to-github","title":"Push any possible changes to Github","text":"<pre><code>git push\n</code></pre>"},{"location":"api_classes/data_source/","title":"DataSource","text":"<p>DataSource Examples</p> <p>Examples of using the DataSource class are in the Examples section at the bottom of this page. S3 data, local files, and Pandas dataframes, DataSource can read data from many different sources.</p> <p>DataSource: Manages AWS Data Catalog creation and management. DataSources are set up so that can easily be queried with AWS Athena. All DataSources are run through a full set of Exploratory Data Analysis (EDA) techniques (data quality, distributions, stats, outliers, etc.) DataSources can be viewed and explored within the SageWorks Dashboard UI.</p>"},{"location":"api_classes/data_source/#sageworks.api.data_source.DataSource","title":"<code>DataSource</code>","text":"<p>             Bases: <code>AthenaSource</code></p> <p>DataSource: SageWorks DataSource API Class</p> Common Usage <pre><code>my_data = DataSource(name_of_source)\nmy_data.summary()\nmy_data.details()\nmy_features = my_data.to_features()\n</code></pre> Source code in <code>src/sageworks/api/data_source.py</code> <pre><code>class DataSource(AthenaSource):\n    \"\"\"DataSource: SageWorks DataSource API Class\n\n    Common Usage:\n        ```\n        my_data = DataSource(name_of_source)\n        my_data.summary()\n        my_data.details()\n        my_features = my_data.to_features()\n        ```\n    \"\"\"\n\n    def __init__(self, source, name: str = None, tags: list = None):\n        \"\"\"\n        Initializes a new DataSource object.\n\n        Args:\n            source (str): The source of the data. This can be an S3 bucket, file path,\n                          DataFrame object, or an existing DataSource object.\n            name (str): The name of the data source. If not specified, a name will be generated.\n            tags (list[str]): A list of tags associated with the data source. If not specified tags will be generated.\n        \"\"\"\n        self.log = logging.getLogger(\"sageworks\")\n\n        # Load the source (S3, File, or Existing DataSource)\n        ds_name = extract_data_source_basename(source) if name is None else name\n        if ds_name == \"dataframe\":\n            msg = \"Set the 'name' argument in the constructor: DataSource(df, name='my_data')\"\n            self.log.critical(msg)\n            raise ValueError(msg)\n        tags = [ds_name] if tags is None else tags\n        self._load_source(source, ds_name, tags)\n\n        # Call superclass init\n        super().__init__(ds_name)\n\n    def to_features(\n        self, name: str = None, tags: list = None, id_column: str = None, event_time_column: str = None\n    ) -&gt; FeatureSet:\n        \"\"\"\n        Convert the DataSource to a FeatureSet\n\n        Args:\n            name (str): Set the name for feature set. If not specified, a name will be generated\n            tags (list): Set the tags for the feature set. If not specified tags will be generated.\n            id_column (str): Set the id column for the feature set. If not specified will be generated.\n            event_time_column (str): Set the event time for the feature set. If not specified will be generated.\n\n        Returns:\n            FeatureSet: The FeatureSet created from the DataSource\n        \"\"\"\n\n        # Create the FeatureSet Name and Tags\n        fs_name = self.uuid.replace(\"_data\", \"\") + \"_features\" if name is None else name\n        tags = [fs_name] if tags is None else tags\n\n        # Transform the DataSource to a FeatureSet\n        data_to_features = DataToFeaturesLight(self.uuid, fs_name)\n        data_to_features.set_output_tags(tags)\n        data_to_features.transform(id_column=id_column, event_time_column=event_time_column)\n\n        # Return the FeatureSet (which will now be up-to-date)\n        return FeatureSet(fs_name)\n\n    def _load_source(self, source: str, name: str, tags: list):\n        \"\"\"Load the source of the data\"\"\"\n        self.log.info(f\"Loading source: {source}...\")\n\n        # Pandas DataFrame Source\n        if isinstance(source, pd.DataFrame):\n            my_loader = PandasToData(name)\n            my_loader.set_input(source)\n            my_loader.set_output_tags(tags)\n            my_loader.transform()\n\n        # S3 Source\n        source = source if isinstance(source, str) else str(source)\n        if source.startswith(\"s3://\"):\n            my_loader = S3ToDataSourceLight(source, name)\n            my_loader.set_output_tags(tags)\n            my_loader.transform()\n\n        # File Source\n        elif os.path.isfile(source):\n            my_loader = CSVToDataSource(source, name)\n            my_loader.set_output_tags(tags)\n            my_loader.transform()\n</code></pre>"},{"location":"api_classes/data_source/#sageworks.api.data_source.DataSource.__init__","title":"<code>__init__(source, name=None, tags=None)</code>","text":"<p>Initializes a new DataSource object.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source of the data. This can be an S3 bucket, file path,           DataFrame object, or an existing DataSource object.</p> required <code>name</code> <code>str</code> <p>The name of the data source. If not specified, a name will be generated.</p> <code>None</code> <code>tags</code> <code>list[str]</code> <p>A list of tags associated with the data source. If not specified tags will be generated.</p> <code>None</code> Source code in <code>src/sageworks/api/data_source.py</code> <pre><code>def __init__(self, source, name: str = None, tags: list = None):\n    \"\"\"\n    Initializes a new DataSource object.\n\n    Args:\n        source (str): The source of the data. This can be an S3 bucket, file path,\n                      DataFrame object, or an existing DataSource object.\n        name (str): The name of the data source. If not specified, a name will be generated.\n        tags (list[str]): A list of tags associated with the data source. If not specified tags will be generated.\n    \"\"\"\n    self.log = logging.getLogger(\"sageworks\")\n\n    # Load the source (S3, File, or Existing DataSource)\n    ds_name = extract_data_source_basename(source) if name is None else name\n    if ds_name == \"dataframe\":\n        msg = \"Set the 'name' argument in the constructor: DataSource(df, name='my_data')\"\n        self.log.critical(msg)\n        raise ValueError(msg)\n    tags = [ds_name] if tags is None else tags\n    self._load_source(source, ds_name, tags)\n\n    # Call superclass init\n    super().__init__(ds_name)\n</code></pre>"},{"location":"api_classes/data_source/#sageworks.api.data_source.DataSource.to_features","title":"<code>to_features(name=None, tags=None, id_column=None, event_time_column=None)</code>","text":"<p>Convert the DataSource to a FeatureSet</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Set the name for feature set. If not specified, a name will be generated</p> <code>None</code> <code>tags</code> <code>list</code> <p>Set the tags for the feature set. If not specified tags will be generated.</p> <code>None</code> <code>id_column</code> <code>str</code> <p>Set the id column for the feature set. If not specified will be generated.</p> <code>None</code> <code>event_time_column</code> <code>str</code> <p>Set the event time for the feature set. If not specified will be generated.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>FeatureSet</code> <code>FeatureSet</code> <p>The FeatureSet created from the DataSource</p> Source code in <code>src/sageworks/api/data_source.py</code> <pre><code>def to_features(\n    self, name: str = None, tags: list = None, id_column: str = None, event_time_column: str = None\n) -&gt; FeatureSet:\n    \"\"\"\n    Convert the DataSource to a FeatureSet\n\n    Args:\n        name (str): Set the name for feature set. If not specified, a name will be generated\n        tags (list): Set the tags for the feature set. If not specified tags will be generated.\n        id_column (str): Set the id column for the feature set. If not specified will be generated.\n        event_time_column (str): Set the event time for the feature set. If not specified will be generated.\n\n    Returns:\n        FeatureSet: The FeatureSet created from the DataSource\n    \"\"\"\n\n    # Create the FeatureSet Name and Tags\n    fs_name = self.uuid.replace(\"_data\", \"\") + \"_features\" if name is None else name\n    tags = [fs_name] if tags is None else tags\n\n    # Transform the DataSource to a FeatureSet\n    data_to_features = DataToFeaturesLight(self.uuid, fs_name)\n    data_to_features.set_output_tags(tags)\n    data_to_features.transform(id_column=id_column, event_time_column=event_time_column)\n\n    # Return the FeatureSet (which will now be up-to-date)\n    return FeatureSet(fs_name)\n</code></pre>"},{"location":"api_classes/data_source/#examples","title":"Examples","text":"<p>All of the SageWorks Examples are in the Sageworks Repository under the <code>examples/</code> directory. For a full code listing of any example please visit our SageWorks Examples</p> <p>Create a DataSource from an S3 Path or File Path</p> datasource_from_s3.py<pre><code>from sageworks.api.data_source import DataSource\n\n# Create a DataSource from an S3 Path (or a local file)\nsource_path = \"s3://sageworks-public-data/common/abalone.csv\"\n# source_path = \"/full/path/to/local/file.csv\"\n\nmy_data = DataSource(source_path)\nprint(my_data.details())\n</code></pre> <p>Create a DataSource from a Pandas Dataframe</p> datasource_from_df.py<pre><code>from sageworks.utils.test_data_generator import TestDataGenerator\nfrom sageworks.api.data_source import DataSource\n\n# Create a DataSource from a Pandas DataFrame\ngen_data = TestDataGenerator()\ndf = gen_data.person_data()\n\ntest_data = DataSource(df, name=\"test_data\")\nprint(test_data.details())\n</code></pre> <p>Create a FeatureSet from a Datasource</p> datasource_to_featureset.py<pre><code>from sageworks.api.data_source import DataSource\n\n# Convert the Data Source to a Feature Set\ntest_data = DataSource('test_data')\nmy_features = test_data.to_features()\nprint(my_features.details())\n</code></pre>"},{"location":"api_classes/data_source/#sageworks-ui","title":"SageWorks UI","text":"<p>Running these few lines of code performs a comprehensive set of Exploratory Data Analysis techniques on your data, pushes the results into AWS, and provides a detailed web visualization of the results.</p> SageWorks Dashboard: DataSources <p>Not Finding a particular method?</p> <p>The SageWorks API Classes use 'Core' Classes Internally, so you can lookup all the methods in SageWorks Core Classes</p>"},{"location":"api_classes/endpoint/","title":"Endpoint","text":"<p>Endpoint Examples</p> <p>Examples of using the Endpoint class are listed at the bottom of this page Examples.</p> <p>Endpoint: Manages AWS Endpoint creation and deployment. Endpoints are automatically set up and provisioned for deployment into AWS. Endpoints can be viewed in the AWS Sagemaker interfaces or in the SageWorks Dashboard UI, which provides additional model details and performance metrics</p>"},{"location":"api_classes/endpoint/#sageworks.api.endpoint.Endpoint","title":"<code>Endpoint</code>","text":"<p>             Bases: <code>EndpointCore</code></p> <p>Endpoint: SageWorks Endpoint API Class</p> Common Usage <pre><code>my_endpoint = Endpoint(name)\nmy_endpoint.summary()\nmy_endpoint.details()\nmy_endpoint.predict(df)\n</code></pre> Source code in <code>src/sageworks/api/endpoint.py</code> <pre><code>class Endpoint(EndpointCore):\n    \"\"\"Endpoint: SageWorks Endpoint API Class\n\n    Common Usage:\n        ```\n        my_endpoint = Endpoint(name)\n        my_endpoint.summary()\n        my_endpoint.details()\n        my_endpoint.predict(df)\n        ```\n    \"\"\"\n\n    def __init__(self, name):\n        \"\"\"Endpoint Initialization\n\n        Args:\n            name (str): The name of the Endpoint\n        \"\"\"\n        # Call superclass init\n        super().__init__(name)\n</code></pre>"},{"location":"api_classes/endpoint/#sageworks.api.endpoint.Endpoint.__init__","title":"<code>__init__(name)</code>","text":"<p>Endpoint Initialization</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the Endpoint</p> required Source code in <code>src/sageworks/api/endpoint.py</code> <pre><code>def __init__(self, name):\n    \"\"\"Endpoint Initialization\n\n    Args:\n        name (str): The name of the Endpoint\n    \"\"\"\n    # Call superclass init\n    super().__init__(name)\n</code></pre>"},{"location":"api_classes/endpoint/#examples","title":"Examples","text":"<p>Run Inference on an Endpoint</p> endpoint_inference.py<pre><code>from sageworks.api.feature_set import FeatureSet\nfrom sageworks.api.model import Model\nfrom sageworks.api.endpoint import Endpoint\n\n# Grab an existing Endpoint\nendpoint = Endpoint(\"abalone-regression-end\")\n\n# SageWorks has full ML Pipeline provenance, so we can backtrack the inputs,\n# get a DataFrame of data (not used for training) and run inference\nmodel_name = endpoint.get_input()\nfs_name = Model(model_name).get_input()\nfs = FeatureSet(fs_name)\nathena_table = fs.get_training_view_table()\ndf = fs.query(f\"SELECT * FROM {athena_table} where training = 0\")\n\n# Run inference/predictions on the Endpoint\nresults = endpoint.predict(df)\nprint(results[[\"class_number_of_rings\", \"prediction\"]])\n</code></pre> <p>Output</p> <p><pre><code>Processing...\n     class_number_of_rings  prediction\n0                       13   11.477922\n1                       12   12.316887\n2                        8    7.612847\n3                        8    9.663341\n4                        9    9.075263\n..                     ...         ...\n839                      8    8.069856\n840                     15   14.915502\n841                     11   10.977605\n842                     10   10.173433\n843                      7    7.297976\n</code></pre> Endpoint Details</p> <p>The details() method</p> <p>The <code>detail()</code> method on the Endpoint class provides a lot of useful information. All of the SageWorks classes have a <code>details()</code> method try it out!</p> endpoint_details.py<pre><code>from sageworks.api.endpoint import Endpoint\nfrom pprint import pprint\n\n# Get Endpoint and print out it's details\nendpoint = Endpoint(\"abalone-regression-end\")\npprint(endpoint.details())\n</code></pre> <p>Output</p> <pre><code>{\n 'input': 'abalone-regression',\n 'instance': 'Serverless (2GB/5)',\n 'model_metrics':   metric_name  value\n            0        RMSE  2.190\n            1         MAE  1.544\n            2          R2  0.504,\n 'model_name': 'abalone-regression',\n 'model_type': 'regressor',\n 'modified': datetime.datetime(2023, 12, 29, 17, 48, 35, 115000, tzinfo=datetime.timezone.utc),\n     class_number_of_rings  prediction\n0                        9    8.648378\n1                       11    9.717787\n2                       11   10.933070\n3                       10    9.899738\n4                        9   10.014504\n..                     ...         ...\n495                     10   10.261657\n496                      9   10.788254\n497                     13    7.779886\n498                     12   14.718514\n499                     13   10.637320\n 'sageworks_tags': ['abalone', 'regression'],\n 'status': 'InService',\n 'uuid': 'abalone-regression-end',\n 'variant': 'AllTraffic'}\n</code></pre> <p>Endpoint Metrics</p> endpoint_metrics.py<pre><code>from sageworks.api.endpoint import Endpoint\n\n# Grab an existing Endpoint\nendpoint = Endpoint(\"abalone-regression-end\")\n\n# SageWorks tracks both Model performance and Endpoint Metrics\nmodel_metrics = endpoint.details()[\"model_metrics\"]\nendpoint_metrics = endpoint.endpoint_metrics()\nprint(model_metrics)\nprint(endpoint_metrics)\n</code></pre> <p>Output</p> <pre><code>  metric_name  value\n0        RMSE  2.190\n1         MAE  1.544\n2          R2  0.504\n\n    Invocations  ModelLatency  OverheadLatency  ModelSetupTime  Invocation5XXErrors\n29          0.0          0.00             0.00            0.00                  0.0\n30          1.0          1.11            23.73           23.34                  0.0\n31          0.0          0.00             0.00            0.00                  0.0\n48          0.0          0.00             0.00            0.00                  0.0\n49          5.0          0.45             9.64           23.57                  0.0\n50          2.0          0.57             0.08            0.00                  0.0\n51          0.0          0.00             0.00            0.00                  0.0\n60          4.0          0.33             5.80           22.65                  0.0\n61          1.0          1.11            23.35           23.10                  0.0\n62          0.0          0.00             0.00            0.00                  0.0\n...\n</code></pre>"},{"location":"api_classes/endpoint/#sageworks-ui","title":"SageWorks UI","text":"<p>Running these few lines of code creates and deploys an AWS Endpoint. The Endpoint artifacts can be viewed in the Sagemaker Console/Notebook interfaces or in the SageWorks Dashboard UI. SageWorks will monitor the endpoint, plot invocations, latencies, and tracks error metrics.</p> SageWorks Dashboard: Endpoints <p>Not Finding a particular method?</p> <p>The SageWorks API Classes use 'Core' Classes Internally, so you can lookup all the methods in</p> <p>SageWorks Core Classes</p>"},{"location":"api_classes/feature_set/","title":"FeatureSet","text":"<p>FeatureSet Examples</p> <p>Examples of using the FeatureSet Class are in the Examples section at the bottom of this page. AWS Feature Store and Feature Groups are quite complicated to set up manually but the SageWorks FeatureSet makes it a breeze!</p> <p>FeatureSet: Manages AWS Feature Store/Group creation and management. FeatureSets are set up so they can easily be queried with AWS Athena. All FeatureSets are run through a full set of Exploratory Data Analysis (EDA) techniques (data quality, distributions, stats, outliers, etc.) FeatureSets can be viewed and explored within the SageWorks Dashboard UI.</p>"},{"location":"api_classes/feature_set/#sageworks.api.feature_set.FeatureSet","title":"<code>FeatureSet</code>","text":"<p>             Bases: <code>FeatureSetCore</code></p> <p>FeatureSet: SageWorks FeatureSet API Class</p> Common Usage <pre><code>my_features = FeatureSet(name)\nmy_features.summary()\nmy_features.details()\nmy_features.to_model()\n</code></pre> Source code in <code>src/sageworks/api/feature_set.py</code> <pre><code>class FeatureSet(FeatureSetCore):\n    \"\"\"FeatureSet: SageWorks FeatureSet API Class\n\n    Common Usage:\n        ```\n        my_features = FeatureSet(name)\n        my_features.summary()\n        my_features.details()\n        my_features.to_model()\n        ```\n    \"\"\"\n\n    def __init__(self, name):\n        \"\"\"FeatureSet Initialization\n\n        Args:\n            name (str): The name of the FeatureSet\n        \"\"\"\n        # Call superclass init\n        super().__init__(name)\n\n    def to_model(\n        self,\n        model_type: ModelType,\n        target_column: str,\n        name: str = None,\n        tags: list = None,\n        description: str = None,\n        feature_list: list = None,\n    ) -&gt; Model:\n        \"\"\"Create a Model from the FeatureSet\n\n        Args:\n            model_type (ModelType): The type of model to create (See sageworks.model.ModelType)\n            target_column (str): The target column for the model (use None for unsupervised model)\n            name (str): Set the name for the model. If not specified, a name will be generated\n            tags (list): Set the tags for the model.  If not specified tags will be generated.\n            description (str): Set the description for the model. If not specified a description is generated.\n            feature_list (list): Set the feature list for the model. If not specified a feature list is generated.\n\n        Returns:\n            Model: The Model created from the FeatureSet\n        \"\"\"\n\n        # Create the Model Name and Tags\n        model_name = self.uuid.replace(\"_features\", \"\").replace(\"_\", \"-\") + \"-model\" if name is None else name\n        tags = [model_name] if tags is None else tags\n\n        # Transform the FeatureSet into a Model\n        features_to_model = FeaturesToModel(self.uuid, model_name, model_type=model_type)\n        features_to_model.set_output_tags(tags)\n        features_to_model.transform(target_column=target_column, description=description, feature_list=feature_list)\n\n        # Return the Model\n        return Model(model_name)\n</code></pre>"},{"location":"api_classes/feature_set/#sageworks.api.feature_set.FeatureSet.__init__","title":"<code>__init__(name)</code>","text":"<p>FeatureSet Initialization</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the FeatureSet</p> required Source code in <code>src/sageworks/api/feature_set.py</code> <pre><code>def __init__(self, name):\n    \"\"\"FeatureSet Initialization\n\n    Args:\n        name (str): The name of the FeatureSet\n    \"\"\"\n    # Call superclass init\n    super().__init__(name)\n</code></pre>"},{"location":"api_classes/feature_set/#sageworks.api.feature_set.FeatureSet.to_model","title":"<code>to_model(model_type, target_column, name=None, tags=None, description=None, feature_list=None)</code>","text":"<p>Create a Model from the FeatureSet</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>ModelType</code> <p>The type of model to create (See sageworks.model.ModelType)</p> required <code>target_column</code> <code>str</code> <p>The target column for the model (use None for unsupervised model)</p> required <code>name</code> <code>str</code> <p>Set the name for the model. If not specified, a name will be generated</p> <code>None</code> <code>tags</code> <code>list</code> <p>Set the tags for the model.  If not specified tags will be generated.</p> <code>None</code> <code>description</code> <code>str</code> <p>Set the description for the model. If not specified a description is generated.</p> <code>None</code> <code>feature_list</code> <code>list</code> <p>Set the feature list for the model. If not specified a feature list is generated.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>The Model created from the FeatureSet</p> Source code in <code>src/sageworks/api/feature_set.py</code> <pre><code>def to_model(\n    self,\n    model_type: ModelType,\n    target_column: str,\n    name: str = None,\n    tags: list = None,\n    description: str = None,\n    feature_list: list = None,\n) -&gt; Model:\n    \"\"\"Create a Model from the FeatureSet\n\n    Args:\n        model_type (ModelType): The type of model to create (See sageworks.model.ModelType)\n        target_column (str): The target column for the model (use None for unsupervised model)\n        name (str): Set the name for the model. If not specified, a name will be generated\n        tags (list): Set the tags for the model.  If not specified tags will be generated.\n        description (str): Set the description for the model. If not specified a description is generated.\n        feature_list (list): Set the feature list for the model. If not specified a feature list is generated.\n\n    Returns:\n        Model: The Model created from the FeatureSet\n    \"\"\"\n\n    # Create the Model Name and Tags\n    model_name = self.uuid.replace(\"_features\", \"\").replace(\"_\", \"-\") + \"-model\" if name is None else name\n    tags = [model_name] if tags is None else tags\n\n    # Transform the FeatureSet into a Model\n    features_to_model = FeaturesToModel(self.uuid, model_name, model_type=model_type)\n    features_to_model.set_output_tags(tags)\n    features_to_model.transform(target_column=target_column, description=description, feature_list=feature_list)\n\n    # Return the Model\n    return Model(model_name)\n</code></pre>"},{"location":"api_classes/feature_set/#examples","title":"Examples","text":"<p>All of the SageWorks Examples are in the Sageworks Repository under the <code>examples/</code> directory. For a full code listing of any example please visit our SageWorks Examples</p> <p>Create a FeatureSet from a Datasource</p> datasource_to_featureset.py<pre><code>from sageworks.api.data_source import DataSource\n\n# Convert the Data Source to a Feature Set\ntest_data = DataSource('test_data')\nmy_features = test_data.to_features()\nprint(my_features.details())\n</code></pre> <p>FeatureSet EDA Statistics</p> <p>featureset_eda.py<pre><code>from sageworks.api.feature_set import FeatureSet\nimport pandas as pd\n\n# Grab a FeatureSet and pull some of the EDA Stats\nmy_features = FeatureSet('test_features')\n\n# Grab some of the EDA Stats\ncorr_data = my_features.correlations()\ncorr_df = pd.DataFrame(corr_data)\nprint(corr_df)\n\n# Get some outliers\noutliers = my_features.outliers()\npprint(outliers.head())\n\n# Full set of EDA Stats\neda_stats = my_features.column_stats()\npprint(eda_stats)\n</code></pre> Output</p> <pre><code>                 age  food_pizza  food_steak  food_sushi  food_tacos    height        id  iq_score\nage              NaN   -0.188645   -0.256356    0.263048    0.054211  0.439678 -0.054948 -0.295513\nfood_pizza -0.188645         NaN   -0.288175   -0.229591   -0.196818 -0.494380  0.137282  0.395378\nfood_steak -0.256356   -0.288175         NaN   -0.374920   -0.321403 -0.002542 -0.005199  0.076477\nfood_sushi  0.263048   -0.229591   -0.374920         NaN   -0.256064  0.536396  0.038279 -0.435033\nfood_tacos  0.054211   -0.196818   -0.321403   -0.256064         NaN -0.091493 -0.051398  0.033364\nheight      0.439678   -0.494380   -0.002542    0.536396   -0.091493       NaN -0.117372 -0.655210\nid         -0.054948    0.137282   -0.005199    0.038279   -0.051398 -0.117372       NaN  0.106020\niq_score   -0.295513    0.395378    0.076477   -0.435033    0.033364 -0.655210  0.106020       NaN\n\n        name     height      weight         salary  age    iq_score  likes_dogs  food_pizza  food_steak  food_sushi  food_tacos outlier_group\n0  Person 96  57.582840  148.461349   80000.000000   43  150.000000           1           0           0           0           0    height_low\n1  Person 68  73.918663  189.527313  219994.000000   80  100.000000           0           0           0           1           0  iq_score_low\n2  Person 49  70.381790  261.237000  175633.703125   49  107.933998           0           0           0           1           0  iq_score_low\n3  Person 90  73.488739  193.840698  227760.000000   72  110.821541           1           0           0           0           0   salary_high\n\n&lt;lots of EDA data and statistics&gt;\n</code></pre> <p>Create a Model from a FeatureSet</p> featureset_to_model.py<pre><code>from sageworks.api.feature_set import FeatureSet\nfrom sageworks.api.model import ModelType\nfrom pprint import pprint\n\n# Grab a FeatureSet\nmy_features = FeatureSet('test_features')\n\n# Create a Model from the FeatureSet\n# Note: ModelTypes can be CLASSIFIER, REGRESSOR, \n#       UNSUPERVISED, or TRANSFORMER\nmy_model = my_features.to_model(model_type=ModelType.REGRESSOR, \n                                target_column=\"iq_score\")\npprint(my_model.details())\n</code></pre> <p>Output</p> <pre><code>{'approval_status': 'Approved',\n 'content_types': ['text/csv'],\n ...\n 'inference_types': ['ml.t2.medium'],\n 'input': 'test_features',\n 'model_metrics':   metric_name  value\n                0        RMSE  7.924\n                1         MAE  6.554,\n                2          R2  0.604,\n 'regression_predictions':       iq_score  prediction\n                            0   136.519012  139.964460\n                            1   133.616974  130.819950\n                            2   122.495415  124.967834\n                            3   133.279510  121.010284\n                            4   127.881073  113.825005\n    ...\n 'response_types': ['text/csv'],\n 'sageworks_tags': ['test-model'],\n 'shapley_values': None,\n 'size': 0.0,\n 'status': 'Completed',\n 'transform_types': ['ml.m5.large'],\n 'uuid': 'test-model',\n 'version': 1}\n</code></pre>"},{"location":"api_classes/feature_set/#sageworks-ui","title":"SageWorks UI","text":"<p>Running these few lines of code performs a comprehensive set of Exploratory Data Analysis techniques on your data, pushes the results into AWS, and provides a detailed web visualization of the results.</p> SageWorks Dashboard: FeatureSets <p>Not Finding a particular method?</p> <p>The SageWorks API Classes use 'Core' Classes Internally, so you can lookup all the methods in SageWorks Core Classes</p>"},{"location":"api_classes/model/","title":"Model","text":"<p>Model Examples</p> <p>Examples of using the Model Class are in the Examples section at the bottom of this page. AWS Model setup and deployment are quite complicated to do manually but the SageWorks Model Class makes it a breeze!</p> <p>Model: Manages AWS Model Package/Group creation and management. Models are automatically set up and provisioned for deployment into AWS. Models can be viewed in the AWS Sagemaker interfaces or in the SageWorks Dashboard UI, which provides additional model details and performance metrics</p>"},{"location":"api_classes/model/#sageworks.api.model.Model","title":"<code>Model</code>","text":"<p>             Bases: <code>ModelCore</code></p> <p>Model: SageWorks Model API Class</p> Common Usage <pre><code>my_features = Model(name)\nmy_features.summary()\nmy_features.details()\nmy_features.to_endpoint()\n</code></pre> Source code in <code>src/sageworks/api/model.py</code> <pre><code>class Model(ModelCore):\n    \"\"\"Model: SageWorks Model API Class\n\n    Common Usage:\n        ```\n        my_features = Model(name)\n        my_features.summary()\n        my_features.details()\n        my_features.to_endpoint()\n        ```\n    \"\"\"\n\n    def __init__(self, name):\n        \"\"\"Model Initialization\n        Args:\n            name (str): The name of the Model\n        \"\"\"\n        # Call superclass init\n        super().__init__(name)\n\n    def to_endpoint(self, name: str = None, tags: list = None, serverless: bool = True) -&gt; Endpoint:\n        \"\"\"Create an Endpoint from the Model\n\n        Args:\n            name (str): Set the name for the endpoint. If not specified, an automatic name will be generated\n            tags (list): Set the tags for the endpoint. If not specified automatic tags will be generated.\n            serverless (bool): Set the endpoint to be serverless (default: True)\n\n        Returns:\n            Endpoint: The Endpoint created from the Model\n        \"\"\"\n\n        # Create the Endpoint Name and Tags\n        endpoint_name = self.uuid.replace(\"-model\", \"\") + \"-end\" if name is None else name\n        tags = [endpoint_name] if tags is None else tags\n\n        # Create an Endpoint from the Model\n        model_to_endpoint = ModelToEndpoint(self.uuid, endpoint_name, serverless=serverless)\n        model_to_endpoint.set_output_tags(tags)\n        model_to_endpoint.transform()\n\n        # Return the Endpoint\n        return Endpoint(endpoint_name)\n</code></pre>"},{"location":"api_classes/model/#sageworks.api.model.Model.__init__","title":"<code>__init__(name)</code>","text":"<p>Model Initialization Args:     name (str): The name of the Model</p> Source code in <code>src/sageworks/api/model.py</code> <pre><code>def __init__(self, name):\n    \"\"\"Model Initialization\n    Args:\n        name (str): The name of the Model\n    \"\"\"\n    # Call superclass init\n    super().__init__(name)\n</code></pre>"},{"location":"api_classes/model/#sageworks.api.model.Model.to_endpoint","title":"<code>to_endpoint(name=None, tags=None, serverless=True)</code>","text":"<p>Create an Endpoint from the Model</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Set the name for the endpoint. If not specified, an automatic name will be generated</p> <code>None</code> <code>tags</code> <code>list</code> <p>Set the tags for the endpoint. If not specified automatic tags will be generated.</p> <code>None</code> <code>serverless</code> <code>bool</code> <p>Set the endpoint to be serverless (default: True)</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Endpoint</code> <code>Endpoint</code> <p>The Endpoint created from the Model</p> Source code in <code>src/sageworks/api/model.py</code> <pre><code>def to_endpoint(self, name: str = None, tags: list = None, serverless: bool = True) -&gt; Endpoint:\n    \"\"\"Create an Endpoint from the Model\n\n    Args:\n        name (str): Set the name for the endpoint. If not specified, an automatic name will be generated\n        tags (list): Set the tags for the endpoint. If not specified automatic tags will be generated.\n        serverless (bool): Set the endpoint to be serverless (default: True)\n\n    Returns:\n        Endpoint: The Endpoint created from the Model\n    \"\"\"\n\n    # Create the Endpoint Name and Tags\n    endpoint_name = self.uuid.replace(\"-model\", \"\") + \"-end\" if name is None else name\n    tags = [endpoint_name] if tags is None else tags\n\n    # Create an Endpoint from the Model\n    model_to_endpoint = ModelToEndpoint(self.uuid, endpoint_name, serverless=serverless)\n    model_to_endpoint.set_output_tags(tags)\n    model_to_endpoint.transform()\n\n    # Return the Endpoint\n    return Endpoint(endpoint_name)\n</code></pre>"},{"location":"api_classes/model/#examples","title":"Examples","text":"<p>All of the SageWorks Examples are in the Sageworks Repository under the <code>examples/</code> directory. For a full code listing of any example please visit our SageWorks Examples</p> <p>Create a Model from a FeatureSet</p> featureset_to_model.py<pre><code>from sageworks.api.feature_set import FeatureSet\nfrom sageworks.api.model import ModelType\nfrom pprint import pprint\n\n# Grab a FeatureSet\nmy_features = FeatureSet(\"test_features\")\n\n# Create a Model from the FeatureSet\n# Note: ModelTypes can be CLASSIFIER, REGRESSOR, \n#       UNSUPERVISED, or TRANSFORMER\nmy_model = my_features.to_model(model_type=ModelType.REGRESSOR, \n                                target_column=\"iq_score\")\npprint(my_model.details())\n</code></pre> <p>Output</p> <pre><code>{'approval_status': 'Approved',\n 'content_types': ['text/csv'],\n ...\n 'inference_types': ['ml.t2.medium'],\n 'input': 'test_features',\n 'model_metrics':   metric_name  value\n                0        RMSE  7.924\n                1         MAE  6.554,\n                2          R2  0.604,\n 'regression_predictions':       iq_score  prediction\n                            0   136.519012  139.964460\n                            1   133.616974  130.819950\n                            2   122.495415  124.967834\n                            3   133.279510  121.010284\n                            4   127.881073  113.825005\n    ...\n 'response_types': ['text/csv'],\n 'sageworks_tags': ['test-model'],\n 'shapley_values': None,\n 'size': 0.0,\n 'status': 'Completed',\n 'transform_types': ['ml.m5.large'],\n 'uuid': 'test-model',\n 'version': 1}\n</code></pre> <p>Create an Endpoint from a Model</p> <p>Endpoint Costs</p> <p>Serverless endpoints are a great option, they have no AWS charges when not running. A realtime endpoint has less latency (no cold start) but AWS charges an hourly fee which can add up quickly!</p> model_to_endpoint.py<pre><code>from sageworks.api.model import Model\n\n# Grab the abalone regression Model\nmodel = Model(\"abalone-regression\")\n\n# By default, an Endpoint is serverless, you can\n# make a realtime endpoint with serverless=False\nmodel.to_endpoint(name=\"abalone-regression-end\",\n                  tags=[\"abalone\", \"regression\"],\n                  serverless=True)\n</code></pre> <p>Model Health Check and Metrics</p> model_metrics.py<pre><code>from sageworks.api.model import Model\n\n# Grab the abalone-regression Model\nmodel = Model(\"abalone-regression\")\n\n# Perform a health check on the model\n# Note: The health_check() method returns 'issues' if there are any\n#       problems, so if there are no issues, the model is healthy\nhealth_issues = model.health_check()\nif not health_issues:\n    print(\"Model is Healthy\")\nelse:\n    print(\"Model has issues\")\n    print(health_issues)\n\n# Get the model metrics and regression predictions\nprint(model.model_metrics())\nprint(model.regression_predictions())\n</code></pre> <p>Output</p> <pre><code>Model is Healthy\n  metric_name  value\n0        RMSE  2.190\n1         MAE  1.544\n2          R2  0.504\n\n     class_number_of_rings  prediction\n0                        9    8.648378\n1                       11    9.717787\n2                       11   10.933070\n3                       10    9.899738\n4                        9   10.014504\n..                     ...         ...\n495                     10   10.261657\n496                      9   10.788254\n497                     13    7.779886\n498                     12   14.718514\n499                     13   10.637320\n</code></pre>"},{"location":"api_classes/model/#sageworks-ui","title":"SageWorks UI","text":"<p>Running these few lines of code creates an AWS Model Package Group and an AWS Model Package. These model artifacts can be viewed in the Sagemaker Console/Notebook interfaces or in the SageWorks Dashboard UI.</p> SageWorks Dashboard: Models <p>Not Finding a particular method?</p> <p>The SageWorks API Classes use 'Core' Classes Internally, so you can lookup all the methods in SageWorks Core Classes</p>"},{"location":"api_classes/overview/","title":"Overview","text":"<p>Just Getting Started?</p> <p>You're in the right place, the SageWorks API Classes are the best way to get started with SageWorks!</p>"},{"location":"api_classes/overview/#welcome-to-the-sageworks-api-classes","title":"Welcome to the SageWorks API Classes","text":"<p>These classes provide high-level APIs for the SageWorks package, they enable your team to build full AWS Machine Learning Pipelines. They handle all the details around updating and managing a complex set of AWS Services. Each class provides an essential component of the overall ML Pipline. Simply combine the classes to build production ready, AWS powered, machine learning pipelines. </p> <ul> <li>DataSource: Manages AWS Data Catalog and Athena</li> <li>FeatureSet: Manages AWS Feature Store and Feature Groups</li> <li>Model: Manages the training and deployment of AWS Model Groups and Packages</li> <li>Endpoint: Manages the deployment and invocations/inference on AWS Endpoints</li> </ul> <p></p>"},{"location":"api_classes/overview/#example-ml-pipline","title":"Example ML Pipline","text":"full_ml_pipeline.py<pre><code>from sageworks.api.data_source import DataSource\nfrom sageworks.api.feature_set import FeatureSet\nfrom sageworks.api.model import Model, ModelType\nfrom sageworks.api.endpoint import Endpoint\n\n# Create the abalone_data DataSource\nds = DataSource(\"s3://sageworks-public-data/common/abalone.csv\")\n\n# Now create a FeatureSet\nds.to_features(\"abalone_features\")\n\n# Create the abalone_regression Model\nfs = FeatureSet(\"abalone_features\")\nfs.to_model(\n    ModelType.REGRESSOR,\n    name=\"abalone-regression\",\n    target_column=\"class_number_of_rings\",\n    tags=[\"abalone\", \"regression\"],\n    description=\"Abalone Regression Model\",\n)\n\n# Create the abalone_regression Endpoint\nmodel = Model(\"abalone-regression\")\nmodel.to_endpoint(name=\"abalone-regression-end\", tags=[\"abalone\", \"regression\"])\n\n# Now we'll run inference on the endpoint\nendpoint = Endpoint(\"abalone-regression-end\")\n\n# Get a DataFrame of data (not used to train) and run predictions\nathena_table = fs.get_training_view_table()\ndf = fs.query(f\"SELECT * FROM {athena_table} where training = 0\")\nresults = endpoint.predict(df)\nprint(results[[\"class_number_of_rings\", \"prediction\"]])\n</code></pre> <p>Output</p> <pre><code>Processing...\n     class_number_of_rings  prediction\n0                       12   10.477794\n1                       11    11.11835\n2                       14   13.605763\n3                       12   11.744759\n4                       17    15.55189\n..                     ...         ...\n826                      7    7.981503\n827                     11   11.246113\n828                      9    9.592911\n829                      6    6.129388\n830                      8    7.628252\n</code></pre> <p>Full AWS ML Pipeline Achievement Unlocked!</p> <p>Bing! You just built and deployed a full AWS Machine Learning Pipeline. You can now use the SageWorks Dashboard web interface to inspect your AWS artifacts. A comprehensive set of Exploratory Data Analysis techniques and Model Performance Metrics are available for your entire team to review, inspect and interact with.</p> <p></p> <p>Examples</p> <p>All of the SageWorks Examples are in the Sageworks Repository under the <code>examples/</code> directory. For a full code listing of any example please visit our SageWorks Examples</p>"},{"location":"aws_setup/aws_tips_and_tricks/","title":"AWS Tips and Tricks","text":"<p>Need AWS Help?</p> <p>The SuperCowPowers team is happy to give any assistance needed when setting up AWS and SageWorks. So please contact us at sageworks@supercowpowers.com or on chat us up on Discord </p> <p>This page tries to give helpful guidance when setting up AWS Accounts, Users, and Groups. In general AWS can be a bit tricky to set up the first time. Feel free to use any material in this guide but we're more than happy to help clients get their AWS Setup ready to go for FREE. Below are some guides for setting up a new AWS account for SageWorks and also setting up SSO Users and Groups within AWS.</p>"},{"location":"aws_setup/aws_tips_and_tricks/#new-aws-account-with-aws-organizations-easy","title":"New AWS Account (with AWS Organizations: easy)","text":"<ul> <li>If you already have an AWS Account you can activate the AWS Identity Center/Organization functionality.</li> <li>Now go to AWS Organizations page and hit 'Add an AWS Account' button</li> <li>Add a new User with permissions that allows AWS Stack creation</li> </ul> <p>Note</p> <p>If you need a 'new' email just add a plus sign '+' at the end of your existing email. That will works and any emails to that address will get forwarded to the existing email <code>bob.smith+aws_1@gmail.com</code></p>"},{"location":"aws_setup/aws_tips_and_tricks/#new-aws-account-without-aws-organizations-a-bit-harder","title":"New AWS Account (without AWS Organizations: a bit harder)","text":"<ul> <li>Goto: https://aws.amazon.com/free and hit the big button 'Create a Free Account'</li> <li>Enter email and the account name you'd like (anything is fine)</li> <li>You'll get a validation email and go through the rest of the Account setup procedure</li> <li>Add a new User with permissions that allows AWS Stack creation</li> </ul>"},{"location":"aws_setup/aws_tips_and_tricks/#sso-users-and-groups","title":"SSO Users and Groups","text":"<p>AWS SSO (Single Sign-On) is a cloud-based service that allows users to manage access to multiple AWS accounts and business applications using a single set of credentials. It simplifies the authentication process for users and provides centralized management of permissions and access control across various AWS resources. With AWS SSO, users can log in once and access all the applications and accounts they need, streamlining the user experience and increasing productivity. AWS SSO also enables IT administrators to manage access more efficiently by providing a single point of control for managing user access, permissions, and policies, reducing the risk of unauthorized access or security breaches.</p>"},{"location":"aws_setup/aws_tips_and_tricks/#setting-up-sso-users","title":"Setting up SSO Users","text":"<ul> <li>Log in to your AWS account and go to the AWS Identity Center console.</li> <li>Click on the \"Users\" tab and then click on the \"Add user\" button.</li> </ul> <p>The 'Add User' setup is fairly straight forward but here are some screen shots:</p> <p>On the first panel you can fill in the users information.</p> <p></p>"},{"location":"aws_setup/aws_tips_and_tricks/#groups","title":"Groups","text":"<p>On the second panel we suggest that you have at LEAST two groups: - Admin group - DataScientists group</p>"},{"location":"aws_setup/aws_tips_and_tricks/#setting-up-groups","title":"Setting up Groups","text":"<p>This allows you to put most of the users into the DataScientists group that has AWS policies based on their job role. AWS uses 'permission sets' and you assign AWS Policies. This approach makes it easy to give a group of users a set of relevant policies for their tasks. </p> <p>Our standard setup is to have two permission sets with the following policies: - IAM Identity Center --&gt; Permission sets --&gt; DataScientist     - Add Policy: arn:aws:iam::aws:policy/job-function/DataScientist</p> <ul> <li>IAM Identity Center --&gt; Permission sets --&gt; AdministratorAccess </li> <li>Add Policy: arn:aws:iam::aws:policy/job-function/AdministratorAccess</li> </ul> <p>See: Permission Sets for more details and instructions.</p> <p>Another benefit of creating groups is that you can include that group in 'Trust Policy (assume_role)' for the SageWorks-ExecutionRole (this gets deployed as part of the SageWorks AWS Stack). This means that the management of what SageWorks can do/see/read/write is completely done through the SageWorks-ExecutionRole.</p>"},{"location":"aws_setup/aws_tips_and_tricks/#back-to-adding-user","title":"Back to Adding User","text":"<p>Okay now that we have our groups set up we can go back to our original goal of adding a user. So here's the second panel with the groups and now we can hit 'Next'</p> <p></p> <p>On the third panel just review the details and hit the 'Add User' button at the bottom. The user will get an email giving them instructions on how to log on to their AWS account.</p> <p></p>"},{"location":"aws_setup/aws_tips_and_tricks/#aws-console","title":"AWS Console","text":"<p>Now when the user logs onto the AWS Console they should see something like this: </p>"},{"location":"aws_setup/aws_tips_and_tricks/#sso-setup-for-command-linepython-usage","title":"SSO Setup for Command Line/Python Usage","text":"<p>For full instructions see SSO Command Line/Python Configure. But here's a quick summary</p>"},{"location":"aws_setup/aws_tips_and_tricks/#get-some-information","title":"Get some information","text":"<ul> <li>Goto your AWS Identity Center in the AWS Console</li> <li>On the right side there will be two important pieces of information<ul> <li>Region</li> <li>Start URL</li> </ul> </li> </ul>"},{"location":"aws_setup/aws_tips_and_tricks/#install-aws-cli","title":"Install AWS CLI","text":"<ul> <li>Mac: <code>brew install awscli</code></li> <li>Linus: TBD</li> <li>Windows: TBD</li> </ul>"},{"location":"aws_setup/aws_tips_and_tricks/#running-the-sso-configuration","title":"Running the SSO Configuration","text":"<p>Note: You only need to do this once! <pre><code>aws configure sso --profile &lt;the name of the new profile&gt; (something like bob_sso)\nSSO session name (Recommended): my-sso\nSSO start URL []: &lt;the Start URL from info above&gt;\nSSO region []: &lt;the Region from info above&gt;\nSSO registration scopes [sso:account:access]:\n</code></pre></p> <p>You will get a browser open/redirect at this point and get a list of available accounts.. something like below, just pick the correct account</p> <pre><code>There are 2 AWS accounts available to you.\n&gt; SCP_Sandbox, briford+sandbox@supercowpowers.com (XXXX40646YYY)\n  SCP_Main, briford@supercowpowers.com (XXX576391YYY)\n</code></pre> <p>Now pick the role that you're going to use</p> <pre><code>There are 2 roles available to you.\n&gt; DataScientist\n  AdministratorAccess\n</code></pre>"},{"location":"aws_setup/aws_tips_and_tricks/#setting-up-some-aliases-for-bashzsh","title":"Setting up some aliases for bash/zsh","text":"<p>Edit your favorite ~/.bashrc ~/.zshrc and add these nice aliases/helper</p> <pre><code># AWS Aliases\nalias bob_sso='export AWS_PROFILE=bob_sso'\n\n# Default AWS Profile\nexport AWS_PROFILE=bob_sso\n</code></pre>"},{"location":"aws_setup/aws_tips_and_tricks/#testing-your-new-aws-profile","title":"Testing your new AWS Profile","text":"<p>Make sure your profile is active/set <pre><code>env | grep AWS\nAWS_PROFILE=&lt;bob_sso or whatever&gt;\n</code></pre> Now you can list the S3 buckets in the AWS Account <pre><code>aws ls s3\n</code></pre> If you get some message like this...</p> <p><code>The SSO session associated with this profile has expired or is otherwise invalid. To refresh this SSO session run aws sso login with the corresponding profile.</code></p> <p>This is fine/good, a browser will open up and you can refresh your SSO Token.</p> <p>After that you should get a listing of the S3 buckets without needed to refresh your token.</p> <pre><code>aws s3 ls\n\u276f aws s3 ls\n2023-03-20 20:06:53 aws-athena-query-results-XXXYYY-us-west-2\n2023-03-30 13:22:28 sagemaker-studio-XXXYYY-dbgyvq8ruka\n2023-03-24 22:05:55 sagemaker-us-west-2-XXXYYY\n2023-04-30 13:43:29 scp-sageworks-artifacts\n</code></pre>"},{"location":"aws_setup/aws_tips_and_tricks/#aws-resources","title":"AWS Resources","text":"<ul> <li>AWS Identity Center</li> <li>Users and Groups</li> <li>Permission Sets</li> <li>SSO Command Line/Python Configure</li> </ul>"},{"location":"aws_setup/core_stack/","title":"Initial AWS Setup","text":"<p>Welcome to the SageWorks AWS Setup Guide. SageWorks is deployed as an AWS Stack following the well architected system practices of AWS. </p> <p>AWS Setup can be a bit complex</p> <p>Setting up SageWorks with AWS can be a bit complex, but you only have to do it ONCE and SageWorks tries to make it straight forward. If you have any troubles at all feel free to contact us a sageworks@supercowpowers.com or on Discord and we're happy to help you with AWS for FREE.</p>"},{"location":"aws_setup/core_stack/#two-main-options-when-using-sageworks","title":"Two main options when using SageWorks","text":"<ol> <li>Spin up a new AWS Account for the SageWorks Stacks (Make a New Account)</li> <li>Deploy SageWorks Stacks into your existing AWS Account</li> </ol> <p>Either of these options are fully supported, but we highly suggest a NEW account as it gives the following benefits:</p> <ul> <li>AWS Data Isolation: Data Scientists will feel empowered to play in the sandbox without impacting production services.</li> <li>AWS Cost Accounting: Monitor and Track all those new ML Pipelines that your team creates with SageWorks :)</li> </ul>"},{"location":"aws_setup/core_stack/#setting-up-users-and-groups","title":"Setting up Users and Groups","text":"<p>If your AWS Account already has users and groups set up you can skip this but here's our recommendations on setting up SSO Users and Groups</p>"},{"location":"aws_setup/core_stack/#onboarding-sageworks-to-your-aws-account","title":"Onboarding SageWorks to your AWS Account","text":"<p>Pulling down the SageWorks Repo   <pre><code>git clone https://github.com/SuperCowPowers/sageworks.git\n</code></pre></p>"},{"location":"aws_setup/core_stack/#sageworks-uses-aws-python-cdk-for-deployments","title":"SageWorks uses AWS Python CDK for Deployments","text":"<p>If you don't have AWS CDK already installed you can do these steps:</p> <p>Mac</p> <p><pre><code>brew install node \nnpm install -g aws-cdk\n</code></pre> Linux</p> <p><pre><code>sudo apt install nodejs\nsudo npm install -g aws-cdk\n</code></pre> For more information on Linux installs see Digital Ocean NodeJS</p>"},{"location":"aws_setup/core_stack/#create-an-s3-bucket-for-sageworks","title":"Create an S3 Bucket for SageWorks","text":"<p>SageWorks pushes and pulls data from AWS, it will use this S3 Bucket for storage and processing. You should create a NEW S3 Bucket, we suggest a name like <code>&lt;company_name/url&gt;-sageworks</code></p>"},{"location":"aws_setup/core_stack/#deploying-the-sageworks-core-stack","title":"Deploying the SageWorks Core Stack","text":"<p>AWS Stuff</p> <p>Activate your AWS Account that's used for SageWorks deployment. For this one time install you should use an Admin Account (or an account that had permissions to create/update AWS Stacks)</p> <pre><code>cd sageworks/aws_setup/sageworks_core\nexport AWS_PROFLE=&lt;aws_admin_account&gt;\nexport SAGEWORKS_BUCKET=&lt;name of your S3 bucket&gt;\n(optional) export SAGEWORKS_SSO_GROUP=&lt;your SSO group&gt;\npip install -r requirements.txt\ncdk bootstrap\ncdk deploy\n</code></pre>"},{"location":"aws_setup/core_stack/#aws-account-setup-check","title":"AWS Account Setup Check","text":"<p>After setting up SageWorks config/AWS Account you can run this test/checking script. If the results ends with <code>INFO AWS Account Clamp: AOK!</code> you're in good shape. If not feel free to contact us on Discord and we'll get it straightened out for you :)</p> <pre><code>pip install sageworks\ncd sageworks/aws_setup\npython aws_account_check.py\n&lt;lot of print outs for various checks&gt;\n2023-04-12 11:17:09 (aws_account_check.py:48) INFO AWS Account Clamp: AOK!\n</code></pre>"},{"location":"aws_setup/core_stack/#building-our-first-ml-pipeline","title":"Building our first ML Pipeline","text":"<p>Okay, now the more significant testing. We're literally going to build an entire AWS ML Pipeline. The script <code>build_ml_pipeline.py</code> uses the SageWorks API to quickly and easily build an AWS Modeling Pipeline. - DataLoader(abalone.csv) --&gt; DataSource - DataToFeatureSet Transform --&gt; FeatureSet - FeatureSetToModel Transform --&gt; Model - ModelToEndpoint Transform --&gt; Endpoint</p> <p>This script will take a LONG TiME to run, most of the time is waiting on AWS to finalize FeatureGroup population.</p> <p><pre><code>\u276f python build_ml_pipeline.py\n&lt;lot of building ML pipeline outputs&gt;\n</code></pre> After the script completes you will see that it's built out an AWS ML Pipeline and testing artifacts.</p>"},{"location":"aws_setup/core_stack/#run-the-sageworks-dashboard-local","title":"Run the SageWorks Dashboard (Local)","text":"<p>Dashboard AWS Stack</p> <p>Deploying the Dashboard Stack is straight-forward and provides a robust AWS Web Server with Load Balancer, Elastic Container Service, VPC Networks, etc. (see AWS Dashboard Stack)</p> <p>For testing it's nice to run the Dashboard locally, but for longterm use the SageWorks Dashboard should be deployed as an AWS Stack. The deployed Stack allows everyone in the company to use, view, and interact with the AWS Machine Learning Artifacts created with SageWorks.</p> <p><pre><code>cd sageworks/application/aws_dashboard\n./dashboard\n</code></pre> This will open a browser to http://localhost:8000</p> <p> SageWorks Dashboard: AWS Pipelines in a Whole New Light! <p>Success</p> <p>Congratulations: SageWorks is now deployed to your AWS Account. Deploying the AWS Stack only needs to be done once. Now that this is complete your developers can simply <code>pip install sageworks</code> and start using the API.</p> <p>If you ran into any issues with this procedure please contact us via Discord or email sageworks@supercowpowers.com and the SCP team will provide free setup and support for new SageWorks users.</p>"},{"location":"aws_setup/dashboard_stack/","title":"Deploy the SageWorks Dashboard Stack","text":"<p>Deploying the Dashboard Stack is reasonably straight forward, it's the same approach as the Core Stack that you've already deployed.</p> <p>Please review the Stack Details section to understand all the AWS components that are included and utilized in the SageWorks Dashboard Stack.</p>"},{"location":"aws_setup/dashboard_stack/#deploying-the-dashboard-stack","title":"Deploying the Dashboard Stack","text":"<p>AWS Stuff</p> <p>Activate your AWS Account that's used for SageWorks deployment. For this one time install you should use an Admin Account (or an account that had permissions to create/update AWS Stacks)</p> <pre><code>cd sageworks/aws_setup/sageworks_dashboard_full\nexport AWS_PROFLE=&lt;aws_admin_account&gt;\nexport SAGEWORKS_BUCKET=&lt;name of your S3 bucket&gt;\n(optional) export SAGEWORKS_SSO_GROUP=&lt;your SSO group&gt;\npip install -r requirements.txt\ncdk bootstrap\ncdk deploy\n</code></pre>"},{"location":"aws_setup/dashboard_stack/#stack-details","title":"Stack Details","text":"<p>AWS Questions?</p> <p>There's quite a bit to unpack when deploying an AWS powered Web Service. We're happy to help walk you through the details and options. Contact us anytime for a free consultation.</p> <ul> <li>ECS Fargate</li> <li>Load Balancer</li> <li>2 Availability Zones</li> <li>4 VPC (2 public and 2 private)</li> <li>2 Nat Gateways</li> <li>ElasticCache Cluster (shared Redis Caching)</li> </ul>"},{"location":"aws_setup/dashboard_stack/#pros","title":"Pros","text":"<ol> <li>Scalability: Includes an Application Load Balancer and uses ECS with Fargate, and ElasticCache for more robust scaling options.</li> <li>Higher Security: Utilizes security groups for both the ECS tasks, load balancer, plus VPC private subnets for Redis and the utilization of NAT Gateways.</li> </ol> <p>AWS Costs</p> <p>Deploying the SageWorks Dashboard does incur some monthly AWS costs. If you'd prefer a simpler, less costly, deployment please contact us at sageworks@supercowpowers.com or on chat us up on Discord</p>"},{"location":"aws_setup/dashboard_stack/#cons","title":"Cons","text":"<ol> <li>Cost: This setup uses additional AWS services like Elasticache, Load Balancer, and NAT Gateways.</li> </ol>"},{"location":"core_classes/overview/","title":"Core Classes","text":"<p>SageWorks Core Classes</p> <p>These classes interact with many of the AWS service details and are therefore more complex. They provide additional control and refinement over the AWS ML Pipline. For most use cases the API Classes should be used</p> <p>Welcome to the SageWorks Core Classes</p> <p>The Core Classes provide low-level APIs for the SageWorks package, these classes directly interface with the AWS Sagemaker Pipeline interfaces and have a large number of methods with reasonable complexity.</p> <p>The API Classes have method pass-through so just call the method on the API Class and voil\u00e0 it works the same.</p> <p></p>"},{"location":"core_classes/overview/#artifacts","title":"Artifacts","text":"<ul> <li>AthenaSource: Manages AWS Data Catalog and Athena</li> <li>FeatureSetCore: Manages AWS Feature Store and Feature Groups</li> <li>ModelCore: Manages the training and deployment of AWS Model Groups and Packages</li> <li>EndpointCore: Manages the deployment and invocations/inference on AWS Endpoints</li> </ul>"},{"location":"core_classes/overview/#transforms","title":"Transforms","text":"<p>Transforms are a set of classes that transform one type of <code>Artifact</code> to another type. For instance <code>DataToFeatureSet</code> takes a <code>DataSource</code> artifact and creates a <code>FeatureSet</code> artifact.</p> <ul> <li>DataLoaders Light: Loads various light/smaller data into AWS Data Catalog and Athena</li> <li>DataLoaders Heavy: Loads heavy/larger data (via Glue) into AWS Data Catalog and Athena</li> <li>DataToFeatures: Transforms a DataSource into a FeatureSet (AWS Feature Store/Group)</li> <li>FeaturesToModel: Trains and deploys an AWS Model Package/Group from a FeatureSet</li> <li>ModelToEndpoint: Manages the provisioning and deployment of a Model Endpoint</li> <li>PandasTransforms:Pandas DataFrame transforms and helper methods.</li> </ul>"},{"location":"core_classes/artifacts/athena_source/","title":"AthenaSource","text":"<p>API Classes</p> <p>Found a method here you want to use? The API Classes have method pass-through so just call the method on the DataSource API Class and voil\u00e0 it works the same.</p> <p>AthenaSource: SageWorks Data Source accessible through Athena</p>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource","title":"<code>AthenaSource</code>","text":"<p>             Bases: <code>DataSourceAbstract</code></p> <p>AthenaSource: SageWorks Data Source accessible through Athena</p> Common Usage <pre><code>my_data = AthenaSource(data_uuid, database=\"sageworks\")\nmy_data.summary()\nmy_data.details()\ndf = my_data.query(f\"select * from {data_uuid} limit 5\")\n</code></pre> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>class AthenaSource(DataSourceAbstract):\n    \"\"\"AthenaSource: SageWorks Data Source accessible through Athena\n\n    Common Usage:\n        ```\n        my_data = AthenaSource(data_uuid, database=\"sageworks\")\n        my_data.summary()\n        my_data.details()\n        df = my_data.query(f\"select * from {data_uuid} limit 5\")\n        ```\n    \"\"\"\n\n    def __init__(self, data_uuid, database=\"sageworks\", force_refresh: bool = False):\n        \"\"\"AthenaSource Initialization\n\n        Args:\n            data_uuid (str): Name of Athena Table\n            database (str): Athena Database Name (default: sageworks)\n            force_refresh (bool): Force refresh of AWS Metadata (default: False)\n        \"\"\"\n\n        # Call superclass init\n        super().__init__(data_uuid, database)\n\n        # Setup our AWS Broker catalog metadata\n        _catalog_meta = self.aws_broker.get_metadata(ServiceCategory.DATA_CATALOG, force_refresh=force_refresh)\n        try:\n            self.catalog_table_meta = _catalog_meta[self.get_database()].get(self.get_table_name())\n        except KeyError:\n            self.log.critical(f\"Unable to find {self.get_database()} in Catalogs...\")\n            self.log.critical(\"You must run the sageworks/aws_setup/aws_account_check.py script\")\n            raise RuntimeError(\"Unable to find {self.get_database()} in Catalogs...\")\n\n        # Call superclass post init\n        super().__post_init__()\n\n        # All done\n        self.log.debug(f\"AthenaSource Initialized: {self.get_database()}.{self.get_table_name()}\")\n\n    def refresh_meta(self):\n        \"\"\"Refresh our internal AWS Broker catalog metadata\"\"\"\n        _catalog_meta = self.aws_broker.get_metadata(ServiceCategory.DATA_CATALOG, force_refresh=True)\n        self.catalog_table_meta = _catalog_meta[self.get_database()].get(self.get_table_name())\n\n    def exists(self) -&gt; bool:\n        \"\"\"Validation Checks for this Data Source\"\"\"\n\n        # We're we able to pull AWS Metadata for this table_name?\"\"\"\n        if self.catalog_table_meta is None:\n            self.log.debug(f\"AthenaSource {self.get_table_name()} not found in SageWorks Metadata...\")\n            return False\n        return True\n\n    def arn(self) -&gt; str:\n        \"\"\"AWS ARN (Amazon Resource Name) for this artifact\"\"\"\n        # Grab our SageWorks Role Manager, get our AWS account id, and region for ARN creation\n        account_id = self.aws_account_clamp.account_id\n        region = self.aws_account_clamp.region\n        arn = f\"arn:aws:glue:{region}:{account_id}:table/{self.get_database()}/{self.get_table_name()}\"\n        return arn\n\n    @trace_calls\n    def sageworks_meta(self) -&gt; dict:\n        \"\"\"Get the SageWorks specific metadata for this Artifact\"\"\"\n\n        # Sanity Check if we have invalid AWS Metadata\n        self.log.info(f\"Retrieving SageWorks Metadata for Artifact: {self.uuid}...\")\n        if self.catalog_table_meta is None:\n            if not self.exists():\n                self.log.error(f\"DataSource {self.uuid} doesn't appear to exist...\")\n            else:\n                self.log.critical(f\"Unable to get AWS Metadata for {self.get_table_name()}\")\n                self.log.critical(\"Malformed Artifact! Delete this Artifact and recreate it!\")\n            return {}\n\n        # Get the SageWorks Metadata from the Catalog Table Metadata\n        return sageworks_meta_from_catalog_table_meta(self.catalog_table_meta)\n\n    def upsert_sageworks_meta(self, new_meta: dict):\n        \"\"\"Add SageWorks specific metadata to this Artifact\n\n        Args:\n            new_meta (dict): Dictionary of new metadata to add\n        \"\"\"\n\n        # Give a warning message for keys that don't start with sageworks_\n        for key in new_meta.keys():\n            if not key.startswith(\"sageworks_\"):\n                self.log.warning(\"Append 'sageworks_' to key names to avoid overwriting AWS meta data\")\n\n        # Now convert any non-string values to JSON strings\n        for key, value in new_meta.items():\n            if not isinstance(value, str):\n                new_meta[key] = json.dumps(value, cls=CustomEncoder)\n\n        # Store our updated metadata\n        try:\n            wr.catalog.upsert_table_parameters(\n                parameters=new_meta,\n                database=self.get_database(),\n                table=self.get_table_name(),\n                boto3_session=self.boto_session,\n            )\n        except botocore.exceptions.ClientError as e:\n            error_code = e.response[\"Error\"][\"Code\"]\n            if error_code == \"InvalidInputException\":\n                self.log.error(f\"Unable to upsert metadata for {self.get_table_name()}\")\n                self.log.error(\"Probably because the metadata is too large\")\n                self.log.error(new_meta)\n            elif error_code == \"ConcurrentModificationException\":\n                self.log.warning(\"ConcurrentModificationException... trying again...\")\n                time.sleep(1)\n                wr.catalog.upsert_table_parameters(\n                    parameters=new_meta,\n                    database=self.get_database(),\n                    table=self.get_table_name(),\n                    boto3_session=self.boto_session,\n                )\n            else:\n                raise e\n\n    def size(self) -&gt; float:\n        \"\"\"Return the size of this data in MegaBytes\"\"\"\n        size_in_bytes = sum(wr.s3.size_objects(self.s3_storage_location(), boto3_session=self.boto_session).values())\n        size_in_mb = size_in_bytes / 1_000_000\n        return size_in_mb\n\n    def aws_meta(self) -&gt; dict:\n        \"\"\"Get the FULL AWS metadata for this artifact\"\"\"\n        return self.catalog_table_meta\n\n    def aws_url(self):\n        \"\"\"The AWS URL for looking at/querying this data source\"\"\"\n        sageworks_details = self.sageworks_meta().get(\"sageworks_details\", \"{}\")\n        return sageworks_details.get(\"aws_url\", \"unknown\")\n\n    def created(self) -&gt; datetime:\n        \"\"\"Return the datetime when this artifact was created\"\"\"\n        return self.catalog_table_meta[\"CreateTime\"]\n\n    def modified(self) -&gt; datetime:\n        \"\"\"Return the datetime when this artifact was last modified\"\"\"\n        return self.catalog_table_meta[\"UpdateTime\"]\n\n    def num_rows(self) -&gt; int:\n        \"\"\"Return the number of rows for this Data Source\"\"\"\n        count_df = self.query(f'select count(*) AS count from \"{self.get_database()}\".\"{self.get_table_name()}\"')\n        return count_df[\"count\"][0]\n\n    def num_columns(self) -&gt; int:\n        \"\"\"Return the number of columns for this Data Source\"\"\"\n        return len(self.column_names())\n\n    def column_names(self) -&gt; list[str]:\n        \"\"\"Return the column names for this Athena Table\"\"\"\n        return [item[\"Name\"] for item in self.catalog_table_meta[\"StorageDescriptor\"][\"Columns\"]]\n\n    def column_types(self) -&gt; list[str]:\n        \"\"\"Return the column types of the internal AthenaSource\"\"\"\n        return [item[\"Type\"] for item in self.catalog_table_meta[\"StorageDescriptor\"][\"Columns\"]]\n\n    def query(self, query: str) -&gt; pd.DataFrame:\n        \"\"\"Query the AthenaSource\"\"\"\n        df = wr.athena.read_sql_query(\n            sql=query,\n            database=self.get_database(),\n            ctas_approach=False,\n            boto3_session=self.boto_session,\n        )\n        scanned_bytes = df.query_metadata[\"Statistics\"][\"DataScannedInBytes\"]\n        if scanned_bytes &gt; 0:\n            self.log.info(f\"Athena Query successful (scanned bytes: {scanned_bytes})\")\n        return df\n\n    def execute_statement(self, query: str):\n        \"\"\"Execute a non-returning SQL statement in Athena.\"\"\"\n        try:\n            # Start the query execution\n            query_execution_id = wr.athena.start_query_execution(\n                sql=query,\n                database=self.get_database(),\n                boto3_session=self.boto_session,\n            )\n            self.log.debug(f\"QueryExecutionId: {query_execution_id}\")\n\n            # Wait for the query to complete\n            wr.athena.wait_query(query_execution_id=query_execution_id, boto3_session=self.boto_session)\n            self.log.debug(f\"Statement executed successfully: {query_execution_id}\")\n        except Exception as e:\n            self.log.error(f\"Failed to execute statement: {e}\")\n            raise\n\n    def s3_storage_location(self) -&gt; str:\n        \"\"\"Get the S3 Storage Location for this Data Source\"\"\"\n        return self.catalog_table_meta[\"StorageDescriptor\"][\"Location\"]\n\n    def athena_test_query(self):\n        \"\"\"Validate that Athena Queries are working\"\"\"\n        query = f\"select count(*) as count from {self.get_table_name()}\"\n        df = wr.athena.read_sql_query(\n            sql=query,\n            database=self.get_database(),\n            ctas_approach=False,\n            boto3_session=self.boto_session,\n        )\n        scanned_bytes = df.query_metadata[\"Statistics\"][\"DataScannedInBytes\"]\n        self.log.info(f\"Athena TEST Query successful (scanned bytes: {scanned_bytes})\")\n\n    def sample_impl(self) -&gt; pd.DataFrame:\n        \"\"\"Pull a sample of rows from the DataSource\n\n        Returns:\n            pd.DataFrame: A sample DataFrame for an Athena DataSource\n        \"\"\"\n\n        # Call the SQL function to pull a sample of the rows\n        return sample_rows.sample_rows(self)\n\n    def descriptive_stats(self, recompute: bool = False) -&gt; dict[dict]:\n        \"\"\"Compute Descriptive Stats for all the numeric columns in a DataSource\n\n        Args:\n            recompute (bool): Recompute the descriptive stats (default: False)\n\n        Returns:\n            dict(dict): A dictionary of descriptive stats for each column in the form\n                 {'col1': {'min': 0, 'q1': 1, 'median': 2, 'q3': 3, 'max': 4},\n                  'col2': ...}\n        \"\"\"\n\n        # First check if we have already computed the descriptive stats\n        stat_dict_json = self.sageworks_meta().get(\"sageworks_descriptive_stats\")\n        if stat_dict_json and not recompute:\n            return stat_dict_json\n\n        # Call the SQL function to compute descriptive stats\n        stat_dict = descriptive_stats.descriptive_stats(self)\n\n        # Push the descriptive stat data into our DataSource Metadata\n        self.upsert_sageworks_meta({\"sageworks_descriptive_stats\": stat_dict})\n\n        # Return the descriptive stats\n        return stat_dict\n\n    def outliers_impl(self, scale: float = 1.5, use_stddev=False, recompute: bool = False) -&gt; pd.DataFrame:\n        \"\"\"Compute outliers for all the numeric columns in a DataSource\n\n        Args:\n            scale (float): The scale to use for the IQR (default: 1.5)\n            use_stddev (bool): Use Standard Deviation instead of IQR (default: False)\n            recompute (bool): Recompute the outliers (default: False)\n\n        Returns:\n            pd.DataFrame: A DataFrame of outliers from this DataSource\n\n        Notes:\n            Uses the IQR * 1.5 (~= 2.5 Sigma) (use 1.7 for ~= 3 Sigma)\n            The scale parameter can be adjusted to change the IQR multiplier\n        \"\"\"\n\n        # Compute outliers using the SQL Outliers class\n        sql_outliers = outliers.Outliers()\n        return sql_outliers.compute_outliers(self, scale=scale, use_stddev=use_stddev)\n\n    def smart_sample(self) -&gt; pd.DataFrame:\n        \"\"\"Get a smart sample dataframe for this DataSource\n\n        Note:\n            smart = sample data + outliers for the DataSource\"\"\"\n\n        # Outliers DataFrame\n        outlier_rows = self.outliers()\n\n        # Sample DataFrame\n        sample_rows = self.sample()\n        sample_rows[\"outlier_group\"] = \"sample\"\n\n        # Combine the sample rows with the outlier rows\n        all_rows = pd.concat([outlier_rows, sample_rows]).reset_index(drop=True)\n\n        # Drop duplicates\n        all_except_outlier_group = [col for col in all_rows.columns if col != \"outlier_group\"]\n        all_rows = all_rows.drop_duplicates(subset=all_except_outlier_group, ignore_index=True)\n        return all_rows\n\n    def correlations(self, recompute: bool = False) -&gt; dict[dict]:\n        \"\"\"Compute Correlations for all the numeric columns in a DataSource\n\n        Args:\n            recompute (bool): Recompute the column stats (default: False)\n\n        Returns:\n            dict(dict): A dictionary of correlations for each column in this format\n                 {'col1': {'col2': 0.5, 'col3': 0.9, 'col4': 0.4, ...},\n                  'col2': {'col1': 0.5, 'col3': 0.8, 'col4': 0.3, ...}}\n        \"\"\"\n\n        # First check if we have already computed the correlations\n        correlations_dict = self.sageworks_meta().get(\"sageworks_correlations\")\n        if correlations_dict and not recompute:\n            return correlations_dict\n\n        # Call the SQL function to compute correlations\n        correlations_dict = correlations.correlations(self)\n\n        # Push the correlation data into our DataSource Metadata\n        self.upsert_sageworks_meta({\"sageworks_correlations\": correlations_dict})\n\n        # Return the correlation data\n        return correlations_dict\n\n    def column_stats(self, recompute: bool = False) -&gt; dict[dict]:\n        \"\"\"Compute Column Stats for all the columns in a DataSource\n\n        Args:\n            recompute (bool): Recompute the column stats (default: False)\n\n        Returns:\n            dict(dict): A dictionary of stats for each column this format\n            NB: String columns will NOT have num_zeros, descriptive_stats or correlation data\n                {'col1': {'dtype': 'string', 'unique': 4321, 'nulls': 12},\n                 'col2': {'dtype': 'int', 'unique': 4321, 'nulls': 12, 'num_zeros': 100,\n                          'descriptive_stats': {...}, 'correlations': {...}},\n                 ...}\n        \"\"\"\n\n        # First check if we have already computed the column stats\n        columns_stats_dict = self.sageworks_meta().get(\"sageworks_column_stats\")\n        if columns_stats_dict and not recompute:\n            return columns_stats_dict\n\n        # Call the SQL function to compute column stats\n        column_stats_dict = column_stats.column_stats(self, recompute=recompute)\n\n        # Push the column stats data into our DataSource Metadata\n        self.upsert_sageworks_meta({\"sageworks_column_stats\": column_stats_dict})\n\n        # Return the column stats data\n        return column_stats_dict\n\n    def value_counts(self, recompute: bool = False) -&gt; dict[dict]:\n        \"\"\"Compute 'value_counts' for all the string columns in a DataSource\n\n        Args:\n            recompute (bool): Recompute the value counts (default: False)\n\n        Returns:\n            dict(dict): A dictionary of value counts for each column in the form\n                 {'col1': {'value_1': 42, 'value_2': 16, 'value_3': 9,...},\n                  'col2': ...}\n        \"\"\"\n\n        # First check if we have already computed the value counts\n        value_counts_dict = self.sageworks_meta().get(\"sageworks_value_counts\")\n        if value_counts_dict and not recompute:\n            return value_counts_dict\n\n        # Call the SQL function to compute value_counts\n        value_count_dict = value_counts.value_counts(self)\n\n        # Push the value_count data into our DataSource Metadata\n        self.upsert_sageworks_meta({\"sageworks_value_counts\": value_count_dict})\n\n        # Return the value_count data\n        return value_count_dict\n\n    def details(self, recompute: bool = False) -&gt; dict[dict]:\n        \"\"\"Additional Details about this AthenaSource Artifact\n\n        Args:\n            recompute (bool): Recompute the details (default: False)\n\n        Returns:\n            dict(dict): A dictionary of details about this AthenaSource\n        \"\"\"\n\n        # Check if we have cached version of the DataSource Details\n        storage_key = f\"data_source:{self.uuid}:details\"\n        cached_details = self.data_storage.get(storage_key)\n        if cached_details and not recompute:\n            return cached_details\n\n        self.log.info(f\"Recomputing DataSource Details ({self.uuid})...\")\n\n        # Get the details from the base class\n        details = super().details()\n\n        # Compute additional details\n        details[\"s3_storage_location\"] = self.s3_storage_location()\n        details[\"storage_type\"] = \"athena\"\n\n        # Compute our AWS URL\n        query = f\"select * from {self.get_database()}.{self.get_table_name()} limit 10\"\n        query_exec_id = wr.athena.start_query_execution(\n            sql=query, database=self.get_database(), boto3_session=self.boto_session\n        )\n        base_url = \"https://console.aws.amazon.com/athena/home\"\n        details[\"aws_url\"] = f\"{base_url}?region={self.aws_region}#query/history/{query_exec_id}\"\n\n        # Push the aws_url data into our DataSource Metadata\n        self.upsert_sageworks_meta({\"sageworks_details\": {\"aws_url\": details[\"aws_url\"]}})\n\n        # Convert any datetime fields to ISO-8601 strings\n        details = convert_all_to_iso8601(details)\n\n        # Add the column stats\n        details[\"column_stats\"] = self.column_stats()\n\n        # Cache the details\n        self.data_storage.set(storage_key, details)\n\n        # Return the details data\n        return details\n\n    def delete(self):\n        \"\"\"Delete the AWS Data Catalog Table and S3 Storage Objects\"\"\"\n\n        # Make sure the Feature Group exists\n        if not self.exists():\n            self.log.warning(f\"Trying to delete a AthenaSource that doesn't exist: {self.get_table_name()}\")\n\n        # Delete Data Catalog Table\n        self.log.info(f\"Deleting DataCatalog Table: {self.get_database()}.{self.get_table_name()}...\")\n        wr.catalog.delete_table_if_exists(self.get_database(), self.get_table_name(), boto3_session=self.boto_session)\n\n        # Delete S3 Storage Objects (if they exist)\n        try:\n            self.log.info(f\"Deleting S3 Storage Object: {self.s3_storage_location()}...\")\n            wr.s3.delete_objects(self.s3_storage_location(), boto3_session=self.boto_session)\n        except TypeError:\n            self.log.warning(\"Malformed Artifact... good thing it's being deleted...\")\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.__init__","title":"<code>__init__(data_uuid, database='sageworks', force_refresh=False)</code>","text":"<p>AthenaSource Initialization</p> <p>Parameters:</p> Name Type Description Default <code>data_uuid</code> <code>str</code> <p>Name of Athena Table</p> required <code>database</code> <code>str</code> <p>Athena Database Name (default: sageworks)</p> <code>'sageworks'</code> <code>force_refresh</code> <code>bool</code> <p>Force refresh of AWS Metadata (default: False)</p> <code>False</code> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def __init__(self, data_uuid, database=\"sageworks\", force_refresh: bool = False):\n    \"\"\"AthenaSource Initialization\n\n    Args:\n        data_uuid (str): Name of Athena Table\n        database (str): Athena Database Name (default: sageworks)\n        force_refresh (bool): Force refresh of AWS Metadata (default: False)\n    \"\"\"\n\n    # Call superclass init\n    super().__init__(data_uuid, database)\n\n    # Setup our AWS Broker catalog metadata\n    _catalog_meta = self.aws_broker.get_metadata(ServiceCategory.DATA_CATALOG, force_refresh=force_refresh)\n    try:\n        self.catalog_table_meta = _catalog_meta[self.get_database()].get(self.get_table_name())\n    except KeyError:\n        self.log.critical(f\"Unable to find {self.get_database()} in Catalogs...\")\n        self.log.critical(\"You must run the sageworks/aws_setup/aws_account_check.py script\")\n        raise RuntimeError(\"Unable to find {self.get_database()} in Catalogs...\")\n\n    # Call superclass post init\n    super().__post_init__()\n\n    # All done\n    self.log.debug(f\"AthenaSource Initialized: {self.get_database()}.{self.get_table_name()}\")\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.arn","title":"<code>arn()</code>","text":"<p>AWS ARN (Amazon Resource Name) for this artifact</p> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def arn(self) -&gt; str:\n    \"\"\"AWS ARN (Amazon Resource Name) for this artifact\"\"\"\n    # Grab our SageWorks Role Manager, get our AWS account id, and region for ARN creation\n    account_id = self.aws_account_clamp.account_id\n    region = self.aws_account_clamp.region\n    arn = f\"arn:aws:glue:{region}:{account_id}:table/{self.get_database()}/{self.get_table_name()}\"\n    return arn\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.athena_test_query","title":"<code>athena_test_query()</code>","text":"<p>Validate that Athena Queries are working</p> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def athena_test_query(self):\n    \"\"\"Validate that Athena Queries are working\"\"\"\n    query = f\"select count(*) as count from {self.get_table_name()}\"\n    df = wr.athena.read_sql_query(\n        sql=query,\n        database=self.get_database(),\n        ctas_approach=False,\n        boto3_session=self.boto_session,\n    )\n    scanned_bytes = df.query_metadata[\"Statistics\"][\"DataScannedInBytes\"]\n    self.log.info(f\"Athena TEST Query successful (scanned bytes: {scanned_bytes})\")\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.aws_meta","title":"<code>aws_meta()</code>","text":"<p>Get the FULL AWS metadata for this artifact</p> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def aws_meta(self) -&gt; dict:\n    \"\"\"Get the FULL AWS metadata for this artifact\"\"\"\n    return self.catalog_table_meta\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.aws_url","title":"<code>aws_url()</code>","text":"<p>The AWS URL for looking at/querying this data source</p> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def aws_url(self):\n    \"\"\"The AWS URL for looking at/querying this data source\"\"\"\n    sageworks_details = self.sageworks_meta().get(\"sageworks_details\", \"{}\")\n    return sageworks_details.get(\"aws_url\", \"unknown\")\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.column_names","title":"<code>column_names()</code>","text":"<p>Return the column names for this Athena Table</p> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def column_names(self) -&gt; list[str]:\n    \"\"\"Return the column names for this Athena Table\"\"\"\n    return [item[\"Name\"] for item in self.catalog_table_meta[\"StorageDescriptor\"][\"Columns\"]]\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.column_stats","title":"<code>column_stats(recompute=False)</code>","text":"<p>Compute Column Stats for all the columns in a DataSource</p> <p>Parameters:</p> Name Type Description Default <code>recompute</code> <code>bool</code> <p>Recompute the column stats (default: False)</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of stats for each column this format</p> <code>NB</code> <code>dict[dict]</code> <p>String columns will NOT have num_zeros, descriptive_stats or correlation data {'col1': {'dtype': 'string', 'unique': 4321, 'nulls': 12},  'col2': {'dtype': 'int', 'unique': 4321, 'nulls': 12, 'num_zeros': 100,           'descriptive_stats': {...}, 'correlations': {...}},  ...}</p> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def column_stats(self, recompute: bool = False) -&gt; dict[dict]:\n    \"\"\"Compute Column Stats for all the columns in a DataSource\n\n    Args:\n        recompute (bool): Recompute the column stats (default: False)\n\n    Returns:\n        dict(dict): A dictionary of stats for each column this format\n        NB: String columns will NOT have num_zeros, descriptive_stats or correlation data\n            {'col1': {'dtype': 'string', 'unique': 4321, 'nulls': 12},\n             'col2': {'dtype': 'int', 'unique': 4321, 'nulls': 12, 'num_zeros': 100,\n                      'descriptive_stats': {...}, 'correlations': {...}},\n             ...}\n    \"\"\"\n\n    # First check if we have already computed the column stats\n    columns_stats_dict = self.sageworks_meta().get(\"sageworks_column_stats\")\n    if columns_stats_dict and not recompute:\n        return columns_stats_dict\n\n    # Call the SQL function to compute column stats\n    column_stats_dict = column_stats.column_stats(self, recompute=recompute)\n\n    # Push the column stats data into our DataSource Metadata\n    self.upsert_sageworks_meta({\"sageworks_column_stats\": column_stats_dict})\n\n    # Return the column stats data\n    return column_stats_dict\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.column_types","title":"<code>column_types()</code>","text":"<p>Return the column types of the internal AthenaSource</p> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def column_types(self) -&gt; list[str]:\n    \"\"\"Return the column types of the internal AthenaSource\"\"\"\n    return [item[\"Type\"] for item in self.catalog_table_meta[\"StorageDescriptor\"][\"Columns\"]]\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.correlations","title":"<code>correlations(recompute=False)</code>","text":"<p>Compute Correlations for all the numeric columns in a DataSource</p> <p>Parameters:</p> Name Type Description Default <code>recompute</code> <code>bool</code> <p>Recompute the column stats (default: False)</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of correlations for each column in this format  {'col1': {'col2': 0.5, 'col3': 0.9, 'col4': 0.4, ...},   'col2': {'col1': 0.5, 'col3': 0.8, 'col4': 0.3, ...}}</p> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def correlations(self, recompute: bool = False) -&gt; dict[dict]:\n    \"\"\"Compute Correlations for all the numeric columns in a DataSource\n\n    Args:\n        recompute (bool): Recompute the column stats (default: False)\n\n    Returns:\n        dict(dict): A dictionary of correlations for each column in this format\n             {'col1': {'col2': 0.5, 'col3': 0.9, 'col4': 0.4, ...},\n              'col2': {'col1': 0.5, 'col3': 0.8, 'col4': 0.3, ...}}\n    \"\"\"\n\n    # First check if we have already computed the correlations\n    correlations_dict = self.sageworks_meta().get(\"sageworks_correlations\")\n    if correlations_dict and not recompute:\n        return correlations_dict\n\n    # Call the SQL function to compute correlations\n    correlations_dict = correlations.correlations(self)\n\n    # Push the correlation data into our DataSource Metadata\n    self.upsert_sageworks_meta({\"sageworks_correlations\": correlations_dict})\n\n    # Return the correlation data\n    return correlations_dict\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.created","title":"<code>created()</code>","text":"<p>Return the datetime when this artifact was created</p> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def created(self) -&gt; datetime:\n    \"\"\"Return the datetime when this artifact was created\"\"\"\n    return self.catalog_table_meta[\"CreateTime\"]\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.delete","title":"<code>delete()</code>","text":"<p>Delete the AWS Data Catalog Table and S3 Storage Objects</p> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def delete(self):\n    \"\"\"Delete the AWS Data Catalog Table and S3 Storage Objects\"\"\"\n\n    # Make sure the Feature Group exists\n    if not self.exists():\n        self.log.warning(f\"Trying to delete a AthenaSource that doesn't exist: {self.get_table_name()}\")\n\n    # Delete Data Catalog Table\n    self.log.info(f\"Deleting DataCatalog Table: {self.get_database()}.{self.get_table_name()}...\")\n    wr.catalog.delete_table_if_exists(self.get_database(), self.get_table_name(), boto3_session=self.boto_session)\n\n    # Delete S3 Storage Objects (if they exist)\n    try:\n        self.log.info(f\"Deleting S3 Storage Object: {self.s3_storage_location()}...\")\n        wr.s3.delete_objects(self.s3_storage_location(), boto3_session=self.boto_session)\n    except TypeError:\n        self.log.warning(\"Malformed Artifact... good thing it's being deleted...\")\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.descriptive_stats","title":"<code>descriptive_stats(recompute=False)</code>","text":"<p>Compute Descriptive Stats for all the numeric columns in a DataSource</p> <p>Parameters:</p> Name Type Description Default <code>recompute</code> <code>bool</code> <p>Recompute the descriptive stats (default: False)</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of descriptive stats for each column in the form  {'col1': {'min': 0, 'q1': 1, 'median': 2, 'q3': 3, 'max': 4},   'col2': ...}</p> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def descriptive_stats(self, recompute: bool = False) -&gt; dict[dict]:\n    \"\"\"Compute Descriptive Stats for all the numeric columns in a DataSource\n\n    Args:\n        recompute (bool): Recompute the descriptive stats (default: False)\n\n    Returns:\n        dict(dict): A dictionary of descriptive stats for each column in the form\n             {'col1': {'min': 0, 'q1': 1, 'median': 2, 'q3': 3, 'max': 4},\n              'col2': ...}\n    \"\"\"\n\n    # First check if we have already computed the descriptive stats\n    stat_dict_json = self.sageworks_meta().get(\"sageworks_descriptive_stats\")\n    if stat_dict_json and not recompute:\n        return stat_dict_json\n\n    # Call the SQL function to compute descriptive stats\n    stat_dict = descriptive_stats.descriptive_stats(self)\n\n    # Push the descriptive stat data into our DataSource Metadata\n    self.upsert_sageworks_meta({\"sageworks_descriptive_stats\": stat_dict})\n\n    # Return the descriptive stats\n    return stat_dict\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.details","title":"<code>details(recompute=False)</code>","text":"<p>Additional Details about this AthenaSource Artifact</p> <p>Parameters:</p> Name Type Description Default <code>recompute</code> <code>bool</code> <p>Recompute the details (default: False)</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of details about this AthenaSource</p> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def details(self, recompute: bool = False) -&gt; dict[dict]:\n    \"\"\"Additional Details about this AthenaSource Artifact\n\n    Args:\n        recompute (bool): Recompute the details (default: False)\n\n    Returns:\n        dict(dict): A dictionary of details about this AthenaSource\n    \"\"\"\n\n    # Check if we have cached version of the DataSource Details\n    storage_key = f\"data_source:{self.uuid}:details\"\n    cached_details = self.data_storage.get(storage_key)\n    if cached_details and not recompute:\n        return cached_details\n\n    self.log.info(f\"Recomputing DataSource Details ({self.uuid})...\")\n\n    # Get the details from the base class\n    details = super().details()\n\n    # Compute additional details\n    details[\"s3_storage_location\"] = self.s3_storage_location()\n    details[\"storage_type\"] = \"athena\"\n\n    # Compute our AWS URL\n    query = f\"select * from {self.get_database()}.{self.get_table_name()} limit 10\"\n    query_exec_id = wr.athena.start_query_execution(\n        sql=query, database=self.get_database(), boto3_session=self.boto_session\n    )\n    base_url = \"https://console.aws.amazon.com/athena/home\"\n    details[\"aws_url\"] = f\"{base_url}?region={self.aws_region}#query/history/{query_exec_id}\"\n\n    # Push the aws_url data into our DataSource Metadata\n    self.upsert_sageworks_meta({\"sageworks_details\": {\"aws_url\": details[\"aws_url\"]}})\n\n    # Convert any datetime fields to ISO-8601 strings\n    details = convert_all_to_iso8601(details)\n\n    # Add the column stats\n    details[\"column_stats\"] = self.column_stats()\n\n    # Cache the details\n    self.data_storage.set(storage_key, details)\n\n    # Return the details data\n    return details\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.execute_statement","title":"<code>execute_statement(query)</code>","text":"<p>Execute a non-returning SQL statement in Athena.</p> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def execute_statement(self, query: str):\n    \"\"\"Execute a non-returning SQL statement in Athena.\"\"\"\n    try:\n        # Start the query execution\n        query_execution_id = wr.athena.start_query_execution(\n            sql=query,\n            database=self.get_database(),\n            boto3_session=self.boto_session,\n        )\n        self.log.debug(f\"QueryExecutionId: {query_execution_id}\")\n\n        # Wait for the query to complete\n        wr.athena.wait_query(query_execution_id=query_execution_id, boto3_session=self.boto_session)\n        self.log.debug(f\"Statement executed successfully: {query_execution_id}\")\n    except Exception as e:\n        self.log.error(f\"Failed to execute statement: {e}\")\n        raise\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.exists","title":"<code>exists()</code>","text":"<p>Validation Checks for this Data Source</p> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"Validation Checks for this Data Source\"\"\"\n\n    # We're we able to pull AWS Metadata for this table_name?\"\"\"\n    if self.catalog_table_meta is None:\n        self.log.debug(f\"AthenaSource {self.get_table_name()} not found in SageWorks Metadata...\")\n        return False\n    return True\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.modified","title":"<code>modified()</code>","text":"<p>Return the datetime when this artifact was last modified</p> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def modified(self) -&gt; datetime:\n    \"\"\"Return the datetime when this artifact was last modified\"\"\"\n    return self.catalog_table_meta[\"UpdateTime\"]\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.num_columns","title":"<code>num_columns()</code>","text":"<p>Return the number of columns for this Data Source</p> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def num_columns(self) -&gt; int:\n    \"\"\"Return the number of columns for this Data Source\"\"\"\n    return len(self.column_names())\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.num_rows","title":"<code>num_rows()</code>","text":"<p>Return the number of rows for this Data Source</p> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def num_rows(self) -&gt; int:\n    \"\"\"Return the number of rows for this Data Source\"\"\"\n    count_df = self.query(f'select count(*) AS count from \"{self.get_database()}\".\"{self.get_table_name()}\"')\n    return count_df[\"count\"][0]\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.outliers_impl","title":"<code>outliers_impl(scale=1.5, use_stddev=False, recompute=False)</code>","text":"<p>Compute outliers for all the numeric columns in a DataSource</p> <p>Parameters:</p> Name Type Description Default <code>scale</code> <code>float</code> <p>The scale to use for the IQR (default: 1.5)</p> <code>1.5</code> <code>use_stddev</code> <code>bool</code> <p>Use Standard Deviation instead of IQR (default: False)</p> <code>False</code> <code>recompute</code> <code>bool</code> <p>Recompute the outliers (default: False)</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame of outliers from this DataSource</p> Notes <p>Uses the IQR * 1.5 (~= 2.5 Sigma) (use 1.7 for ~= 3 Sigma) The scale parameter can be adjusted to change the IQR multiplier</p> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def outliers_impl(self, scale: float = 1.5, use_stddev=False, recompute: bool = False) -&gt; pd.DataFrame:\n    \"\"\"Compute outliers for all the numeric columns in a DataSource\n\n    Args:\n        scale (float): The scale to use for the IQR (default: 1.5)\n        use_stddev (bool): Use Standard Deviation instead of IQR (default: False)\n        recompute (bool): Recompute the outliers (default: False)\n\n    Returns:\n        pd.DataFrame: A DataFrame of outliers from this DataSource\n\n    Notes:\n        Uses the IQR * 1.5 (~= 2.5 Sigma) (use 1.7 for ~= 3 Sigma)\n        The scale parameter can be adjusted to change the IQR multiplier\n    \"\"\"\n\n    # Compute outliers using the SQL Outliers class\n    sql_outliers = outliers.Outliers()\n    return sql_outliers.compute_outliers(self, scale=scale, use_stddev=use_stddev)\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.query","title":"<code>query(query)</code>","text":"<p>Query the AthenaSource</p> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def query(self, query: str) -&gt; pd.DataFrame:\n    \"\"\"Query the AthenaSource\"\"\"\n    df = wr.athena.read_sql_query(\n        sql=query,\n        database=self.get_database(),\n        ctas_approach=False,\n        boto3_session=self.boto_session,\n    )\n    scanned_bytes = df.query_metadata[\"Statistics\"][\"DataScannedInBytes\"]\n    if scanned_bytes &gt; 0:\n        self.log.info(f\"Athena Query successful (scanned bytes: {scanned_bytes})\")\n    return df\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.refresh_meta","title":"<code>refresh_meta()</code>","text":"<p>Refresh our internal AWS Broker catalog metadata</p> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def refresh_meta(self):\n    \"\"\"Refresh our internal AWS Broker catalog metadata\"\"\"\n    _catalog_meta = self.aws_broker.get_metadata(ServiceCategory.DATA_CATALOG, force_refresh=True)\n    self.catalog_table_meta = _catalog_meta[self.get_database()].get(self.get_table_name())\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.s3_storage_location","title":"<code>s3_storage_location()</code>","text":"<p>Get the S3 Storage Location for this Data Source</p> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def s3_storage_location(self) -&gt; str:\n    \"\"\"Get the S3 Storage Location for this Data Source\"\"\"\n    return self.catalog_table_meta[\"StorageDescriptor\"][\"Location\"]\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.sageworks_meta","title":"<code>sageworks_meta()</code>","text":"<p>Get the SageWorks specific metadata for this Artifact</p> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>@trace_calls\ndef sageworks_meta(self) -&gt; dict:\n    \"\"\"Get the SageWorks specific metadata for this Artifact\"\"\"\n\n    # Sanity Check if we have invalid AWS Metadata\n    self.log.info(f\"Retrieving SageWorks Metadata for Artifact: {self.uuid}...\")\n    if self.catalog_table_meta is None:\n        if not self.exists():\n            self.log.error(f\"DataSource {self.uuid} doesn't appear to exist...\")\n        else:\n            self.log.critical(f\"Unable to get AWS Metadata for {self.get_table_name()}\")\n            self.log.critical(\"Malformed Artifact! Delete this Artifact and recreate it!\")\n        return {}\n\n    # Get the SageWorks Metadata from the Catalog Table Metadata\n    return sageworks_meta_from_catalog_table_meta(self.catalog_table_meta)\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.sample_impl","title":"<code>sample_impl()</code>","text":"<p>Pull a sample of rows from the DataSource</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A sample DataFrame for an Athena DataSource</p> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def sample_impl(self) -&gt; pd.DataFrame:\n    \"\"\"Pull a sample of rows from the DataSource\n\n    Returns:\n        pd.DataFrame: A sample DataFrame for an Athena DataSource\n    \"\"\"\n\n    # Call the SQL function to pull a sample of the rows\n    return sample_rows.sample_rows(self)\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.size","title":"<code>size()</code>","text":"<p>Return the size of this data in MegaBytes</p> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def size(self) -&gt; float:\n    \"\"\"Return the size of this data in MegaBytes\"\"\"\n    size_in_bytes = sum(wr.s3.size_objects(self.s3_storage_location(), boto3_session=self.boto_session).values())\n    size_in_mb = size_in_bytes / 1_000_000\n    return size_in_mb\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.smart_sample","title":"<code>smart_sample()</code>","text":"<p>Get a smart sample dataframe for this DataSource</p> Note <p>smart = sample data + outliers for the DataSource</p> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def smart_sample(self) -&gt; pd.DataFrame:\n    \"\"\"Get a smart sample dataframe for this DataSource\n\n    Note:\n        smart = sample data + outliers for the DataSource\"\"\"\n\n    # Outliers DataFrame\n    outlier_rows = self.outliers()\n\n    # Sample DataFrame\n    sample_rows = self.sample()\n    sample_rows[\"outlier_group\"] = \"sample\"\n\n    # Combine the sample rows with the outlier rows\n    all_rows = pd.concat([outlier_rows, sample_rows]).reset_index(drop=True)\n\n    # Drop duplicates\n    all_except_outlier_group = [col for col in all_rows.columns if col != \"outlier_group\"]\n    all_rows = all_rows.drop_duplicates(subset=all_except_outlier_group, ignore_index=True)\n    return all_rows\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.upsert_sageworks_meta","title":"<code>upsert_sageworks_meta(new_meta)</code>","text":"<p>Add SageWorks specific metadata to this Artifact</p> <p>Parameters:</p> Name Type Description Default <code>new_meta</code> <code>dict</code> <p>Dictionary of new metadata to add</p> required Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def upsert_sageworks_meta(self, new_meta: dict):\n    \"\"\"Add SageWorks specific metadata to this Artifact\n\n    Args:\n        new_meta (dict): Dictionary of new metadata to add\n    \"\"\"\n\n    # Give a warning message for keys that don't start with sageworks_\n    for key in new_meta.keys():\n        if not key.startswith(\"sageworks_\"):\n            self.log.warning(\"Append 'sageworks_' to key names to avoid overwriting AWS meta data\")\n\n    # Now convert any non-string values to JSON strings\n    for key, value in new_meta.items():\n        if not isinstance(value, str):\n            new_meta[key] = json.dumps(value, cls=CustomEncoder)\n\n    # Store our updated metadata\n    try:\n        wr.catalog.upsert_table_parameters(\n            parameters=new_meta,\n            database=self.get_database(),\n            table=self.get_table_name(),\n            boto3_session=self.boto_session,\n        )\n    except botocore.exceptions.ClientError as e:\n        error_code = e.response[\"Error\"][\"Code\"]\n        if error_code == \"InvalidInputException\":\n            self.log.error(f\"Unable to upsert metadata for {self.get_table_name()}\")\n            self.log.error(\"Probably because the metadata is too large\")\n            self.log.error(new_meta)\n        elif error_code == \"ConcurrentModificationException\":\n            self.log.warning(\"ConcurrentModificationException... trying again...\")\n            time.sleep(1)\n            wr.catalog.upsert_table_parameters(\n                parameters=new_meta,\n                database=self.get_database(),\n                table=self.get_table_name(),\n                boto3_session=self.boto_session,\n            )\n        else:\n            raise e\n</code></pre>"},{"location":"core_classes/artifacts/athena_source/#sageworks.core.artifacts.athena_source.AthenaSource.value_counts","title":"<code>value_counts(recompute=False)</code>","text":"<p>Compute 'value_counts' for all the string columns in a DataSource</p> <p>Parameters:</p> Name Type Description Default <code>recompute</code> <code>bool</code> <p>Recompute the value counts (default: False)</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of value counts for each column in the form  {'col1': {'value_1': 42, 'value_2': 16, 'value_3': 9,...},   'col2': ...}</p> Source code in <code>src/sageworks/core/artifacts/athena_source.py</code> <pre><code>def value_counts(self, recompute: bool = False) -&gt; dict[dict]:\n    \"\"\"Compute 'value_counts' for all the string columns in a DataSource\n\n    Args:\n        recompute (bool): Recompute the value counts (default: False)\n\n    Returns:\n        dict(dict): A dictionary of value counts for each column in the form\n             {'col1': {'value_1': 42, 'value_2': 16, 'value_3': 9,...},\n              'col2': ...}\n    \"\"\"\n\n    # First check if we have already computed the value counts\n    value_counts_dict = self.sageworks_meta().get(\"sageworks_value_counts\")\n    if value_counts_dict and not recompute:\n        return value_counts_dict\n\n    # Call the SQL function to compute value_counts\n    value_count_dict = value_counts.value_counts(self)\n\n    # Push the value_count data into our DataSource Metadata\n    self.upsert_sageworks_meta({\"sageworks_value_counts\": value_count_dict})\n\n    # Return the value_count data\n    return value_count_dict\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/","title":"EndpointCore","text":"<p>API Classes</p> <p>Found a method here you want to use? The API Classes have method pass-through so just call the method on the Endpoint API Class and voil\u00e0 it works the same.</p> <p>EndpointCore: SageWorks EndpointCore Class</p>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore","title":"<code>EndpointCore</code>","text":"<p>             Bases: <code>Artifact</code></p> <p>EndpointCore: SageWorks EndpointCore Class</p> Common Usage <pre><code>my_endpoint = EndpointCore(endpoint_uuid)\nprediction_df = my_endpoint.predict(test_df)\nmetrics = my_endpoint.regression_metrics(target_column, prediction_df)\nfor metric, value in metrics.items():\n    print(f\"{metric}: {value:0.3f}\")\n</code></pre> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>class EndpointCore(Artifact):\n    \"\"\"EndpointCore: SageWorks EndpointCore Class\n\n    Common Usage:\n        ```\n        my_endpoint = EndpointCore(endpoint_uuid)\n        prediction_df = my_endpoint.predict(test_df)\n        metrics = my_endpoint.regression_metrics(target_column, prediction_df)\n        for metric, value in metrics.items():\n            print(f\"{metric}: {value:0.3f}\")\n        ```\n    \"\"\"\n\n    def __init__(self, endpoint_uuid, force_refresh: bool = False):\n        \"\"\"EndpointCore Initialization\n\n        Args:\n            endpoint_uuid (str): Name of Endpoint in SageWorks\n            force_refresh (bool, optional): Force a refresh of the AWS Broker. Defaults to False.\n        \"\"\"\n        # Call SuperClass Initialization\n        super().__init__(endpoint_uuid)\n\n        # Grab an AWS Metadata Broker object and pull information for Endpoints\n        self.endpoint_name = endpoint_uuid\n        self.endpoint_meta = self.aws_broker.get_metadata(ServiceCategory.ENDPOINTS, force_refresh=force_refresh).get(\n            self.endpoint_name\n        )\n\n        # Sanity check and then set up our FeatureSet attributes\n        if self.endpoint_meta is None:\n            self.log.important(f\"Could not find endpoint {self.uuid} within current visibility scope\")\n            return\n\n        self.endpoint_return_columns = None\n\n        # Set the Inference, Capture, and Monitoring S3 Paths\n        self.model_name = self.get_input()\n        self.model_inference_path = self.models_s3_path + \"/inference/\" + self.model_name\n        self.model_data_capture_path = self.models_s3_path + \"/data_capture/\" + self.model_name\n        self.model_monitoring_path = self.models_s3_path + \"/monitoring/\" + self.model_name\n\n        # Call SuperClass Post Initialization\n        super().__post_init__()\n\n        # All done\n        self.log.info(f\"EndpointCore Initialized: {self.endpoint_name}\")\n\n    def refresh_meta(self):\n        \"\"\"Refresh the Artifact's metadata\"\"\"\n        self.endpoint_meta = self.aws_broker.get_metadata(ServiceCategory.ENDPOINTS, force_refresh=True).get(\n            self.endpoint_name\n        )\n\n    def exists(self) -&gt; bool:\n        \"\"\"Does the feature_set_name exist in the AWS Metadata?\"\"\"\n        if self.endpoint_meta is None:\n            self.log.debug(f\"Endpoint {self.endpoint_name} not found in AWS Metadata\")\n            return False\n        return True\n\n    def health_check(self) -&gt; list[str]:\n        \"\"\"Perform a health check on this model\n        Returns:\n            list[str]: List of health issues\n        \"\"\"\n        # Call the base class health check\n        health_issues = super().health_check()\n\n        # We're going to check for 5xx errors and no activity\n        endpoint_metrics = self.endpoint_metrics()\n\n        # Check for 5xx errors\n        num_errors = endpoint_metrics[\"Invocation5XXErrors\"].sum()\n        if num_errors &gt; 5:\n            health_issues.append(\"5xx_errors\")\n        elif num_errors &gt; 0:\n            health_issues.append(\"5xx_errors_min\")\n        else:\n            self.remove_sageworks_health_tag(\"5xx_errors\")\n            self.remove_sageworks_health_tag(\"5xx_errors_min\")\n\n        # Check for Endpoint activity\n        num_invocations = endpoint_metrics[\"Invocations\"].sum()\n        if num_invocations == 0:\n            health_issues.append(\"no_activity\")\n        else:\n            self.remove_sageworks_health_tag(\"no_activity\")\n        return health_issues\n\n    def predict(self, feature_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Run inference/prediction on the given Feature DataFrame\n        Args:\n            feature_df (pd.DataFrame): DataFrame to run predictions on (must have superset of features)\n        Returns:\n            pd.DataFrame: Return the feature DataFrame with two additional columns (prediction, pred_proba)\n        \"\"\"\n\n        # Create our Endpoint Predictor Class\n        predictor = Predictor(\n            self.endpoint_name,\n            sagemaker_session=self.sm_session,\n            serializer=CSVSerializer(),\n            deserializer=CSVDeserializer(),\n        )\n\n        # Now split up the dataframe into 500 row chunks, send those chunks to our\n        # endpoint (with error handling) and stitch all the chunks back together\n        df_list = []\n        for index in range(0, len(feature_df), 500):\n            print(\"Processing...\")\n\n            # Compute partial DataFrames, add them to a list, and concatenate at the end\n            partial_df = self._endpoint_error_handling(predictor, feature_df[index : index + 500])\n            df_list.append(partial_df)\n\n        # Concatenate the dataframes\n        combined_df = pd.concat(df_list, ignore_index=True)\n\n        # Convert data to numeric\n        # Note: Since we're using CSV serializers numeric columns often get changed to generic 'object' types\n\n        # Hard Conversion\n        # Note: If are string/object columns we want to use 'ignore' here so those columns\n        #       won't raise an error (columns maintain current type)\n        converted_df = combined_df.apply(pd.to_numeric, errors=\"ignore\")\n\n        # Soft Conversion\n        # Convert columns to the best possible dtype that supports the pd.NA missing value.\n        converted_df = converted_df.convert_dtypes()\n\n        # Return the Dataframe\n        return converted_df\n\n    def is_serverless(self):\n        \"\"\"\n        Check if the current endpoint is serverless.\n        Returns:\n            bool: True if the endpoint is serverless, False otherwise.\n        \"\"\"\n        return \"Serverless\" in self.endpoint_meta[\"InstanceType\"]\n\n    def add_data_capture(self):\n        \"\"\"Add data capture to the endpoint\"\"\"\n        from sageworks.utils.model_monitoring import ModelMonitoring\n\n        mm = ModelMonitoring(self.endpoint_name)\n        mm.add_data_capture()\n\n    def _endpoint_error_handling(self, predictor, feature_df):\n        \"\"\"Internal: Method that handles Errors, Retries, and Binary Search for Error Row(s)\"\"\"\n\n        # Convert the DataFrame into a CSV buffer\n        csv_buffer = StringIO()\n        feature_df.to_csv(csv_buffer, index=False)\n\n        # Error Handling if the Endpoint gives back an error\n        try:\n            # Send the CSV Buffer to the predictor\n            results = predictor.predict(csv_buffer.getvalue())\n\n            # Construct a DataFrame from the results\n            results_df = pd.DataFrame.from_records(results[1:], columns=results[0])\n\n            # Capture the return columns\n            self.endpoint_return_columns = results_df.columns.tolist()\n\n            # Return the results dataframe\n            return results_df\n\n        except botocore.exceptions.ClientError as err:\n            if err.response[\"Error\"][\"Code\"] == \"ModelError\":  # Model Error\n                # Report the error and raise an exception\n                self.log.critical(f\"Endpoint prediction error: {err.response.get('Message')}\")\n                raise err\n\n            # Base case: DataFrame with 1 Row\n            if len(feature_df) == 1:\n                # If we don't have ANY known good results we're kinda screwed\n                if not self.endpoint_return_columns:\n                    raise err\n\n                # Construct an Error DataFrame (one row of NaNs in the return columns)\n                results_df = self._error_df(feature_df, self.endpoint_return_columns)\n                return results_df\n\n            # Recurse on binary splits of the dataframe\n            num_rows = len(feature_df)\n            split = int(num_rows / 2)\n            first_half = self._endpoint_error_handling(predictor, feature_df[0:split])\n            second_half = self._endpoint_error_handling(predictor, feature_df[split:num_rows])\n            return pd.concat([first_half, second_half], ignore_index=True)\n\n    def _error_df(self, df, all_columns):\n        \"\"\"Internal: Method to construct an Error DataFrame (a Pandas DataFrame with one row of NaNs)\"\"\"\n        # Create a new dataframe with all NaNs\n        error_df = pd.DataFrame(dict(zip(all_columns, [[np.NaN]] * len(self.endpoint_return_columns))))\n        # Now set the original values for the incoming dataframe\n        for column in df.columns:\n            error_df[column] = df[column].values\n        return error_df\n\n    def size(self) -&gt; float:\n        \"\"\"Return the size of this data in MegaBytes\"\"\"\n        return 0.0\n\n    def aws_meta(self) -&gt; dict:\n        \"\"\"Get ALL the AWS metadata for this artifact\"\"\"\n        return self.endpoint_meta\n\n    def arn(self) -&gt; str:\n        \"\"\"AWS ARN (Amazon Resource Name) for this artifact\"\"\"\n        return self.endpoint_meta[\"EndpointArn\"]\n\n    def aws_url(self):\n        \"\"\"The AWS URL for looking at/querying this data source\"\"\"\n        return f\"https://{self.aws_region}.console.aws.amazon.com/athena/home\"\n\n    def created(self) -&gt; datetime:\n        \"\"\"Return the datetime when this artifact was created\"\"\"\n        return self.endpoint_meta[\"CreationTime\"]\n\n    def modified(self) -&gt; datetime:\n        \"\"\"Return the datetime when this artifact was last modified\"\"\"\n        return self.endpoint_meta[\"LastModifiedTime\"]\n\n    def endpoint_metrics(self) -&gt; pd.DataFrame:\n        \"\"\"Return the metrics for this endpoint\n        Returns:\n            pd.DataFrame: DataFrame with the metrics for this endpoint\n        \"\"\"\n\n        # Do we have it cached?\n        metrics_key = f\"endpoint:{self.uuid}:endpoint_metrics\"\n        endpoint_metrics = self.temp_storage.get(metrics_key)\n        if endpoint_metrics is not None:\n            return endpoint_metrics\n\n        # We don't have it cached so let's get it from CloudWatch\n        self.log.important(\"Updating endpoint metrics...\")\n        variant = self.endpoint_meta[\"ProductionVariants\"][0][\"VariantName\"]\n        endpoint_metrics = EndpointMetrics().get_metrics(self.uuid, variant=variant)\n        self.temp_storage.set(metrics_key, endpoint_metrics)\n        return endpoint_metrics\n\n    def details(self, recompute: bool = False) -&gt; dict:\n        \"\"\"Additional Details about this Endpoint\n        Args:\n            recompute (bool): Recompute the details (default: False)\n        Returns:\n            dict(dict): A dictionary of details about this Endpoint\n        \"\"\"\n        # Check if we have cached version of the FeatureSet Details\n        details_key = f\"endpoint:{self.uuid}:details\"\n\n        cached_details = self.data_storage.get(details_key)\n        if cached_details and not recompute:\n            # Update the endpoint metrics before returning cached details\n            endpoint_metrics = self.endpoint_metrics()\n            cached_details[\"endpoint_metrics\"] = endpoint_metrics\n            return cached_details\n\n        # Fill in all the details about this Endpoint\n        details = self.summary()\n\n        # Get details from our AWS Metadata\n        details[\"status\"] = self.endpoint_meta[\"EndpointStatus\"]\n        details[\"instance\"] = self.endpoint_meta[\"InstanceType\"]\n        try:\n            details[\"instance_count\"] = self.endpoint_meta[\"ProductionVariants\"][0][\"CurrentInstanceCount\"] or \"-\"\n        except KeyError:\n            details[\"instance_count\"] = \"-\"\n        details[\"variant\"] = self.endpoint_meta[\"ProductionVariants\"][0][\"VariantName\"]\n\n        # Add the underlying model details\n        details[\"model_name\"] = self.model_name\n        model_details = self.model_details()\n        details[\"model_type\"] = model_details.get(\"model_type\", \"unknown\")\n        details[\"model_metrics\"] = model_details.get(\"model_metrics\")\n        details[\"confusion_matrix\"] = model_details.get(\"confusion_matrix\")\n        details[\"regression_predictions\"] = model_details.get(\"regression_predictions\")\n        details[\"inference_meta\"] = model_details.get(\"inference_meta\")\n\n        # Add endpoint metrics from CloudWatch\n        details[\"endpoint_metrics\"] = self.endpoint_metrics()\n\n        # Cache the details\n        self.data_storage.set(details_key, details)\n\n        # Return the details\n        return details\n\n    def make_ready(self) -&gt; bool:\n        \"\"\"This is a BLOCKING method that will wait until the Endpoint is ready\n        Returns:\n            bool: True if the Endpoint is ready, False otherwise\n        \"\"\"\n        self.set_status(\"ready\")\n        self.remove_sageworks_health_tag(\"needs_onboard\")\n        self.details(recompute=True)\n        return True\n\n    def model_details(self) -&gt; dict:\n        \"\"\"Return the details about the model used in this Endpoint\"\"\"\n        if self.model_name == \"unknown\":\n            return {}\n        else:\n            model = ModelCore(self.model_name)\n            if model.exists():\n                return model.details()\n            else:\n                return {}\n\n    def model_type(self) -&gt; str:\n        \"\"\"Return the type of model used in this Endpoint\"\"\"\n        return self.details().get(\"model_type\", \"unknown\")\n\n    def capture_performance_metrics(\n        self, feature_df: pd.DataFrame, target_column: str, data_name: str, data_hash: str, description: str\n    ) -&gt; None:\n        \"\"\"Capture the performance metrics for this Endpoint\n        Args:\n            feature_df (pd.DataFrame): DataFrame to run predictions on (must have superset of features)\n            target_column (str): Name of the target column\n            data_name (str): Name of the data used for inference\n            data_hash (str): Hash of the data used for inference\n            description (str): Description of the data used for inference\n        Returns:\n            None\n        Note:\n            This method captures performance metrics and writes them to the S3 Model Inference Folder\n        \"\"\"\n\n        # Run predictions on the feature_df\n        prediction_df = self.predict(feature_df)\n\n        # Compute the metrics\n        model_type = self.model_type()\n        if model_type == ModelType.REGRESSOR.value:\n            metrics = self.regression_metrics(target_column, prediction_df)\n        elif model_type == ModelType.CLASSIFIER.value:\n            metrics = self.classification_metrics(target_column, prediction_df)\n        else:\n            raise ValueError(f\"Unknown Model Type: {model_type}\")\n\n        # Metadata for the model inference\n        inference_meta = {\n            \"test_data\": data_name,\n            \"test_data_hash\": data_hash,\n            \"test_rows\": len(feature_df),\n            \"description\": description,\n        }\n\n        # Write the metadata dictionary, and metrics to our S3 Model Inference Folder\n        wr.s3.to_json(\n            pd.DataFrame([inference_meta]),\n            f\"{self.model_inference_path}/inference_meta.json\",\n            index=False,\n        )\n        self.log.debug(f\"Writing metrics to {self.model_inference_path}/inference_metrics.csv\")\n        wr.s3.to_csv(metrics, f\"{self.model_inference_path}/inference_metrics.csv\", index=False)\n\n        # Write the confusion matrix to our S3 Model Inference Folder\n        if model_type == ModelType.CLASSIFIER.value:\n            conf_mtx = self.confusion_matrix(target_column, prediction_df)\n            self.log.debug(f\"Writing confusion matrix to {self.model_inference_path}/inference_cm.csv\")\n            # Note: Unlike other dataframes here, we want to write the index (labels) to the CSV\n            wr.s3.to_csv(conf_mtx, f\"{self.model_inference_path}/inference_cm.csv\", index=True)\n\n        # Write the regression predictions to our S3 Model Inference Folder\n        if model_type == ModelType.REGRESSOR.value:\n            pred_df = self.regression_predictions(target_column, prediction_df)\n            self.log.debug(f\"Writing reg predictions to {self.model_inference_path}/inference_predictions.csv\")\n            wr.s3.to_csv(pred_df, f\"{self.model_inference_path}/inference_predictions.csv\", index=False)\n\n        #\n        # Generate SHAP values for our Prediction Dataframe\n        #\n\n        # Grab the model artifact from AWS\n        model_artifact = ExtractModelArtifact(self.endpoint_name).get_model_artifact()\n\n        # Get the exact features used to train the model\n        model_features = model_artifact.feature_names_in_\n        X_pred = prediction_df[model_features]\n\n        # Compute the SHAP values\n        shap_vals = self.shap_values(model_artifact, X_pred)\n\n        # Multiple shap vals CSV for classifiers\n        if model_type == ModelType.CLASSIFIER.value:\n            # Need a separate shapley values CSV for each class\n            for i, class_shap_vals in enumerate(shap_vals):\n                df_shap = pd.DataFrame(class_shap_vals, columns=X_pred.columns)\n\n                # Write shap vals to S3 Model Inference Folder\n                self.log.debug(f\"Writing SHAP values to {self.model_inference_path}/inference_shap_values.csv\")\n                wr.s3.to_csv(df_shap, f\"{self.model_inference_path}/inference_shap_values_class_{i}.csv\", index=False)\n\n        # Single shap vals CSV for regressors\n        if model_type == ModelType.REGRESSOR.value:\n            # Format shap values into single dataframe\n            df_shap = pd.DataFrame(shap_vals, columns=X_pred.columns)\n\n            # Write shap vals to S3 Model Inference Folder\n            self.log.debug(f\"Writing SHAP values to {self.model_inference_path}/inference_shap_values.csv\")\n            wr.s3.to_csv(df_shap, f\"{self.model_inference_path}/inference_shap_values.csv\", index=False)\n\n        # Now recompute the details for our Model\n        self.log.important(f\"Recomputing Details for {self.model_name} to show latest Inference Results...\")\n        model = ModelCore(self.model_name)\n        model.details(recompute=True)\n\n        # Recompute the details so that inference model metrics are updated\n        self.log.important(f\"Recomputing Details for {self.uuid} to show latest Inference Results...\")\n        self.details(recompute=True)\n\n    @staticmethod\n    def shap_values(model, X: pd.DataFrame) -&gt; np.array:\n        \"\"\"Compute the SHAP values for this Model\n        Args:\n            model (Model): Model object\n            X (pd.DataFrame): DataFrame with the prediction results\n        Returns:\n            pd.DataFrame: DataFrame with the SHAP values\n        \"\"\"\n        # Note: For Tree-based models like decision trees, random forests, XGBoost, LightGBM,\n        explainer = shap.TreeExplainer(model)\n        shap_vals = explainer.shap_values(X)\n        return shap_vals\n\n    @staticmethod\n    def regression_metrics(target_column: str, prediction_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Compute the performance metrics for this Endpoint\n        Args:\n            target_column (str): Name of the target column\n            prediction_df (pd.DataFrame): DataFrame with the prediction results\n        Returns:\n            pd.DataFrame: DataFrame with the performance metrics\n        \"\"\"\n\n        # Compute the metrics\n        y_true = prediction_df[target_column]\n        y_pred = prediction_df[\"prediction\"]\n\n        mae = mean_absolute_error(y_true, y_pred)\n        rmse = sqrt(mean_squared_error(y_true, y_pred))\n        r2 = r2_score(y_true, y_pred)\n        # Mean Absolute Percentage Error\n        mape = np.mean(np.where(y_true != 0, np.abs((y_true - y_pred) / y_true), np.abs(y_true - y_pred))) * 100\n        # Median Absolute Error\n        medae = median_absolute_error(y_true, y_pred)\n\n        # Return the metrics\n        return pd.DataFrame.from_records([{\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2, \"MAPE\": mape, \"MedAE\": medae}])\n\n    @staticmethod\n    def classification_metrics(target_column: str, prediction_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Compute the performance metrics for this Endpoint\n        Args:\n            target_column (str): Name of the target column\n            prediction_df (pd.DataFrame): DataFrame with the prediction results\n        Returns:\n            pd.DataFrame: DataFrame with the performance metrics\n        \"\"\"\n\n        # Get a list of unique labels\n        labels = prediction_df[target_column].unique()\n\n        # Calculate scores\n        scores = precision_recall_fscore_support(\n            prediction_df[target_column], prediction_df[\"prediction\"], average=None, labels=labels\n        )\n\n        # Calculate ROC AUC\n        # ROC-AUC score measures the model's ability to distinguish between classes;\n        # - A value of 0.5 indicates no discrimination (equivalent to random guessing)\n        # - A score close to 1 indicates high discriminative power\n\n        # Convert 'pred_proba' column to a 2D NumPy array\n        y_score = np.array([ast.literal_eval(x) for x in prediction_df[\"pred_proba\"]], dtype=float)\n        # y_score = np.array(prediction_df['pred_proba'].tolist())\n\n        # One-hot encode the true labels\n        lb = LabelBinarizer()\n        lb.fit(prediction_df[target_column])\n        y_true = lb.transform(prediction_df[target_column])\n\n        roc_auc = roc_auc_score(y_true, y_score, multi_class=\"ovr\", average=None)\n\n        # Put the scores into a dataframe\n        score_df = pd.DataFrame(\n            {\n                target_column: labels,\n                \"precision\": scores[0],\n                \"recall\": scores[1],\n                \"fscore\": scores[2],\n                \"roc_auc\": roc_auc,\n                \"support\": scores[3],\n            }\n        )\n\n        # Sort the target labels\n        score_df = score_df.sort_values(by=[target_column], ascending=True)\n        return score_df\n\n    def confusion_matrix(self, target_column: str, prediction_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Compute the confusion matrix for this Endpoint\n        Args:\n            target_column (str): Name of the target column\n            prediction_df (pd.DataFrame): DataFrame with the prediction results\n        Returns:\n            pd.DataFrame: DataFrame with the confusion matrix\n        \"\"\"\n\n        y_true = prediction_df[target_column]\n        y_pred = prediction_df[\"prediction\"]\n\n        # Compute the confusion matrix\n        conf_mtx = confusion_matrix(y_true, y_pred)\n\n        # Get unique labels\n        labels = sorted(list(set(y_true) | set(y_pred)))\n\n        # Create a DataFrame\n        conf_mtx_df = pd.DataFrame(conf_mtx, index=labels, columns=labels)\n        return conf_mtx_df\n\n    def regression_predictions(self, target: str, prediction_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Compute the regression predictions for this Endpoint\n        Args:\n            target (str): Name of the target column\n            prediction_df (pd.DataFrame): DataFrame with the prediction results\n        Returns:\n            pd.DataFrame: DataFrame with the regression predictions\n        \"\"\"\n\n        # Return the predictions\n        return prediction_df[[target, \"prediction\"]]\n\n    def endpoint_config_name(self) -&gt; str:\n        # Grab the Endpoint Config Name from the AWS\n        details = self.sm_client.describe_endpoint(EndpointName=self.endpoint_name)\n        return details[\"EndpointConfigName\"]\n\n    def delete(self):\n        \"\"\"Delete an existing Endpoint: Underlying Models, Configuration, and Endpoint\"\"\"\n        self.delete_endpoint_models()\n\n        # Grab the Endpoint Config Name from the AWS\n        endpoint_config_name = self.endpoint_config_name()\n        try:\n            self.log.info(f\"Deleting Endpoint Config {endpoint_config_name}...\")\n            self.sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n        except botocore.exceptions.ClientError:\n            self.log.info(f\"Endpoint Config {endpoint_config_name} doesn't exist...\")\n\n        # Check for any monitoring schedules\n        response = self.sm_client.list_monitoring_schedules(EndpointName=self.uuid)\n        monitoring_schedules = response[\"MonitoringScheduleSummaries\"]\n        for schedule in monitoring_schedules:\n            self.log.info(f\"Deleting Endpoint Monitoring Schedule {schedule['MonitoringScheduleName']}...\")\n            self.sm_client.delete_monitoring_schedule(MonitoringScheduleName=schedule[\"MonitoringScheduleName\"])\n\n        # Okay now delete the Endpoint\n        try:\n            time.sleep(1)  # Let AWS catch up with any deletions performed above\n            self.log.info(f\"Deleting Endpoint {self.uuid}...\")\n            self.sm_client.delete_endpoint(EndpointName=self.uuid)\n        except botocore.exceptions.ClientError as e:\n            self.log.info(\"Endpoint ClientError...\")\n            raise e\n\n        # Now delete any data in the Cache\n        for key in self.data_storage.list_subkeys(f\"endpoint:{self.uuid}\"):\n            self.log.info(f\"Deleting Cache Key: {key}\")\n            self.data_storage.delete(key)\n\n    def delete_endpoint_models(self):\n        \"\"\"Delete the underlying Model for an Endpoint\"\"\"\n\n        # Grab the Endpoint Config Name from the AWS\n        endpoint_config_name = self.endpoint_config_name()\n\n        # Retrieve the Model Names from the Endpoint Config\n        try:\n            endpoint_config = self.sm_client.describe_endpoint_config(EndpointConfigName=endpoint_config_name)\n        except botocore.exceptions.ClientError:\n            self.log.info(f\"Endpoint Config {self.uuid} doesn't exist...\")\n            return\n        model_names = [variant[\"ModelName\"] for variant in endpoint_config[\"ProductionVariants\"]]\n        for model_name in model_names:\n            self.log.info(f\"Deleting Model {model_name}...\")\n            self.sm_client.delete_model(ModelName=model_name)\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore.__init__","title":"<code>__init__(endpoint_uuid, force_refresh=False)</code>","text":"<p>EndpointCore Initialization</p> <p>Parameters:</p> Name Type Description Default <code>endpoint_uuid</code> <code>str</code> <p>Name of Endpoint in SageWorks</p> required <code>force_refresh</code> <code>bool</code> <p>Force a refresh of the AWS Broker. Defaults to False.</p> <code>False</code> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>def __init__(self, endpoint_uuid, force_refresh: bool = False):\n    \"\"\"EndpointCore Initialization\n\n    Args:\n        endpoint_uuid (str): Name of Endpoint in SageWorks\n        force_refresh (bool, optional): Force a refresh of the AWS Broker. Defaults to False.\n    \"\"\"\n    # Call SuperClass Initialization\n    super().__init__(endpoint_uuid)\n\n    # Grab an AWS Metadata Broker object and pull information for Endpoints\n    self.endpoint_name = endpoint_uuid\n    self.endpoint_meta = self.aws_broker.get_metadata(ServiceCategory.ENDPOINTS, force_refresh=force_refresh).get(\n        self.endpoint_name\n    )\n\n    # Sanity check and then set up our FeatureSet attributes\n    if self.endpoint_meta is None:\n        self.log.important(f\"Could not find endpoint {self.uuid} within current visibility scope\")\n        return\n\n    self.endpoint_return_columns = None\n\n    # Set the Inference, Capture, and Monitoring S3 Paths\n    self.model_name = self.get_input()\n    self.model_inference_path = self.models_s3_path + \"/inference/\" + self.model_name\n    self.model_data_capture_path = self.models_s3_path + \"/data_capture/\" + self.model_name\n    self.model_monitoring_path = self.models_s3_path + \"/monitoring/\" + self.model_name\n\n    # Call SuperClass Post Initialization\n    super().__post_init__()\n\n    # All done\n    self.log.info(f\"EndpointCore Initialized: {self.endpoint_name}\")\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore.add_data_capture","title":"<code>add_data_capture()</code>","text":"<p>Add data capture to the endpoint</p> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>def add_data_capture(self):\n    \"\"\"Add data capture to the endpoint\"\"\"\n    from sageworks.utils.model_monitoring import ModelMonitoring\n\n    mm = ModelMonitoring(self.endpoint_name)\n    mm.add_data_capture()\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore.arn","title":"<code>arn()</code>","text":"<p>AWS ARN (Amazon Resource Name) for this artifact</p> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>def arn(self) -&gt; str:\n    \"\"\"AWS ARN (Amazon Resource Name) for this artifact\"\"\"\n    return self.endpoint_meta[\"EndpointArn\"]\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore.aws_meta","title":"<code>aws_meta()</code>","text":"<p>Get ALL the AWS metadata for this artifact</p> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>def aws_meta(self) -&gt; dict:\n    \"\"\"Get ALL the AWS metadata for this artifact\"\"\"\n    return self.endpoint_meta\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore.aws_url","title":"<code>aws_url()</code>","text":"<p>The AWS URL for looking at/querying this data source</p> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>def aws_url(self):\n    \"\"\"The AWS URL for looking at/querying this data source\"\"\"\n    return f\"https://{self.aws_region}.console.aws.amazon.com/athena/home\"\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore.capture_performance_metrics","title":"<code>capture_performance_metrics(feature_df, target_column, data_name, data_hash, description)</code>","text":"<p>Capture the performance metrics for this Endpoint Args:     feature_df (pd.DataFrame): DataFrame to run predictions on (must have superset of features)     target_column (str): Name of the target column     data_name (str): Name of the data used for inference     data_hash (str): Hash of the data used for inference     description (str): Description of the data used for inference Returns:     None Note:     This method captures performance metrics and writes them to the S3 Model Inference Folder</p> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>def capture_performance_metrics(\n    self, feature_df: pd.DataFrame, target_column: str, data_name: str, data_hash: str, description: str\n) -&gt; None:\n    \"\"\"Capture the performance metrics for this Endpoint\n    Args:\n        feature_df (pd.DataFrame): DataFrame to run predictions on (must have superset of features)\n        target_column (str): Name of the target column\n        data_name (str): Name of the data used for inference\n        data_hash (str): Hash of the data used for inference\n        description (str): Description of the data used for inference\n    Returns:\n        None\n    Note:\n        This method captures performance metrics and writes them to the S3 Model Inference Folder\n    \"\"\"\n\n    # Run predictions on the feature_df\n    prediction_df = self.predict(feature_df)\n\n    # Compute the metrics\n    model_type = self.model_type()\n    if model_type == ModelType.REGRESSOR.value:\n        metrics = self.regression_metrics(target_column, prediction_df)\n    elif model_type == ModelType.CLASSIFIER.value:\n        metrics = self.classification_metrics(target_column, prediction_df)\n    else:\n        raise ValueError(f\"Unknown Model Type: {model_type}\")\n\n    # Metadata for the model inference\n    inference_meta = {\n        \"test_data\": data_name,\n        \"test_data_hash\": data_hash,\n        \"test_rows\": len(feature_df),\n        \"description\": description,\n    }\n\n    # Write the metadata dictionary, and metrics to our S3 Model Inference Folder\n    wr.s3.to_json(\n        pd.DataFrame([inference_meta]),\n        f\"{self.model_inference_path}/inference_meta.json\",\n        index=False,\n    )\n    self.log.debug(f\"Writing metrics to {self.model_inference_path}/inference_metrics.csv\")\n    wr.s3.to_csv(metrics, f\"{self.model_inference_path}/inference_metrics.csv\", index=False)\n\n    # Write the confusion matrix to our S3 Model Inference Folder\n    if model_type == ModelType.CLASSIFIER.value:\n        conf_mtx = self.confusion_matrix(target_column, prediction_df)\n        self.log.debug(f\"Writing confusion matrix to {self.model_inference_path}/inference_cm.csv\")\n        # Note: Unlike other dataframes here, we want to write the index (labels) to the CSV\n        wr.s3.to_csv(conf_mtx, f\"{self.model_inference_path}/inference_cm.csv\", index=True)\n\n    # Write the regression predictions to our S3 Model Inference Folder\n    if model_type == ModelType.REGRESSOR.value:\n        pred_df = self.regression_predictions(target_column, prediction_df)\n        self.log.debug(f\"Writing reg predictions to {self.model_inference_path}/inference_predictions.csv\")\n        wr.s3.to_csv(pred_df, f\"{self.model_inference_path}/inference_predictions.csv\", index=False)\n\n    #\n    # Generate SHAP values for our Prediction Dataframe\n    #\n\n    # Grab the model artifact from AWS\n    model_artifact = ExtractModelArtifact(self.endpoint_name).get_model_artifact()\n\n    # Get the exact features used to train the model\n    model_features = model_artifact.feature_names_in_\n    X_pred = prediction_df[model_features]\n\n    # Compute the SHAP values\n    shap_vals = self.shap_values(model_artifact, X_pred)\n\n    # Multiple shap vals CSV for classifiers\n    if model_type == ModelType.CLASSIFIER.value:\n        # Need a separate shapley values CSV for each class\n        for i, class_shap_vals in enumerate(shap_vals):\n            df_shap = pd.DataFrame(class_shap_vals, columns=X_pred.columns)\n\n            # Write shap vals to S3 Model Inference Folder\n            self.log.debug(f\"Writing SHAP values to {self.model_inference_path}/inference_shap_values.csv\")\n            wr.s3.to_csv(df_shap, f\"{self.model_inference_path}/inference_shap_values_class_{i}.csv\", index=False)\n\n    # Single shap vals CSV for regressors\n    if model_type == ModelType.REGRESSOR.value:\n        # Format shap values into single dataframe\n        df_shap = pd.DataFrame(shap_vals, columns=X_pred.columns)\n\n        # Write shap vals to S3 Model Inference Folder\n        self.log.debug(f\"Writing SHAP values to {self.model_inference_path}/inference_shap_values.csv\")\n        wr.s3.to_csv(df_shap, f\"{self.model_inference_path}/inference_shap_values.csv\", index=False)\n\n    # Now recompute the details for our Model\n    self.log.important(f\"Recomputing Details for {self.model_name} to show latest Inference Results...\")\n    model = ModelCore(self.model_name)\n    model.details(recompute=True)\n\n    # Recompute the details so that inference model metrics are updated\n    self.log.important(f\"Recomputing Details for {self.uuid} to show latest Inference Results...\")\n    self.details(recompute=True)\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore.classification_metrics","title":"<code>classification_metrics(target_column, prediction_df)</code>  <code>staticmethod</code>","text":"<p>Compute the performance metrics for this Endpoint Args:     target_column (str): Name of the target column     prediction_df (pd.DataFrame): DataFrame with the prediction results Returns:     pd.DataFrame: DataFrame with the performance metrics</p> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>@staticmethod\ndef classification_metrics(target_column: str, prediction_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Compute the performance metrics for this Endpoint\n    Args:\n        target_column (str): Name of the target column\n        prediction_df (pd.DataFrame): DataFrame with the prediction results\n    Returns:\n        pd.DataFrame: DataFrame with the performance metrics\n    \"\"\"\n\n    # Get a list of unique labels\n    labels = prediction_df[target_column].unique()\n\n    # Calculate scores\n    scores = precision_recall_fscore_support(\n        prediction_df[target_column], prediction_df[\"prediction\"], average=None, labels=labels\n    )\n\n    # Calculate ROC AUC\n    # ROC-AUC score measures the model's ability to distinguish between classes;\n    # - A value of 0.5 indicates no discrimination (equivalent to random guessing)\n    # - A score close to 1 indicates high discriminative power\n\n    # Convert 'pred_proba' column to a 2D NumPy array\n    y_score = np.array([ast.literal_eval(x) for x in prediction_df[\"pred_proba\"]], dtype=float)\n    # y_score = np.array(prediction_df['pred_proba'].tolist())\n\n    # One-hot encode the true labels\n    lb = LabelBinarizer()\n    lb.fit(prediction_df[target_column])\n    y_true = lb.transform(prediction_df[target_column])\n\n    roc_auc = roc_auc_score(y_true, y_score, multi_class=\"ovr\", average=None)\n\n    # Put the scores into a dataframe\n    score_df = pd.DataFrame(\n        {\n            target_column: labels,\n            \"precision\": scores[0],\n            \"recall\": scores[1],\n            \"fscore\": scores[2],\n            \"roc_auc\": roc_auc,\n            \"support\": scores[3],\n        }\n    )\n\n    # Sort the target labels\n    score_df = score_df.sort_values(by=[target_column], ascending=True)\n    return score_df\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore.confusion_matrix","title":"<code>confusion_matrix(target_column, prediction_df)</code>","text":"<p>Compute the confusion matrix for this Endpoint Args:     target_column (str): Name of the target column     prediction_df (pd.DataFrame): DataFrame with the prediction results Returns:     pd.DataFrame: DataFrame with the confusion matrix</p> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>def confusion_matrix(self, target_column: str, prediction_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Compute the confusion matrix for this Endpoint\n    Args:\n        target_column (str): Name of the target column\n        prediction_df (pd.DataFrame): DataFrame with the prediction results\n    Returns:\n        pd.DataFrame: DataFrame with the confusion matrix\n    \"\"\"\n\n    y_true = prediction_df[target_column]\n    y_pred = prediction_df[\"prediction\"]\n\n    # Compute the confusion matrix\n    conf_mtx = confusion_matrix(y_true, y_pred)\n\n    # Get unique labels\n    labels = sorted(list(set(y_true) | set(y_pred)))\n\n    # Create a DataFrame\n    conf_mtx_df = pd.DataFrame(conf_mtx, index=labels, columns=labels)\n    return conf_mtx_df\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore.created","title":"<code>created()</code>","text":"<p>Return the datetime when this artifact was created</p> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>def created(self) -&gt; datetime:\n    \"\"\"Return the datetime when this artifact was created\"\"\"\n    return self.endpoint_meta[\"CreationTime\"]\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore.delete","title":"<code>delete()</code>","text":"<p>Delete an existing Endpoint: Underlying Models, Configuration, and Endpoint</p> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>def delete(self):\n    \"\"\"Delete an existing Endpoint: Underlying Models, Configuration, and Endpoint\"\"\"\n    self.delete_endpoint_models()\n\n    # Grab the Endpoint Config Name from the AWS\n    endpoint_config_name = self.endpoint_config_name()\n    try:\n        self.log.info(f\"Deleting Endpoint Config {endpoint_config_name}...\")\n        self.sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n    except botocore.exceptions.ClientError:\n        self.log.info(f\"Endpoint Config {endpoint_config_name} doesn't exist...\")\n\n    # Check for any monitoring schedules\n    response = self.sm_client.list_monitoring_schedules(EndpointName=self.uuid)\n    monitoring_schedules = response[\"MonitoringScheduleSummaries\"]\n    for schedule in monitoring_schedules:\n        self.log.info(f\"Deleting Endpoint Monitoring Schedule {schedule['MonitoringScheduleName']}...\")\n        self.sm_client.delete_monitoring_schedule(MonitoringScheduleName=schedule[\"MonitoringScheduleName\"])\n\n    # Okay now delete the Endpoint\n    try:\n        time.sleep(1)  # Let AWS catch up with any deletions performed above\n        self.log.info(f\"Deleting Endpoint {self.uuid}...\")\n        self.sm_client.delete_endpoint(EndpointName=self.uuid)\n    except botocore.exceptions.ClientError as e:\n        self.log.info(\"Endpoint ClientError...\")\n        raise e\n\n    # Now delete any data in the Cache\n    for key in self.data_storage.list_subkeys(f\"endpoint:{self.uuid}\"):\n        self.log.info(f\"Deleting Cache Key: {key}\")\n        self.data_storage.delete(key)\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore.delete_endpoint_models","title":"<code>delete_endpoint_models()</code>","text":"<p>Delete the underlying Model for an Endpoint</p> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>def delete_endpoint_models(self):\n    \"\"\"Delete the underlying Model for an Endpoint\"\"\"\n\n    # Grab the Endpoint Config Name from the AWS\n    endpoint_config_name = self.endpoint_config_name()\n\n    # Retrieve the Model Names from the Endpoint Config\n    try:\n        endpoint_config = self.sm_client.describe_endpoint_config(EndpointConfigName=endpoint_config_name)\n    except botocore.exceptions.ClientError:\n        self.log.info(f\"Endpoint Config {self.uuid} doesn't exist...\")\n        return\n    model_names = [variant[\"ModelName\"] for variant in endpoint_config[\"ProductionVariants\"]]\n    for model_name in model_names:\n        self.log.info(f\"Deleting Model {model_name}...\")\n        self.sm_client.delete_model(ModelName=model_name)\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore.details","title":"<code>details(recompute=False)</code>","text":"<p>Additional Details about this Endpoint Args:     recompute (bool): Recompute the details (default: False) Returns:     dict(dict): A dictionary of details about this Endpoint</p> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>def details(self, recompute: bool = False) -&gt; dict:\n    \"\"\"Additional Details about this Endpoint\n    Args:\n        recompute (bool): Recompute the details (default: False)\n    Returns:\n        dict(dict): A dictionary of details about this Endpoint\n    \"\"\"\n    # Check if we have cached version of the FeatureSet Details\n    details_key = f\"endpoint:{self.uuid}:details\"\n\n    cached_details = self.data_storage.get(details_key)\n    if cached_details and not recompute:\n        # Update the endpoint metrics before returning cached details\n        endpoint_metrics = self.endpoint_metrics()\n        cached_details[\"endpoint_metrics\"] = endpoint_metrics\n        return cached_details\n\n    # Fill in all the details about this Endpoint\n    details = self.summary()\n\n    # Get details from our AWS Metadata\n    details[\"status\"] = self.endpoint_meta[\"EndpointStatus\"]\n    details[\"instance\"] = self.endpoint_meta[\"InstanceType\"]\n    try:\n        details[\"instance_count\"] = self.endpoint_meta[\"ProductionVariants\"][0][\"CurrentInstanceCount\"] or \"-\"\n    except KeyError:\n        details[\"instance_count\"] = \"-\"\n    details[\"variant\"] = self.endpoint_meta[\"ProductionVariants\"][0][\"VariantName\"]\n\n    # Add the underlying model details\n    details[\"model_name\"] = self.model_name\n    model_details = self.model_details()\n    details[\"model_type\"] = model_details.get(\"model_type\", \"unknown\")\n    details[\"model_metrics\"] = model_details.get(\"model_metrics\")\n    details[\"confusion_matrix\"] = model_details.get(\"confusion_matrix\")\n    details[\"regression_predictions\"] = model_details.get(\"regression_predictions\")\n    details[\"inference_meta\"] = model_details.get(\"inference_meta\")\n\n    # Add endpoint metrics from CloudWatch\n    details[\"endpoint_metrics\"] = self.endpoint_metrics()\n\n    # Cache the details\n    self.data_storage.set(details_key, details)\n\n    # Return the details\n    return details\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore.endpoint_metrics","title":"<code>endpoint_metrics()</code>","text":"<p>Return the metrics for this endpoint Returns:     pd.DataFrame: DataFrame with the metrics for this endpoint</p> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>def endpoint_metrics(self) -&gt; pd.DataFrame:\n    \"\"\"Return the metrics for this endpoint\n    Returns:\n        pd.DataFrame: DataFrame with the metrics for this endpoint\n    \"\"\"\n\n    # Do we have it cached?\n    metrics_key = f\"endpoint:{self.uuid}:endpoint_metrics\"\n    endpoint_metrics = self.temp_storage.get(metrics_key)\n    if endpoint_metrics is not None:\n        return endpoint_metrics\n\n    # We don't have it cached so let's get it from CloudWatch\n    self.log.important(\"Updating endpoint metrics...\")\n    variant = self.endpoint_meta[\"ProductionVariants\"][0][\"VariantName\"]\n    endpoint_metrics = EndpointMetrics().get_metrics(self.uuid, variant=variant)\n    self.temp_storage.set(metrics_key, endpoint_metrics)\n    return endpoint_metrics\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore.exists","title":"<code>exists()</code>","text":"<p>Does the feature_set_name exist in the AWS Metadata?</p> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"Does the feature_set_name exist in the AWS Metadata?\"\"\"\n    if self.endpoint_meta is None:\n        self.log.debug(f\"Endpoint {self.endpoint_name} not found in AWS Metadata\")\n        return False\n    return True\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore.health_check","title":"<code>health_check()</code>","text":"<p>Perform a health check on this model Returns:     list[str]: List of health issues</p> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>def health_check(self) -&gt; list[str]:\n    \"\"\"Perform a health check on this model\n    Returns:\n        list[str]: List of health issues\n    \"\"\"\n    # Call the base class health check\n    health_issues = super().health_check()\n\n    # We're going to check for 5xx errors and no activity\n    endpoint_metrics = self.endpoint_metrics()\n\n    # Check for 5xx errors\n    num_errors = endpoint_metrics[\"Invocation5XXErrors\"].sum()\n    if num_errors &gt; 5:\n        health_issues.append(\"5xx_errors\")\n    elif num_errors &gt; 0:\n        health_issues.append(\"5xx_errors_min\")\n    else:\n        self.remove_sageworks_health_tag(\"5xx_errors\")\n        self.remove_sageworks_health_tag(\"5xx_errors_min\")\n\n    # Check for Endpoint activity\n    num_invocations = endpoint_metrics[\"Invocations\"].sum()\n    if num_invocations == 0:\n        health_issues.append(\"no_activity\")\n    else:\n        self.remove_sageworks_health_tag(\"no_activity\")\n    return health_issues\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore.is_serverless","title":"<code>is_serverless()</code>","text":"<p>Check if the current endpoint is serverless. Returns:     bool: True if the endpoint is serverless, False otherwise.</p> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>def is_serverless(self):\n    \"\"\"\n    Check if the current endpoint is serverless.\n    Returns:\n        bool: True if the endpoint is serverless, False otherwise.\n    \"\"\"\n    return \"Serverless\" in self.endpoint_meta[\"InstanceType\"]\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore.make_ready","title":"<code>make_ready()</code>","text":"<p>This is a BLOCKING method that will wait until the Endpoint is ready Returns:     bool: True if the Endpoint is ready, False otherwise</p> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>def make_ready(self) -&gt; bool:\n    \"\"\"This is a BLOCKING method that will wait until the Endpoint is ready\n    Returns:\n        bool: True if the Endpoint is ready, False otherwise\n    \"\"\"\n    self.set_status(\"ready\")\n    self.remove_sageworks_health_tag(\"needs_onboard\")\n    self.details(recompute=True)\n    return True\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore.model_details","title":"<code>model_details()</code>","text":"<p>Return the details about the model used in this Endpoint</p> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>def model_details(self) -&gt; dict:\n    \"\"\"Return the details about the model used in this Endpoint\"\"\"\n    if self.model_name == \"unknown\":\n        return {}\n    else:\n        model = ModelCore(self.model_name)\n        if model.exists():\n            return model.details()\n        else:\n            return {}\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore.model_type","title":"<code>model_type()</code>","text":"<p>Return the type of model used in this Endpoint</p> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>def model_type(self) -&gt; str:\n    \"\"\"Return the type of model used in this Endpoint\"\"\"\n    return self.details().get(\"model_type\", \"unknown\")\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore.modified","title":"<code>modified()</code>","text":"<p>Return the datetime when this artifact was last modified</p> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>def modified(self) -&gt; datetime:\n    \"\"\"Return the datetime when this artifact was last modified\"\"\"\n    return self.endpoint_meta[\"LastModifiedTime\"]\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore.predict","title":"<code>predict(feature_df)</code>","text":"<p>Run inference/prediction on the given Feature DataFrame Args:     feature_df (pd.DataFrame): DataFrame to run predictions on (must have superset of features) Returns:     pd.DataFrame: Return the feature DataFrame with two additional columns (prediction, pred_proba)</p> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>def predict(self, feature_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Run inference/prediction on the given Feature DataFrame\n    Args:\n        feature_df (pd.DataFrame): DataFrame to run predictions on (must have superset of features)\n    Returns:\n        pd.DataFrame: Return the feature DataFrame with two additional columns (prediction, pred_proba)\n    \"\"\"\n\n    # Create our Endpoint Predictor Class\n    predictor = Predictor(\n        self.endpoint_name,\n        sagemaker_session=self.sm_session,\n        serializer=CSVSerializer(),\n        deserializer=CSVDeserializer(),\n    )\n\n    # Now split up the dataframe into 500 row chunks, send those chunks to our\n    # endpoint (with error handling) and stitch all the chunks back together\n    df_list = []\n    for index in range(0, len(feature_df), 500):\n        print(\"Processing...\")\n\n        # Compute partial DataFrames, add them to a list, and concatenate at the end\n        partial_df = self._endpoint_error_handling(predictor, feature_df[index : index + 500])\n        df_list.append(partial_df)\n\n    # Concatenate the dataframes\n    combined_df = pd.concat(df_list, ignore_index=True)\n\n    # Convert data to numeric\n    # Note: Since we're using CSV serializers numeric columns often get changed to generic 'object' types\n\n    # Hard Conversion\n    # Note: If are string/object columns we want to use 'ignore' here so those columns\n    #       won't raise an error (columns maintain current type)\n    converted_df = combined_df.apply(pd.to_numeric, errors=\"ignore\")\n\n    # Soft Conversion\n    # Convert columns to the best possible dtype that supports the pd.NA missing value.\n    converted_df = converted_df.convert_dtypes()\n\n    # Return the Dataframe\n    return converted_df\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore.refresh_meta","title":"<code>refresh_meta()</code>","text":"<p>Refresh the Artifact's metadata</p> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>def refresh_meta(self):\n    \"\"\"Refresh the Artifact's metadata\"\"\"\n    self.endpoint_meta = self.aws_broker.get_metadata(ServiceCategory.ENDPOINTS, force_refresh=True).get(\n        self.endpoint_name\n    )\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore.regression_metrics","title":"<code>regression_metrics(target_column, prediction_df)</code>  <code>staticmethod</code>","text":"<p>Compute the performance metrics for this Endpoint Args:     target_column (str): Name of the target column     prediction_df (pd.DataFrame): DataFrame with the prediction results Returns:     pd.DataFrame: DataFrame with the performance metrics</p> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>@staticmethod\ndef regression_metrics(target_column: str, prediction_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Compute the performance metrics for this Endpoint\n    Args:\n        target_column (str): Name of the target column\n        prediction_df (pd.DataFrame): DataFrame with the prediction results\n    Returns:\n        pd.DataFrame: DataFrame with the performance metrics\n    \"\"\"\n\n    # Compute the metrics\n    y_true = prediction_df[target_column]\n    y_pred = prediction_df[\"prediction\"]\n\n    mae = mean_absolute_error(y_true, y_pred)\n    rmse = sqrt(mean_squared_error(y_true, y_pred))\n    r2 = r2_score(y_true, y_pred)\n    # Mean Absolute Percentage Error\n    mape = np.mean(np.where(y_true != 0, np.abs((y_true - y_pred) / y_true), np.abs(y_true - y_pred))) * 100\n    # Median Absolute Error\n    medae = median_absolute_error(y_true, y_pred)\n\n    # Return the metrics\n    return pd.DataFrame.from_records([{\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2, \"MAPE\": mape, \"MedAE\": medae}])\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore.regression_predictions","title":"<code>regression_predictions(target, prediction_df)</code>","text":"<p>Compute the regression predictions for this Endpoint Args:     target (str): Name of the target column     prediction_df (pd.DataFrame): DataFrame with the prediction results Returns:     pd.DataFrame: DataFrame with the regression predictions</p> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>def regression_predictions(self, target: str, prediction_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Compute the regression predictions for this Endpoint\n    Args:\n        target (str): Name of the target column\n        prediction_df (pd.DataFrame): DataFrame with the prediction results\n    Returns:\n        pd.DataFrame: DataFrame with the regression predictions\n    \"\"\"\n\n    # Return the predictions\n    return prediction_df[[target, \"prediction\"]]\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore.shap_values","title":"<code>shap_values(model, X)</code>  <code>staticmethod</code>","text":"<p>Compute the SHAP values for this Model Args:     model (Model): Model object     X (pd.DataFrame): DataFrame with the prediction results Returns:     pd.DataFrame: DataFrame with the SHAP values</p> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>@staticmethod\ndef shap_values(model, X: pd.DataFrame) -&gt; np.array:\n    \"\"\"Compute the SHAP values for this Model\n    Args:\n        model (Model): Model object\n        X (pd.DataFrame): DataFrame with the prediction results\n    Returns:\n        pd.DataFrame: DataFrame with the SHAP values\n    \"\"\"\n    # Note: For Tree-based models like decision trees, random forests, XGBoost, LightGBM,\n    explainer = shap.TreeExplainer(model)\n    shap_vals = explainer.shap_values(X)\n    return shap_vals\n</code></pre>"},{"location":"core_classes/artifacts/endpoint_core/#sageworks.core.artifacts.endpoint_core.EndpointCore.size","title":"<code>size()</code>","text":"<p>Return the size of this data in MegaBytes</p> Source code in <code>src/sageworks/core/artifacts/endpoint_core.py</code> <pre><code>def size(self) -&gt; float:\n    \"\"\"Return the size of this data in MegaBytes\"\"\"\n    return 0.0\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/","title":"FeatureSetCore","text":"<p>API Classes</p> <p>Found a method here you want to use? The API Classes have method pass-through so just call the method on the FeatureSet API Class and voil\u00e0 it works the same.</p> <p>FeatureSet: SageWorks Feature Set accessible through Athena</p>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore","title":"<code>FeatureSetCore</code>","text":"<p>             Bases: <code>Artifact</code></p> <p>FeatureSetCore: SageWorks FeatureSetCore Class</p> Common Usage <pre><code>my_features = FeatureSetCore(feature_uuid)\nmy_features.summary()\nmy_features.details()\n</code></pre> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>class FeatureSetCore(Artifact):\n    \"\"\"FeatureSetCore: SageWorks FeatureSetCore Class\n\n    Common Usage:\n        ```\n        my_features = FeatureSetCore(feature_uuid)\n        my_features.summary()\n        my_features.details()\n        ```\n    \"\"\"\n\n    def __init__(self, feature_set_uuid: str, force_refresh: bool = False):\n        \"\"\"FeatureSetCore Initialization\n\n        Args:\n            feature_set_uuid (str): Name of Feature Set\n            force_refresh (bool): Force a refresh of the Feature Set metadata (default: False)\n        \"\"\"\n\n        # Call superclass init\n        super().__init__(feature_set_uuid)\n\n        # Setup our AWS Broker catalog metadata\n        _catalog_meta = self.aws_broker.get_metadata(ServiceCategory.FEATURE_STORE, force_refresh=force_refresh)\n        self.feature_meta = _catalog_meta.get(self.uuid)\n\n        # Sanity check and then set up our FeatureSet attributes\n        if self.feature_meta is None:\n            self.log.important(f\"Could not find feature set {self.uuid} within current visibility scope\")\n            self.data_source = None\n            return\n        else:\n            self.record_id = self.feature_meta[\"RecordIdentifierFeatureName\"]\n            self.event_time = self.feature_meta[\"EventTimeFeatureName\"]\n\n            # Pull Athena and S3 Storage information from metadata\n            self.athena_database = self.feature_meta[\"sageworks_meta\"].get(\"athena_database\")\n            self.athena_table = self.feature_meta[\"sageworks_meta\"].get(\"athena_table\")\n            self.s3_storage = self.feature_meta[\"sageworks_meta\"].get(\"s3_storage\")\n\n            # Create our internal DataSource (hardcoded to Athena for now)\n            self.data_source = AthenaSource(self.athena_table, self.athena_database)\n\n        # Spin up our Feature Store\n        self.feature_store = FeatureStore(self.sm_session)\n\n        # Call superclass post_init\n        super().__post_init__()\n\n        # All done\n        self.log.info(f\"FeatureSet Initialized: {self.uuid}\")\n\n    def refresh_meta(self):\n        \"\"\"Internal: Refresh our internal AWS Feature Store metadata\"\"\"\n        self.log.info(\"Calling refresh_meta() on the underlying DataSource\")\n        self.data_source.refresh_meta()\n\n    def exists(self) -&gt; bool:\n        \"\"\"Does the feature_set_name exist in the AWS Metadata?\"\"\"\n        if self.feature_meta is None:\n            self.log.debug(f\"FeatureSet {self.uuid} not found in AWS Metadata!\")\n            return False\n        return True\n\n    def health_check(self) -&gt; list[str]:\n        \"\"\"Perform a health check on this model\n\n        Returns:\n            list[str]: List of health issues\n        \"\"\"\n        # Call the base class health check\n        health_issues = super().health_check()\n\n        # If we have a 'needs_onboard' in the health check then just return\n        if \"needs_onboard\" in health_issues:\n            return health_issues\n\n        # Check our DataSource\n        if not self.data_source.exists():\n            self.log.critical(f\"Data Source check failed for {self.uuid}\")\n            self.log.critical(\"Delete this Feature Set and recreate it to fix this issue\")\n            health_issues.append(\"data_source_missing\")\n        return health_issues\n\n    def aws_meta(self) -&gt; dict:\n        \"\"\"Get ALL the AWS metadata for this artifact\"\"\"\n        return self.feature_meta\n\n    def arn(self) -&gt; str:\n        \"\"\"AWS ARN (Amazon Resource Name) for this artifact\"\"\"\n        return self.feature_meta[\"FeatureGroupArn\"]\n\n    def size(self) -&gt; float:\n        \"\"\"Return the size of the internal DataSource in MegaBytes\"\"\"\n        return self.data_source.size()\n\n    def column_names(self) -&gt; list[str]:\n        \"\"\"Return the column names of the Feature Set\"\"\"\n        return list(self.column_details().keys())\n\n    def column_types(self) -&gt; list[str]:\n        \"\"\"Return the column types of the Feature Set\"\"\"\n        return list(self.column_details().values())\n\n    def column_details(self, view: str = \"all\") -&gt; dict:\n        \"\"\"Return the column details of the Feature Set\n\n        Args:\n            view (str): The view to get column details for (default: \"all\")\n\n        Returns:\n            dict: The column details of the Feature Set\n\n        Notes:\n            We can't call just call self.data_source.column_details() because FeatureSets have different\n            types, so we need to overlay that type information on top of the DataSource type information\n        \"\"\"\n        fs_details = {item[\"FeatureName\"]: item[\"FeatureType\"] for item in self.feature_meta[\"FeatureDefinitions\"]}\n        ds_details = self.data_source.column_details(view)\n\n        # Overlay the FeatureSet type information on top of the DataSource type information\n        for col, dtype in ds_details.items():\n            ds_details[col] = fs_details.get(col, dtype)\n        return ds_details\n\n        # Not going to use these for now\n        \"\"\"\n        internal = {\n            \"write_time\": \"Timestamp\",\n            \"api_invocation_time\": \"Timestamp\",\n            \"is_deleted\": \"Boolean\",\n        }\n        details.update(internal)\n        return details\n        \"\"\"\n\n    def get_display_columns(self) -&gt; list[str]:\n        \"\"\"Get the display columns for this FeatureSet\n\n        Returns:\n            list[str]: The display columns for this FeatureSet\n\n        Notes:\n            This just pulls the display columns from the underlying DataSource\n        \"\"\"\n        return self.data_source.get_display_columns()\n\n    def set_display_columns(self, display_columns: list[str]):\n        \"\"\"Set the display columns for this FeatureSet\n\n        Args:\n            display_columns (list[str]): The display columns for this FeatureSet\n\n        Notes:\n            This just sets the display columns for the underlying DataSource\n        \"\"\"\n        self.data_source.set_display_columns(display_columns)\n        self.onboard()\n\n    def num_columns(self) -&gt; int:\n        \"\"\"Return the number of columns of the Feature Set\"\"\"\n        return len(self.column_names())\n\n    def num_rows(self) -&gt; int:\n        \"\"\"Return the number of rows of the internal DataSource\"\"\"\n        return self.data_source.num_rows()\n\n    def query(self, query: str) -&gt; pd.DataFrame:\n        \"\"\"Query the internal DataSource\"\"\"\n        return self.data_source.query(query)\n\n    def aws_url(self):\n        \"\"\"The AWS URL for looking at/querying the underlying data source\"\"\"\n        return self.data_source.details().get(\"aws_url\", \"unknown\")\n\n    def created(self) -&gt; datetime:\n        \"\"\"Return the datetime when this artifact was created\"\"\"\n        return self.feature_meta[\"CreationTime\"]\n\n    def modified(self) -&gt; datetime:\n        \"\"\"Return the datetime when this artifact was last modified\"\"\"\n        # Note: We can't currently figure out how to this from AWS Metadata\n        return self.feature_meta[\"CreationTime\"]\n\n    def get_data_source(self) -&gt; DataSourceFactory:\n        \"\"\"Return the underlying DataSource object\"\"\"\n        return self.data_source\n\n    def get_feature_store(self) -&gt; FeatureStore:\n        \"\"\"Return the underlying AWS FeatureStore object. This can be useful for more advanced usage\n        with create_dataset() such as Joins and time ranges and a host of other options\n        See: https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store-create-a-dataset.html\n        \"\"\"\n        return self.feature_store\n\n    def create_s3_training_data(self) -&gt; str:\n        \"\"\"Create some Training Data (S3 CSV) from a Feature Set using standard options. If you want\n        additional options/features use the get_feature_store() method and see AWS docs for all\n        the details: https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store-create-a-dataset.html\n        Returns:\n            str: The full path/file for the CSV file created by Feature Store create_dataset()\n        \"\"\"\n\n        # Set up the S3 Query results path\n        date_time = datetime.now(timezone.utc).strftime(\"%Y-%m-%d_%H:%M:%S\")\n        s3_output_path = self.feature_sets_s3_path + f\"/{self.uuid}/datasets/all_{date_time}\"\n\n        # Do we have a training view?\n        training_view = self.get_training_view_table()\n        if training_view:\n            self.log.important(f\"Creating S3 Training Data from Training View {training_view}...\")\n            table_name = training_view\n        else:\n            self.log.warning(f\"No Training View found for {self.uuid}, using FeatureSet directly...\")\n            table_name = self.athena_table\n\n        # Make a query that gets all the data from the FeatureSet\n        query = f\"SELECT * FROM {table_name}\"\n\n        \"\"\"\n        Note: We're going to circle back to this\n        # Get the snapshot query\n        query = self.snapshot_query(table_name=table_name)\n        \"\"\"\n\n        # Make the query\n        athena_query = FeatureGroup(name=self.uuid, sagemaker_session=self.sm_session).athena_query()\n        athena_query.run(query, output_location=s3_output_path)\n        athena_query.wait()\n        query_execution = athena_query.get_query_execution()\n\n        # Get the full path to the S3 files with the results\n        full_s3_path = s3_output_path + f\"/{query_execution['QueryExecution']['QueryExecutionId']}.csv\"\n        return full_s3_path\n\n    def snapshot_query(self, table_name: str = None) -&gt; str:\n        \"\"\"An Athena query to get the latest snapshot of features\n\n        Args:\n            table_name (str): The name of the table to query (default: None)\n\n        Returns:\n            str: The Athena query to get the latest snapshot of features\n        \"\"\"\n        # Remove FeatureGroup metadata columns that might have gotten added\n        columns = self.column_names()\n        filter_columns = [\"write_time\", \"api_invocation_time\", \"is_deleted\"]\n        columns = \", \".join(['\"' + x + '\"' for x in columns if x not in filter_columns])\n\n        query = (\n            f\"SELECT {columns} \"\n            f\"    FROM (SELECT *, row_number() OVER (PARTITION BY {self.record_id} \"\n            f\"        ORDER BY {self.event_time} desc, api_invocation_time DESC, write_time DESC) AS row_num \"\n            f'        FROM \"{table_name}\") '\n            \"    WHERE row_num = 1 and NOT is_deleted;\"\n        )\n        return query\n\n    def details(self, recompute: bool = False) -&gt; dict[dict]:\n        \"\"\"Additional Details about this FeatureSet Artifact\n\n        Args:\n            recompute (bool): Recompute the details (default: False)\n\n        Returns:\n            dict(dict): A dictionary of details about this FeatureSet\n        \"\"\"\n\n        # Check if we have cached version of the FeatureSet Details\n        storage_key = f\"feature_set:{self.uuid}:details\"\n        cached_details = self.data_storage.get(storage_key)\n        if cached_details and not recompute:\n            return cached_details\n\n        self.log.info(f\"Recomputing FeatureSet Details ({self.uuid})...\")\n        details = self.summary()\n        details[\"aws_url\"] = self.aws_url()\n\n        # Now get a summary of the underlying DataSource\n        details[\"storage_summary\"] = self.data_source.summary()\n\n        # Number of Columns\n        details[\"num_columns\"] = self.num_columns()\n\n        # Number of Rows\n        details[\"num_rows\"] = self.num_rows()\n\n        # Additional Details\n        details[\"sageworks_status\"] = self.get_status()\n        details[\"sageworks_input\"] = self.get_input()\n        details[\"sageworks_tags\"] = \":\".join(self.sageworks_tags())\n\n        # Underlying Storage Details\n        details[\"storage_type\"] = \"athena\"  # TODO: Add RDS support\n        details[\"storage_uuid\"] = self.data_source.uuid\n\n        # Add the column details and column stats\n        details[\"column_details\"] = self.column_details()\n        details[\"column_stats\"] = self.column_stats()\n\n        # Cache the details\n        self.data_storage.set(storage_key, details)\n\n        # Return the details data\n        return details\n\n    def delete(self):\n        \"\"\"Delete the Feature Set: Feature Group, Catalog Table, and S3 Storage Objects\"\"\"\n\n        # Delete the Feature Group and ensure that it gets deleted\n        self.log.important(f\"Deleting FeatureSet {self.uuid}...\")\n        remove_fg = FeatureGroup(name=self.uuid, sagemaker_session=self.sm_session)\n        remove_fg.delete()\n        self.ensure_feature_group_deleted(remove_fg)\n\n        # Delete our underlying DataSource (Data Catalog Table and S3 Storage Objects)\n        self.data_source.delete()\n\n        # Delete the training view\n        self.delete_training_view()\n\n        # Feature Sets can often have a lot of cruft so delete the entire bucket/prefix\n        s3_delete_path = self.feature_sets_s3_path + f\"/{self.uuid}\"\n        self.log.info(f\"Deleting All FeatureSet S3 Storage Objects {s3_delete_path}\")\n        wr.s3.delete_objects(s3_delete_path, boto3_session=self.boto_session)\n\n        # Now delete any data in the Cache\n        for key in self.data_storage.list_subkeys(f\"feature_set:{self.uuid}\"):\n            self.log.info(f\"Deleting Cache Key: {key}\")\n            self.data_storage.delete(key)\n\n        # Force a refresh of the AWS Metadata (to make sure references to deleted artifacts are gone)\n        self.aws_broker.get_metadata(ServiceCategory.FEATURE_STORE, force_refresh=True)\n\n    def ensure_feature_group_deleted(self, feature_group):\n        status = \"Deleting\"\n        while status == \"Deleting\":\n            self.log.debug(\"FeatureSet being Deleted...\")\n            try:\n                status = feature_group.describe().get(\"FeatureGroupStatus\")\n            except botocore.exceptions.ClientError as error:\n                # For ResourceNotFound/ValidationException, this is fine, otherwise raise all other exceptions\n                if error.response[\"Error\"][\"Code\"] in [\"ResourceNotFound\", \"ValidationException\"]:\n                    break\n                else:\n                    raise error\n            time.sleep(1)\n        self.log.info(f\"FeatureSet {feature_group.name} successfully deleted\")\n\n    def create_default_training_view(self):\n        \"\"\"Create a default view in Athena that assigns roughly 80% of the data to training\"\"\"\n\n        # Create the view name\n        view_name = f\"{self.athena_table}_training\"\n        self.log.important(f\"Creating default Training View {view_name}...\")\n\n        # Construct the CREATE VIEW query with random assignment\n        create_view_query = f\"\"\"\n        CREATE OR REPLACE VIEW {view_name} AS\n        SELECT *, CASE\n            WHEN RAND() &lt; 0.8 THEN 1  -- Assign roughly 80% to training\n            ELSE 0  -- Assign roughly 20% to hold-out\n        END AS training\n        FROM {self.athena_table}\n        \"\"\"\n\n        # Execute the CREATE VIEW query\n        self.data_source.execute_statement(create_view_query)\n\n    def create_training_view(self, id_column: str, hold_out_ids: list[str]):\n        \"\"\"Create a view in Athena that marks hold out ids for this FeatureSet\n\n        Args:\n            id_column (str): The name of the id column in the output DataFrame.\n            hold_out_ids (list[str]): The list of hold out ids.\n        \"\"\"\n\n        # Create the view name\n        view_name = f\"{self.athena_table}_training\"\n        self.log.important(f\"Creating Training View {view_name}...\")\n\n        # Format the list of hold out ids for SQL IN clause\n        if hold_out_ids and all(isinstance(id, str) for id in hold_out_ids):\n            formatted_hold_out_ids = \", \".join(f\"'{id}'\" for id in hold_out_ids)\n        else:\n            formatted_hold_out_ids = \", \".join(map(str, hold_out_ids))\n\n        # Construct the CREATE VIEW query\n        create_view_query = f\"\"\"\n        CREATE OR REPLACE VIEW {view_name} AS\n        SELECT *, CASE\n            WHEN {id_column} IN ({formatted_hold_out_ids}) THEN 0\n            ELSE 1\n        END AS training\n        FROM {self.athena_table}\n        \"\"\"\n\n        # Execute the CREATE VIEW query\n        self.data_source.execute_statement(create_view_query)\n\n    def get_training_view_table(self, create: bool = True) -&gt; Union[str, None]:\n        \"\"\"Get the name of the training view for this FeatureSet\n        Args:\n            create (bool): Create the training view if it doesn't exist (default=True)\n        Returns:\n            str: The name of the training view for this FeatureSet (or None if it doesn't exist)\n        \"\"\"\n        training_view_name = f\"{self.athena_table}_training\"\n        glue_client = self.boto_session.client(\"glue\")\n        try:\n            glue_client.get_table(DatabaseName=self.athena_database, Name=training_view_name)\n            return training_view_name\n        except glue_client.exceptions.EntityNotFoundException:\n            if not create:\n                return None\n            self.log.warning(f\"Training View for {self.uuid} doesn't exist, creating a default one...\")\n            self.create_default_training_view()\n            time.sleep(1)  # Give AWS a second to catch up\n            return training_view_name\n\n    def delete_training_view(self):\n        \"\"\"Delete the training view for this FeatureSet\"\"\"\n        training_view_table = self.get_training_view_table()\n        if training_view_table is not None:\n            self.log.info(f\"Deleting Training View {training_view_table} for {self.uuid}\")\n            glue_client = self.boto_session.client(\"glue\")\n            glue_client.delete_table(DatabaseName=self.athena_database, Name=training_view_table)\n\n    def descriptive_stats(self, recompute: bool = False) -&gt; dict:\n        \"\"\"Get the descriptive stats for the numeric columns of the underlying DataSource\n        Args:\n            recompute (bool): Recompute the descriptive stats (default=False)\n        Returns:\n            dict: A dictionary of descriptive stats for the numeric columns\n        \"\"\"\n        return self.data_source.descriptive_stats(recompute)\n\n    def sample(self, recompute: bool = False) -&gt; pd.DataFrame:\n        \"\"\"Get a sample of the data from the underlying DataSource\n        Args:\n            recompute (bool): Recompute the sample (default=False)\n        Returns:\n            pd.DataFrame: A sample of the data from the underlying DataSource\n        \"\"\"\n        return self.data_source.sample(recompute)\n\n    def outliers(self, scale: float = 1.5, recompute: bool = False) -&gt; pd.DataFrame:\n        \"\"\"Compute outliers for all the numeric columns in a DataSource\n        Args:\n            scale (float): The scale to use for the IQR (default: 1.5)\n            recompute (bool): Recompute the outliers (default: False)\n        Returns:\n            pd.DataFrame: A DataFrame of outliers from this DataSource\n        Notes:\n            Uses the IQR * 1.5 (~= 2.5 Sigma) method to compute outliers\n            The scale parameter can be adjusted to change the IQR multiplier\n        \"\"\"\n        return self.data_source.outliers(scale=scale, recompute=recompute)\n\n    def smart_sample(self) -&gt; pd.DataFrame:\n        \"\"\"Get a SMART sample dataframe from this FeatureSet\n        Returns:\n            pd.DataFrame: A combined DataFrame of sample data + outliers\n        \"\"\"\n        return self.data_source.smart_sample()\n\n    def anomalies(self) -&gt; pd.DataFrame:\n        \"\"\"Get a set of anomalous data from the underlying DataSource\n        Returns:\n            pd.DataFrame: A dataframe of anomalies from the underlying DataSource\n        \"\"\"\n\n        # FIXME: Mock this for now\n        anom_df = self.sample().copy()\n        anom_df[\"anomaly_score\"] = np.random.rand(anom_df.shape[0])\n        anom_df[\"cluster\"] = np.random.randint(0, 10, anom_df.shape[0])\n        anom_df[\"x\"] = np.random.rand(anom_df.shape[0])\n        anom_df[\"y\"] = np.random.rand(anom_df.shape[0])\n        return anom_df\n\n    def value_counts(self, recompute: bool = False) -&gt; dict:\n        \"\"\"Get the value counts for the string columns of the underlying DataSource\n        Args:\n            recompute (bool): Recompute the value counts (default=False)\n        Returns:\n            dict: A dictionary of value counts for the string columns\n        \"\"\"\n        return self.data_source.value_counts(recompute)\n\n    def correlations(self, recompute: bool = False) -&gt; dict:\n        \"\"\"Get the correlations for the numeric columns of the underlying DataSource\n        Args:\n            recompute (bool): Recompute the value counts (default=False)\n        Returns:\n            dict: A dictionary of correlations for the numeric columns\n        \"\"\"\n        return self.data_source.correlations(recompute)\n\n    def column_stats(self, recompute: bool = False) -&gt; dict[dict]:\n        \"\"\"Compute Column Stats for all the columns in the FeatureSets underlying DataSource\n        Args:\n            recompute (bool): Recompute the column stats (default: False)\n        Returns:\n            dict(dict): A dictionary of stats for each column this format\n            NB: String columns will NOT have num_zeros and descriptive_stats\n             {'col1': {'dtype': 'string', 'unique': 4321, 'nulls': 12},\n              'col2': {'dtype': 'int', 'unique': 4321, 'nulls': 12, 'num_zeros': 100, 'descriptive_stats': {...}},\n              ...}\n        \"\"\"\n\n        # Grab the column stats from our DataSource\n        ds_column_stats = self.data_source.column_stats(recompute)\n\n        # Map the types from our DataSource to the FeatureSet types\n        fs_type_mapper = self.column_details()\n        for col, details in ds_column_stats.items():\n            details[\"fs_dtype\"] = fs_type_mapper.get(col, \"unknown\")\n\n        return ds_column_stats\n\n    def ready(self) -&gt; bool:\n        \"\"\"Is the FeatureSet ready? Is initial setup complete and expected metadata populated?\n        Note: Since FeatureSet is a composite of DataSource and FeatureGroup, we need to\n           check both to see if the FeatureSet is ready.\"\"\"\n\n        # Check the expected metadata for the FeatureSet\n        expected_meta = self.expected_meta()\n        existing_meta = self.sageworks_meta()\n        feature_set_ready = set(existing_meta.keys()).issuperset(expected_meta)\n        if not feature_set_ready:\n            self.log.info(f\"FeatureSet {self.uuid} is not ready!\")\n            return False\n\n        # Okay now call/return the DataSource ready() method\n        return self.data_source.ready()\n\n    def make_ready(self) -&gt; bool:\n        \"\"\"This is a BLOCKING method that will wait until the FeatureSet is ready\"\"\"\n\n        # Call our underlying DataSource make_ready method\n        self.data_source.refresh_meta()\n        if not self.data_source.exists():\n            self.log.critical(f\"Data Source check failed for {self.uuid}\")\n            self.log.critical(\"Delete this Feature Set and recreate it to fix this issue\")\n            return False\n        if not self.data_source.ready():\n            self.data_source.make_ready()\n\n        # Set ourselves to ready\n        self.set_status(\"ready\")\n        self.remove_sageworks_health_tag(\"needs_onboard\")\n        self.details(recompute=True)\n        return True\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.__init__","title":"<code>__init__(feature_set_uuid, force_refresh=False)</code>","text":"<p>FeatureSetCore Initialization</p> <p>Parameters:</p> Name Type Description Default <code>feature_set_uuid</code> <code>str</code> <p>Name of Feature Set</p> required <code>force_refresh</code> <code>bool</code> <p>Force a refresh of the Feature Set metadata (default: False)</p> <code>False</code> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def __init__(self, feature_set_uuid: str, force_refresh: bool = False):\n    \"\"\"FeatureSetCore Initialization\n\n    Args:\n        feature_set_uuid (str): Name of Feature Set\n        force_refresh (bool): Force a refresh of the Feature Set metadata (default: False)\n    \"\"\"\n\n    # Call superclass init\n    super().__init__(feature_set_uuid)\n\n    # Setup our AWS Broker catalog metadata\n    _catalog_meta = self.aws_broker.get_metadata(ServiceCategory.FEATURE_STORE, force_refresh=force_refresh)\n    self.feature_meta = _catalog_meta.get(self.uuid)\n\n    # Sanity check and then set up our FeatureSet attributes\n    if self.feature_meta is None:\n        self.log.important(f\"Could not find feature set {self.uuid} within current visibility scope\")\n        self.data_source = None\n        return\n    else:\n        self.record_id = self.feature_meta[\"RecordIdentifierFeatureName\"]\n        self.event_time = self.feature_meta[\"EventTimeFeatureName\"]\n\n        # Pull Athena and S3 Storage information from metadata\n        self.athena_database = self.feature_meta[\"sageworks_meta\"].get(\"athena_database\")\n        self.athena_table = self.feature_meta[\"sageworks_meta\"].get(\"athena_table\")\n        self.s3_storage = self.feature_meta[\"sageworks_meta\"].get(\"s3_storage\")\n\n        # Create our internal DataSource (hardcoded to Athena for now)\n        self.data_source = AthenaSource(self.athena_table, self.athena_database)\n\n    # Spin up our Feature Store\n    self.feature_store = FeatureStore(self.sm_session)\n\n    # Call superclass post_init\n    super().__post_init__()\n\n    # All done\n    self.log.info(f\"FeatureSet Initialized: {self.uuid}\")\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.anomalies","title":"<code>anomalies()</code>","text":"<p>Get a set of anomalous data from the underlying DataSource Returns:     pd.DataFrame: A dataframe of anomalies from the underlying DataSource</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def anomalies(self) -&gt; pd.DataFrame:\n    \"\"\"Get a set of anomalous data from the underlying DataSource\n    Returns:\n        pd.DataFrame: A dataframe of anomalies from the underlying DataSource\n    \"\"\"\n\n    # FIXME: Mock this for now\n    anom_df = self.sample().copy()\n    anom_df[\"anomaly_score\"] = np.random.rand(anom_df.shape[0])\n    anom_df[\"cluster\"] = np.random.randint(0, 10, anom_df.shape[0])\n    anom_df[\"x\"] = np.random.rand(anom_df.shape[0])\n    anom_df[\"y\"] = np.random.rand(anom_df.shape[0])\n    return anom_df\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.arn","title":"<code>arn()</code>","text":"<p>AWS ARN (Amazon Resource Name) for this artifact</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def arn(self) -&gt; str:\n    \"\"\"AWS ARN (Amazon Resource Name) for this artifact\"\"\"\n    return self.feature_meta[\"FeatureGroupArn\"]\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.aws_meta","title":"<code>aws_meta()</code>","text":"<p>Get ALL the AWS metadata for this artifact</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def aws_meta(self) -&gt; dict:\n    \"\"\"Get ALL the AWS metadata for this artifact\"\"\"\n    return self.feature_meta\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.aws_url","title":"<code>aws_url()</code>","text":"<p>The AWS URL for looking at/querying the underlying data source</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def aws_url(self):\n    \"\"\"The AWS URL for looking at/querying the underlying data source\"\"\"\n    return self.data_source.details().get(\"aws_url\", \"unknown\")\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.column_details","title":"<code>column_details(view='all')</code>","text":"<p>Return the column details of the Feature Set</p> <p>Parameters:</p> Name Type Description Default <code>view</code> <code>str</code> <p>The view to get column details for (default: \"all\")</p> <code>'all'</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The column details of the Feature Set</p> Notes <p>We can't call just call self.data_source.column_details() because FeatureSets have different types, so we need to overlay that type information on top of the DataSource type information</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def column_details(self, view: str = \"all\") -&gt; dict:\n    \"\"\"Return the column details of the Feature Set\n\n    Args:\n        view (str): The view to get column details for (default: \"all\")\n\n    Returns:\n        dict: The column details of the Feature Set\n\n    Notes:\n        We can't call just call self.data_source.column_details() because FeatureSets have different\n        types, so we need to overlay that type information on top of the DataSource type information\n    \"\"\"\n    fs_details = {item[\"FeatureName\"]: item[\"FeatureType\"] for item in self.feature_meta[\"FeatureDefinitions\"]}\n    ds_details = self.data_source.column_details(view)\n\n    # Overlay the FeatureSet type information on top of the DataSource type information\n    for col, dtype in ds_details.items():\n        ds_details[col] = fs_details.get(col, dtype)\n    return ds_details\n\n    # Not going to use these for now\n    \"\"\"\n    internal = {\n        \"write_time\": \"Timestamp\",\n        \"api_invocation_time\": \"Timestamp\",\n        \"is_deleted\": \"Boolean\",\n    }\n    details.update(internal)\n    return details\n    \"\"\"\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.column_names","title":"<code>column_names()</code>","text":"<p>Return the column names of the Feature Set</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def column_names(self) -&gt; list[str]:\n    \"\"\"Return the column names of the Feature Set\"\"\"\n    return list(self.column_details().keys())\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.column_stats","title":"<code>column_stats(recompute=False)</code>","text":"<p>Compute Column Stats for all the columns in the FeatureSets underlying DataSource Args:     recompute (bool): Recompute the column stats (default: False) Returns:     dict(dict): A dictionary of stats for each column this format     NB: String columns will NOT have num_zeros and descriptive_stats      {'col1': {'dtype': 'string', 'unique': 4321, 'nulls': 12},       'col2': {'dtype': 'int', 'unique': 4321, 'nulls': 12, 'num_zeros': 100, 'descriptive_stats': {...}},       ...}</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def column_stats(self, recompute: bool = False) -&gt; dict[dict]:\n    \"\"\"Compute Column Stats for all the columns in the FeatureSets underlying DataSource\n    Args:\n        recompute (bool): Recompute the column stats (default: False)\n    Returns:\n        dict(dict): A dictionary of stats for each column this format\n        NB: String columns will NOT have num_zeros and descriptive_stats\n         {'col1': {'dtype': 'string', 'unique': 4321, 'nulls': 12},\n          'col2': {'dtype': 'int', 'unique': 4321, 'nulls': 12, 'num_zeros': 100, 'descriptive_stats': {...}},\n          ...}\n    \"\"\"\n\n    # Grab the column stats from our DataSource\n    ds_column_stats = self.data_source.column_stats(recompute)\n\n    # Map the types from our DataSource to the FeatureSet types\n    fs_type_mapper = self.column_details()\n    for col, details in ds_column_stats.items():\n        details[\"fs_dtype\"] = fs_type_mapper.get(col, \"unknown\")\n\n    return ds_column_stats\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.column_types","title":"<code>column_types()</code>","text":"<p>Return the column types of the Feature Set</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def column_types(self) -&gt; list[str]:\n    \"\"\"Return the column types of the Feature Set\"\"\"\n    return list(self.column_details().values())\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.correlations","title":"<code>correlations(recompute=False)</code>","text":"<p>Get the correlations for the numeric columns of the underlying DataSource Args:     recompute (bool): Recompute the value counts (default=False) Returns:     dict: A dictionary of correlations for the numeric columns</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def correlations(self, recompute: bool = False) -&gt; dict:\n    \"\"\"Get the correlations for the numeric columns of the underlying DataSource\n    Args:\n        recompute (bool): Recompute the value counts (default=False)\n    Returns:\n        dict: A dictionary of correlations for the numeric columns\n    \"\"\"\n    return self.data_source.correlations(recompute)\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.create_default_training_view","title":"<code>create_default_training_view()</code>","text":"<p>Create a default view in Athena that assigns roughly 80% of the data to training</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def create_default_training_view(self):\n    \"\"\"Create a default view in Athena that assigns roughly 80% of the data to training\"\"\"\n\n    # Create the view name\n    view_name = f\"{self.athena_table}_training\"\n    self.log.important(f\"Creating default Training View {view_name}...\")\n\n    # Construct the CREATE VIEW query with random assignment\n    create_view_query = f\"\"\"\n    CREATE OR REPLACE VIEW {view_name} AS\n    SELECT *, CASE\n        WHEN RAND() &lt; 0.8 THEN 1  -- Assign roughly 80% to training\n        ELSE 0  -- Assign roughly 20% to hold-out\n    END AS training\n    FROM {self.athena_table}\n    \"\"\"\n\n    # Execute the CREATE VIEW query\n    self.data_source.execute_statement(create_view_query)\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.create_s3_training_data","title":"<code>create_s3_training_data()</code>","text":"<p>Create some Training Data (S3 CSV) from a Feature Set using standard options. If you want additional options/features use the get_feature_store() method and see AWS docs for all the details: https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store-create-a-dataset.html Returns:     str: The full path/file for the CSV file created by Feature Store create_dataset()</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def create_s3_training_data(self) -&gt; str:\n    \"\"\"Create some Training Data (S3 CSV) from a Feature Set using standard options. If you want\n    additional options/features use the get_feature_store() method and see AWS docs for all\n    the details: https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store-create-a-dataset.html\n    Returns:\n        str: The full path/file for the CSV file created by Feature Store create_dataset()\n    \"\"\"\n\n    # Set up the S3 Query results path\n    date_time = datetime.now(timezone.utc).strftime(\"%Y-%m-%d_%H:%M:%S\")\n    s3_output_path = self.feature_sets_s3_path + f\"/{self.uuid}/datasets/all_{date_time}\"\n\n    # Do we have a training view?\n    training_view = self.get_training_view_table()\n    if training_view:\n        self.log.important(f\"Creating S3 Training Data from Training View {training_view}...\")\n        table_name = training_view\n    else:\n        self.log.warning(f\"No Training View found for {self.uuid}, using FeatureSet directly...\")\n        table_name = self.athena_table\n\n    # Make a query that gets all the data from the FeatureSet\n    query = f\"SELECT * FROM {table_name}\"\n\n    \"\"\"\n    Note: We're going to circle back to this\n    # Get the snapshot query\n    query = self.snapshot_query(table_name=table_name)\n    \"\"\"\n\n    # Make the query\n    athena_query = FeatureGroup(name=self.uuid, sagemaker_session=self.sm_session).athena_query()\n    athena_query.run(query, output_location=s3_output_path)\n    athena_query.wait()\n    query_execution = athena_query.get_query_execution()\n\n    # Get the full path to the S3 files with the results\n    full_s3_path = s3_output_path + f\"/{query_execution['QueryExecution']['QueryExecutionId']}.csv\"\n    return full_s3_path\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.create_training_view","title":"<code>create_training_view(id_column, hold_out_ids)</code>","text":"<p>Create a view in Athena that marks hold out ids for this FeatureSet</p> <p>Parameters:</p> Name Type Description Default <code>id_column</code> <code>str</code> <p>The name of the id column in the output DataFrame.</p> required <code>hold_out_ids</code> <code>list[str]</code> <p>The list of hold out ids.</p> required Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def create_training_view(self, id_column: str, hold_out_ids: list[str]):\n    \"\"\"Create a view in Athena that marks hold out ids for this FeatureSet\n\n    Args:\n        id_column (str): The name of the id column in the output DataFrame.\n        hold_out_ids (list[str]): The list of hold out ids.\n    \"\"\"\n\n    # Create the view name\n    view_name = f\"{self.athena_table}_training\"\n    self.log.important(f\"Creating Training View {view_name}...\")\n\n    # Format the list of hold out ids for SQL IN clause\n    if hold_out_ids and all(isinstance(id, str) for id in hold_out_ids):\n        formatted_hold_out_ids = \", \".join(f\"'{id}'\" for id in hold_out_ids)\n    else:\n        formatted_hold_out_ids = \", \".join(map(str, hold_out_ids))\n\n    # Construct the CREATE VIEW query\n    create_view_query = f\"\"\"\n    CREATE OR REPLACE VIEW {view_name} AS\n    SELECT *, CASE\n        WHEN {id_column} IN ({formatted_hold_out_ids}) THEN 0\n        ELSE 1\n    END AS training\n    FROM {self.athena_table}\n    \"\"\"\n\n    # Execute the CREATE VIEW query\n    self.data_source.execute_statement(create_view_query)\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.created","title":"<code>created()</code>","text":"<p>Return the datetime when this artifact was created</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def created(self) -&gt; datetime:\n    \"\"\"Return the datetime when this artifact was created\"\"\"\n    return self.feature_meta[\"CreationTime\"]\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.delete","title":"<code>delete()</code>","text":"<p>Delete the Feature Set: Feature Group, Catalog Table, and S3 Storage Objects</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def delete(self):\n    \"\"\"Delete the Feature Set: Feature Group, Catalog Table, and S3 Storage Objects\"\"\"\n\n    # Delete the Feature Group and ensure that it gets deleted\n    self.log.important(f\"Deleting FeatureSet {self.uuid}...\")\n    remove_fg = FeatureGroup(name=self.uuid, sagemaker_session=self.sm_session)\n    remove_fg.delete()\n    self.ensure_feature_group_deleted(remove_fg)\n\n    # Delete our underlying DataSource (Data Catalog Table and S3 Storage Objects)\n    self.data_source.delete()\n\n    # Delete the training view\n    self.delete_training_view()\n\n    # Feature Sets can often have a lot of cruft so delete the entire bucket/prefix\n    s3_delete_path = self.feature_sets_s3_path + f\"/{self.uuid}\"\n    self.log.info(f\"Deleting All FeatureSet S3 Storage Objects {s3_delete_path}\")\n    wr.s3.delete_objects(s3_delete_path, boto3_session=self.boto_session)\n\n    # Now delete any data in the Cache\n    for key in self.data_storage.list_subkeys(f\"feature_set:{self.uuid}\"):\n        self.log.info(f\"Deleting Cache Key: {key}\")\n        self.data_storage.delete(key)\n\n    # Force a refresh of the AWS Metadata (to make sure references to deleted artifacts are gone)\n    self.aws_broker.get_metadata(ServiceCategory.FEATURE_STORE, force_refresh=True)\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.delete_training_view","title":"<code>delete_training_view()</code>","text":"<p>Delete the training view for this FeatureSet</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def delete_training_view(self):\n    \"\"\"Delete the training view for this FeatureSet\"\"\"\n    training_view_table = self.get_training_view_table()\n    if training_view_table is not None:\n        self.log.info(f\"Deleting Training View {training_view_table} for {self.uuid}\")\n        glue_client = self.boto_session.client(\"glue\")\n        glue_client.delete_table(DatabaseName=self.athena_database, Name=training_view_table)\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.descriptive_stats","title":"<code>descriptive_stats(recompute=False)</code>","text":"<p>Get the descriptive stats for the numeric columns of the underlying DataSource Args:     recompute (bool): Recompute the descriptive stats (default=False) Returns:     dict: A dictionary of descriptive stats for the numeric columns</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def descriptive_stats(self, recompute: bool = False) -&gt; dict:\n    \"\"\"Get the descriptive stats for the numeric columns of the underlying DataSource\n    Args:\n        recompute (bool): Recompute the descriptive stats (default=False)\n    Returns:\n        dict: A dictionary of descriptive stats for the numeric columns\n    \"\"\"\n    return self.data_source.descriptive_stats(recompute)\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.details","title":"<code>details(recompute=False)</code>","text":"<p>Additional Details about this FeatureSet Artifact</p> <p>Parameters:</p> Name Type Description Default <code>recompute</code> <code>bool</code> <p>Recompute the details (default: False)</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of details about this FeatureSet</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def details(self, recompute: bool = False) -&gt; dict[dict]:\n    \"\"\"Additional Details about this FeatureSet Artifact\n\n    Args:\n        recompute (bool): Recompute the details (default: False)\n\n    Returns:\n        dict(dict): A dictionary of details about this FeatureSet\n    \"\"\"\n\n    # Check if we have cached version of the FeatureSet Details\n    storage_key = f\"feature_set:{self.uuid}:details\"\n    cached_details = self.data_storage.get(storage_key)\n    if cached_details and not recompute:\n        return cached_details\n\n    self.log.info(f\"Recomputing FeatureSet Details ({self.uuid})...\")\n    details = self.summary()\n    details[\"aws_url\"] = self.aws_url()\n\n    # Now get a summary of the underlying DataSource\n    details[\"storage_summary\"] = self.data_source.summary()\n\n    # Number of Columns\n    details[\"num_columns\"] = self.num_columns()\n\n    # Number of Rows\n    details[\"num_rows\"] = self.num_rows()\n\n    # Additional Details\n    details[\"sageworks_status\"] = self.get_status()\n    details[\"sageworks_input\"] = self.get_input()\n    details[\"sageworks_tags\"] = \":\".join(self.sageworks_tags())\n\n    # Underlying Storage Details\n    details[\"storage_type\"] = \"athena\"  # TODO: Add RDS support\n    details[\"storage_uuid\"] = self.data_source.uuid\n\n    # Add the column details and column stats\n    details[\"column_details\"] = self.column_details()\n    details[\"column_stats\"] = self.column_stats()\n\n    # Cache the details\n    self.data_storage.set(storage_key, details)\n\n    # Return the details data\n    return details\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.exists","title":"<code>exists()</code>","text":"<p>Does the feature_set_name exist in the AWS Metadata?</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"Does the feature_set_name exist in the AWS Metadata?\"\"\"\n    if self.feature_meta is None:\n        self.log.debug(f\"FeatureSet {self.uuid} not found in AWS Metadata!\")\n        return False\n    return True\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.get_data_source","title":"<code>get_data_source()</code>","text":"<p>Return the underlying DataSource object</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def get_data_source(self) -&gt; DataSourceFactory:\n    \"\"\"Return the underlying DataSource object\"\"\"\n    return self.data_source\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.get_display_columns","title":"<code>get_display_columns()</code>","text":"<p>Get the display columns for this FeatureSet</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: The display columns for this FeatureSet</p> Notes <p>This just pulls the display columns from the underlying DataSource</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def get_display_columns(self) -&gt; list[str]:\n    \"\"\"Get the display columns for this FeatureSet\n\n    Returns:\n        list[str]: The display columns for this FeatureSet\n\n    Notes:\n        This just pulls the display columns from the underlying DataSource\n    \"\"\"\n    return self.data_source.get_display_columns()\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.get_feature_store","title":"<code>get_feature_store()</code>","text":"<p>Return the underlying AWS FeatureStore object. This can be useful for more advanced usage with create_dataset() such as Joins and time ranges and a host of other options See: https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store-create-a-dataset.html</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def get_feature_store(self) -&gt; FeatureStore:\n    \"\"\"Return the underlying AWS FeatureStore object. This can be useful for more advanced usage\n    with create_dataset() such as Joins and time ranges and a host of other options\n    See: https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store-create-a-dataset.html\n    \"\"\"\n    return self.feature_store\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.get_training_view_table","title":"<code>get_training_view_table(create=True)</code>","text":"<p>Get the name of the training view for this FeatureSet Args:     create (bool): Create the training view if it doesn't exist (default=True) Returns:     str: The name of the training view for this FeatureSet (or None if it doesn't exist)</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def get_training_view_table(self, create: bool = True) -&gt; Union[str, None]:\n    \"\"\"Get the name of the training view for this FeatureSet\n    Args:\n        create (bool): Create the training view if it doesn't exist (default=True)\n    Returns:\n        str: The name of the training view for this FeatureSet (or None if it doesn't exist)\n    \"\"\"\n    training_view_name = f\"{self.athena_table}_training\"\n    glue_client = self.boto_session.client(\"glue\")\n    try:\n        glue_client.get_table(DatabaseName=self.athena_database, Name=training_view_name)\n        return training_view_name\n    except glue_client.exceptions.EntityNotFoundException:\n        if not create:\n            return None\n        self.log.warning(f\"Training View for {self.uuid} doesn't exist, creating a default one...\")\n        self.create_default_training_view()\n        time.sleep(1)  # Give AWS a second to catch up\n        return training_view_name\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.health_check","title":"<code>health_check()</code>","text":"<p>Perform a health check on this model</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of health issues</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def health_check(self) -&gt; list[str]:\n    \"\"\"Perform a health check on this model\n\n    Returns:\n        list[str]: List of health issues\n    \"\"\"\n    # Call the base class health check\n    health_issues = super().health_check()\n\n    # If we have a 'needs_onboard' in the health check then just return\n    if \"needs_onboard\" in health_issues:\n        return health_issues\n\n    # Check our DataSource\n    if not self.data_source.exists():\n        self.log.critical(f\"Data Source check failed for {self.uuid}\")\n        self.log.critical(\"Delete this Feature Set and recreate it to fix this issue\")\n        health_issues.append(\"data_source_missing\")\n    return health_issues\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.make_ready","title":"<code>make_ready()</code>","text":"<p>This is a BLOCKING method that will wait until the FeatureSet is ready</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def make_ready(self) -&gt; bool:\n    \"\"\"This is a BLOCKING method that will wait until the FeatureSet is ready\"\"\"\n\n    # Call our underlying DataSource make_ready method\n    self.data_source.refresh_meta()\n    if not self.data_source.exists():\n        self.log.critical(f\"Data Source check failed for {self.uuid}\")\n        self.log.critical(\"Delete this Feature Set and recreate it to fix this issue\")\n        return False\n    if not self.data_source.ready():\n        self.data_source.make_ready()\n\n    # Set ourselves to ready\n    self.set_status(\"ready\")\n    self.remove_sageworks_health_tag(\"needs_onboard\")\n    self.details(recompute=True)\n    return True\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.modified","title":"<code>modified()</code>","text":"<p>Return the datetime when this artifact was last modified</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def modified(self) -&gt; datetime:\n    \"\"\"Return the datetime when this artifact was last modified\"\"\"\n    # Note: We can't currently figure out how to this from AWS Metadata\n    return self.feature_meta[\"CreationTime\"]\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.num_columns","title":"<code>num_columns()</code>","text":"<p>Return the number of columns of the Feature Set</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def num_columns(self) -&gt; int:\n    \"\"\"Return the number of columns of the Feature Set\"\"\"\n    return len(self.column_names())\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.num_rows","title":"<code>num_rows()</code>","text":"<p>Return the number of rows of the internal DataSource</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def num_rows(self) -&gt; int:\n    \"\"\"Return the number of rows of the internal DataSource\"\"\"\n    return self.data_source.num_rows()\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.outliers","title":"<code>outliers(scale=1.5, recompute=False)</code>","text":"<p>Compute outliers for all the numeric columns in a DataSource Args:     scale (float): The scale to use for the IQR (default: 1.5)     recompute (bool): Recompute the outliers (default: False) Returns:     pd.DataFrame: A DataFrame of outliers from this DataSource Notes:     Uses the IQR * 1.5 (~= 2.5 Sigma) method to compute outliers     The scale parameter can be adjusted to change the IQR multiplier</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def outliers(self, scale: float = 1.5, recompute: bool = False) -&gt; pd.DataFrame:\n    \"\"\"Compute outliers for all the numeric columns in a DataSource\n    Args:\n        scale (float): The scale to use for the IQR (default: 1.5)\n        recompute (bool): Recompute the outliers (default: False)\n    Returns:\n        pd.DataFrame: A DataFrame of outliers from this DataSource\n    Notes:\n        Uses the IQR * 1.5 (~= 2.5 Sigma) method to compute outliers\n        The scale parameter can be adjusted to change the IQR multiplier\n    \"\"\"\n    return self.data_source.outliers(scale=scale, recompute=recompute)\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.query","title":"<code>query(query)</code>","text":"<p>Query the internal DataSource</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def query(self, query: str) -&gt; pd.DataFrame:\n    \"\"\"Query the internal DataSource\"\"\"\n    return self.data_source.query(query)\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.ready","title":"<code>ready()</code>","text":"<p>Is the FeatureSet ready? Is initial setup complete and expected metadata populated? Note: Since FeatureSet is a composite of DataSource and FeatureGroup, we need to    check both to see if the FeatureSet is ready.</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def ready(self) -&gt; bool:\n    \"\"\"Is the FeatureSet ready? Is initial setup complete and expected metadata populated?\n    Note: Since FeatureSet is a composite of DataSource and FeatureGroup, we need to\n       check both to see if the FeatureSet is ready.\"\"\"\n\n    # Check the expected metadata for the FeatureSet\n    expected_meta = self.expected_meta()\n    existing_meta = self.sageworks_meta()\n    feature_set_ready = set(existing_meta.keys()).issuperset(expected_meta)\n    if not feature_set_ready:\n        self.log.info(f\"FeatureSet {self.uuid} is not ready!\")\n        return False\n\n    # Okay now call/return the DataSource ready() method\n    return self.data_source.ready()\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.refresh_meta","title":"<code>refresh_meta()</code>","text":"<p>Internal: Refresh our internal AWS Feature Store metadata</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def refresh_meta(self):\n    \"\"\"Internal: Refresh our internal AWS Feature Store metadata\"\"\"\n    self.log.info(\"Calling refresh_meta() on the underlying DataSource\")\n    self.data_source.refresh_meta()\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.sample","title":"<code>sample(recompute=False)</code>","text":"<p>Get a sample of the data from the underlying DataSource Args:     recompute (bool): Recompute the sample (default=False) Returns:     pd.DataFrame: A sample of the data from the underlying DataSource</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def sample(self, recompute: bool = False) -&gt; pd.DataFrame:\n    \"\"\"Get a sample of the data from the underlying DataSource\n    Args:\n        recompute (bool): Recompute the sample (default=False)\n    Returns:\n        pd.DataFrame: A sample of the data from the underlying DataSource\n    \"\"\"\n    return self.data_source.sample(recompute)\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.set_display_columns","title":"<code>set_display_columns(display_columns)</code>","text":"<p>Set the display columns for this FeatureSet</p> <p>Parameters:</p> Name Type Description Default <code>display_columns</code> <code>list[str]</code> <p>The display columns for this FeatureSet</p> required Notes <p>This just sets the display columns for the underlying DataSource</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def set_display_columns(self, display_columns: list[str]):\n    \"\"\"Set the display columns for this FeatureSet\n\n    Args:\n        display_columns (list[str]): The display columns for this FeatureSet\n\n    Notes:\n        This just sets the display columns for the underlying DataSource\n    \"\"\"\n    self.data_source.set_display_columns(display_columns)\n    self.onboard()\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.size","title":"<code>size()</code>","text":"<p>Return the size of the internal DataSource in MegaBytes</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def size(self) -&gt; float:\n    \"\"\"Return the size of the internal DataSource in MegaBytes\"\"\"\n    return self.data_source.size()\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.smart_sample","title":"<code>smart_sample()</code>","text":"<p>Get a SMART sample dataframe from this FeatureSet Returns:     pd.DataFrame: A combined DataFrame of sample data + outliers</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def smart_sample(self) -&gt; pd.DataFrame:\n    \"\"\"Get a SMART sample dataframe from this FeatureSet\n    Returns:\n        pd.DataFrame: A combined DataFrame of sample data + outliers\n    \"\"\"\n    return self.data_source.smart_sample()\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.snapshot_query","title":"<code>snapshot_query(table_name=None)</code>","text":"<p>An Athena query to get the latest snapshot of features</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>The name of the table to query (default: None)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The Athena query to get the latest snapshot of features</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def snapshot_query(self, table_name: str = None) -&gt; str:\n    \"\"\"An Athena query to get the latest snapshot of features\n\n    Args:\n        table_name (str): The name of the table to query (default: None)\n\n    Returns:\n        str: The Athena query to get the latest snapshot of features\n    \"\"\"\n    # Remove FeatureGroup metadata columns that might have gotten added\n    columns = self.column_names()\n    filter_columns = [\"write_time\", \"api_invocation_time\", \"is_deleted\"]\n    columns = \", \".join(['\"' + x + '\"' for x in columns if x not in filter_columns])\n\n    query = (\n        f\"SELECT {columns} \"\n        f\"    FROM (SELECT *, row_number() OVER (PARTITION BY {self.record_id} \"\n        f\"        ORDER BY {self.event_time} desc, api_invocation_time DESC, write_time DESC) AS row_num \"\n        f'        FROM \"{table_name}\") '\n        \"    WHERE row_num = 1 and NOT is_deleted;\"\n    )\n    return query\n</code></pre>"},{"location":"core_classes/artifacts/feature_set_core/#sageworks.core.artifacts.feature_set_core.FeatureSetCore.value_counts","title":"<code>value_counts(recompute=False)</code>","text":"<p>Get the value counts for the string columns of the underlying DataSource Args:     recompute (bool): Recompute the value counts (default=False) Returns:     dict: A dictionary of value counts for the string columns</p> Source code in <code>src/sageworks/core/artifacts/feature_set_core.py</code> <pre><code>def value_counts(self, recompute: bool = False) -&gt; dict:\n    \"\"\"Get the value counts for the string columns of the underlying DataSource\n    Args:\n        recompute (bool): Recompute the value counts (default=False)\n    Returns:\n        dict: A dictionary of value counts for the string columns\n    \"\"\"\n    return self.data_source.value_counts(recompute)\n</code></pre>"},{"location":"core_classes/artifacts/model_core/","title":"ModelCore","text":"<p>API Classes</p> <p>Found a method here you want to use? The API Classes have method pass-through so just call the method on the Model API Class and voil\u00e0 it works the same.</p> <p>ModelCore: SageWorks ModelCore Class</p>"},{"location":"core_classes/artifacts/model_core/#sageworks.core.artifacts.model_core.ModelCore","title":"<code>ModelCore</code>","text":"<p>             Bases: <code>Artifact</code></p> <p>ModelCore: SageWorks ModelCore Class</p> Common Usage <pre><code>my_model = ModelCore(model_uuid)\nmy_model.summary()\nmy_model.details()\n</code></pre> Source code in <code>src/sageworks/core/artifacts/model_core.py</code> <pre><code>class ModelCore(Artifact):\n    \"\"\"ModelCore: SageWorks ModelCore Class\n\n    Common Usage:\n        ```\n        my_model = ModelCore(model_uuid)\n        my_model.summary()\n        my_model.details()\n        ```\n    \"\"\"\n\n    def __init__(self, model_uuid: str, force_refresh: bool = False, model_type: ModelType = None):\n        \"\"\"ModelCore Initialization\n        Args:\n            model_uuid (str): Name of Model in SageWorks.\n            force_refresh (bool, optional): Force a refresh of the AWS Broker. Defaults to False.\n            model_type (ModelType, optional): Set this for newly created Models. Defaults to None.\n        \"\"\"\n        # Call SuperClass Initialization\n        super().__init__(model_uuid)\n\n        # Grab an AWS Metadata Broker object and pull information for Models\n        self.model_name = model_uuid\n        aws_meta = self.aws_broker.get_metadata(ServiceCategory.MODELS, force_refresh=force_refresh)\n        self.model_meta = aws_meta.get(self.model_name)\n        if self.model_meta is None:\n            self.log.important(f\"Could not find model {self.model_name} within current visibility scope\")\n            self.latest_model = None\n            self.model_type = ModelType.UNKNOWN\n        else:\n            try:\n                self.latest_model = self.model_meta[0]\n                self.description = self.latest_model.get(\"ModelPackageDescription\", \"-\")\n                self.training_job_name = self._extract_training_job_name()\n                if model_type:\n                    self._set_model_type(model_type)\n                else:\n                    self.model_type = self._get_model_type()\n            except (IndexError, KeyError):\n                self.log.critical(f\"Model {self.model_name} appears to be malformed. Delete and recreate it!\")\n                self.latest_model = None\n                self.model_type = ModelType.UNKNOWN\n\n        # Set the Model Training S3 Paths\n        self.model_training_path = self.models_s3_path + \"/training/\" + self.model_name\n        self.model_inference_path = self.models_s3_path + \"/inference/\" + self.model_name\n\n        # Call SuperClass Post Initialization\n        super().__post_init__()\n\n        # All done\n        self.log.info(f\"Model Initialized: {self.model_name}\")\n\n    def refresh_meta(self):\n        \"\"\"Refresh the Artifact's metadata\"\"\"\n        self.model_meta = self.aws_broker.get_metadata(ServiceCategory.MODELS, force_refresh=True).get(self.model_name)\n        self.latest_model = self.model_meta[0]\n        self.description = self.latest_model.get(\"ModelPackageDescription\", \"-\")\n        self.training_job_name = self._extract_training_job_name()\n\n    def exists(self) -&gt; bool:\n        \"\"\"Does the model metadata exist in the AWS Metadata?\"\"\"\n        if self.model_meta is None:\n            self.log.debug(f\"Model {self.model_name} not found in AWS Metadata!\")\n            return False\n        return True\n\n    def health_check(self) -&gt; list[str]:\n        \"\"\"Perform a health check on this model\n        Returns:\n            list[str]: List of health issues\n        \"\"\"\n        # Call the base class health check\n        health_issues = super().health_check()\n\n        # Model Type\n        if self._get_model_type() == ModelType.UNKNOWN:\n            health_issues.append(\"model_type_unknown\")\n        else:\n            self.remove_sageworks_health_tag(\"model_type_unknown\")\n\n        # Model Metrics\n        if self.model_metrics() is None:\n            health_issues.append(\"metrics_needed\")\n        else:\n            self.remove_sageworks_health_tag(\"metrics_needed\")\n        return health_issues\n\n    def latest_model_object(self) -&gt; SagemakerModel:\n        \"\"\"Return the latest AWS Sagemaker Model object for this SageWorks Model\n        Returns:\n           sagemaker.model.Model: AWS Sagemaker Model object\n        \"\"\"\n        return SagemakerModel(\n            model_data=self.model_package_arn(), sagemaker_session=self.sm_session, image_uri=self.model_image()\n        )\n\n    def model_metrics(self) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"Retrieve the training metrics for this model\n        Returns:\n            pd.DataFrame: DataFrame of the Model Metrics\n        \"\"\"\n        # Grab the metrics from the SageWorks Metadata (try inference first, then training)\n        metrics = self._pull_inference_metrics()\n        if metrics is not None:\n            return metrics\n        metrics = self.sageworks_meta().get(\"sageworks_training_metrics\")\n        return pd.DataFrame.from_dict(metrics) if isinstance(metrics, dict) else None\n\n    def model_shapley_values(self) -&gt; Union[list[pd.DataFrame], pd.DataFrame, None]:\n        # Shapley only available from inference at the moment, training may come later\n        df_shap = self._pull_shapley_values()\n        return df_shap\n\n    def confusion_matrix(self) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"Retrieve the confusion_matrix for this model\n        Returns:\n            pd.DataFrame: DataFrame of the Confusion Matrix (might be None)\n        \"\"\"\n        # Grab the confusion matrix from the SageWorks Metadata\n        cm = self._pull_inference_cm()\n        if cm is not None:\n            return cm\n        cm = self.sageworks_meta().get(\"sageworks_training_cm\")\n        return pd.DataFrame.from_dict(cm) if cm else None\n\n    def regression_predictions(self) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"Retrieve the regression based predictions for this model\n        Returns:\n            pd.DataFrame: DataFrame of the Regression based Predictions (might be None)\n        \"\"\"\n\n        # Pull the regression predictions, try first from inference, then from training\n        s3_path = f\"{self.model_inference_path}/inference_predictions.csv\"\n        df = self._pull_s3_model_artifacts(s3_path)\n        if df is not None:\n            return df\n        else:\n            s3_path = f\"{self.model_training_path}/validation_predictions.csv\"\n            df = self._pull_s3_model_artifacts(s3_path)\n            return df\n\n    def size(self) -&gt; float:\n        \"\"\"Return the size of this data in MegaBytes\"\"\"\n        return 0.0\n\n    def aws_meta(self) -&gt; dict:\n        \"\"\"Get ALL the AWS metadata for this artifact\"\"\"\n        return self.latest_model\n\n    def arn(self) -&gt; str:\n        \"\"\"AWS ARN (Amazon Resource Name) for the Model Package Group\"\"\"\n        return self.group_arn()\n\n    def group_arn(self) -&gt; str:\n        \"\"\"AWS ARN (Amazon Resource Name) for the Model Package Group\"\"\"\n        return self.latest_model[\"ModelPackageGroupArn\"]\n\n    def model_package_arn(self) -&gt; str:\n        \"\"\"AWS ARN (Amazon Resource Name) for the Model Package (within the Group)\"\"\"\n        return self.latest_model[\"ModelPackageArn\"]\n\n    def model_container_info(self) -&gt; dict:\n        \"\"\"Containiner Info for the Latest Model Package\"\"\"\n        return self.latest_model[\"ModelPackageDetails\"][\"InferenceSpecification\"][\"Containers\"][0]\n\n    def model_image(self) -&gt; str:\n        \"\"\"Containiner Image for the Latest Model Package\"\"\"\n        return self.model_container_info()[\"Image\"]\n\n    def aws_url(self):\n        \"\"\"The AWS URL for looking at/querying this data source\"\"\"\n        return f\"https://{self.aws_region}.console.aws.amazon.com/athena/home\"\n\n    def created(self) -&gt; datetime:\n        \"\"\"Return the datetime when this artifact was created\"\"\"\n        return self.latest_model[\"CreationTime\"]\n\n    def modified(self) -&gt; datetime:\n        \"\"\"Return the datetime when this artifact was last modified\"\"\"\n        return self.latest_model[\"CreationTime\"]\n\n    def details(self, recompute=False) -&gt; dict:\n        \"\"\"Additional Details about this Model\n        Args:\n            recompute (bool, optional): Recompute the details (default: False)\n        Returns:\n            dict: Dictionary of details about this Model\n        \"\"\"\n\n        # Check if we have cached version of the Model Details\n        storage_key = f\"model:{self.uuid}:details\"\n        cached_details = self.data_storage.get(storage_key)\n        if cached_details and not recompute:\n            return cached_details\n\n        self.log.info(\"Recomputing Model Details...\")\n        details = self.summary()\n        details[\"model_type\"] = self.model_type.value\n        details[\"model_package_group_arn\"] = self.group_arn()\n        details[\"model_package_arn\"] = self.model_package_arn()\n        aws_meta = self.aws_meta()\n        details[\"description\"] = aws_meta.get(\"ModelPackageDescription\", \"-\")\n        details[\"version\"] = aws_meta[\"ModelPackageVersion\"]\n        details[\"status\"] = aws_meta[\"ModelPackageStatus\"]\n        details[\"approval_status\"] = aws_meta[\"ModelApprovalStatus\"]\n        details[\"image\"] = self.model_image().split(\"/\")[-1]  # Shorten the image uri\n\n        # Grab the inference and container info\n        package_details = aws_meta[\"ModelPackageDetails\"]\n        inference_spec = package_details[\"InferenceSpecification\"]\n        container_info = self.model_container_info()\n        details[\"framework\"] = container_info.get(\"Framework\", \"unknown\")\n        details[\"framework_version\"] = container_info.get(\"FrameworkVersion\", \"unknown\")\n        details[\"inference_types\"] = inference_spec[\"SupportedRealtimeInferenceInstanceTypes\"]\n        details[\"transform_types\"] = inference_spec[\"SupportedTransformInstanceTypes\"]\n        details[\"content_types\"] = inference_spec[\"SupportedContentTypes\"]\n        details[\"response_types\"] = inference_spec[\"SupportedResponseMIMETypes\"]\n        details[\"model_metrics\"] = self.model_metrics()\n        if self.model_type == ModelType.CLASSIFIER:\n            details[\"confusion_matrix\"] = self.confusion_matrix()\n            details[\"regression_predictions\"] = None\n        else:\n            details[\"confusion_matrix\"] = None\n            details[\"regression_predictions\"] = self.regression_predictions()\n\n        # Set Shapley values\n        details[\"shapley_values\"] = self.model_shapley_values()\n\n        # Grab the inference metadata\n        details[\"inference_meta\"] = self._pull_inference_metadata()\n\n        # Cache the details\n        self.data_storage.set(storage_key, details)\n\n        # Return the details\n        return details\n\n    def expected_meta(self) -&gt; list[str]:\n        \"\"\"Metadata we expect to see for this Model when it's ready\n        Returns:\n            list[str]: List of expected metadata keys\n        \"\"\"\n        # Our current list of expected metadata, we can add to this as needed\n        return [\"sageworks_status\", \"sageworks_training_metrics\", \"sageworks_training_cm\"]\n\n    def onboard(self) -&gt; bool:\n        \"\"\"Onboard this Model into SageWorks\n        Returns:\n            bool: True if the Model was successfully onboarded, False otherwise\n        \"\"\"\n\n        # Determine the Model Type\n        while self.is_model_unknown():\n            self._determine_model_type()\n\n        # Call the superclass onboard\n        super().onboard()\n        return True\n\n    def is_model_unknown(self) -&gt; bool:\n        \"\"\"Is the Model Type unknown?\"\"\"\n        return self.model_type == ModelType.UNKNOWN\n\n    def _determine_model_type(self):\n        \"\"\"Internal: Determine the Model Type\"\"\"\n        model_type = input(\"Model Type? (classifier, regressor, unsupervised, transformer): \")\n        if model_type == \"classifier\":\n            self._set_model_type(ModelType.CLASSIFIER)\n        elif model_type == \"regressor\":\n            self._set_model_type(ModelType.REGRESSOR)\n        elif model_type == \"unsupervised\":\n            self._set_model_type(ModelType.UNSUPERVISED)\n        elif model_type == \"transformer\":\n            self._set_model_type(ModelType.TRANSFORMER)\n        else:\n            self.log.warning(f\"Unknown Model Type {model_type}!\")\n            self._set_model_type(ModelType.UNKNOWN)\n\n    def make_ready(self) -&gt; bool:\n        \"\"\"This is a BLOCKING method that will wait until the Model is ready\n        Returns:\n            bool: True if the Model is ready, False otherwise\n        \"\"\"\n        self._pull_training_job_metrics(force_pull=True)\n        self.set_status(\"ready\")\n        self.remove_sageworks_health_tag(\"needs_onboard\")\n        time.sleep(1)  # Give the AWS Metadata a chance to update\n        self.health_check()\n        self.refresh_meta()\n        self.details(recompute=True)\n        return True\n\n    def delete(self):\n        \"\"\"Delete the Model Packages and the Model Group\"\"\"\n\n        # If we don't have meta then the model probably doesn't exist\n        if self.model_meta is None:\n            self.log.info(f\"Model {self.model_name} doesn't appear to exist...\")\n            return\n\n        # First delete the Model Packages within the Model Group\n        for model in self.model_meta:\n            self.log.info(f\"Deleting Model Package {model['ModelPackageArn']}...\")\n            self.sm_client.delete_model_package(ModelPackageName=model[\"ModelPackageArn\"])\n\n        # Delete the Model Package Group\n        self.log.info(f\"Deleting Model Group {self.model_name}...\")\n        self.sm_client.delete_model_package_group(ModelPackageGroupName=self.model_name)\n\n        # Delete any inference artifacts\n        s3_delete_path = f\"{self.model_inference_path}\"\n        self.log.info(f\"Deleting Training S3 Objects {s3_delete_path}\")\n        wr.s3.delete_objects(s3_delete_path, boto3_session=self.boto_session)\n\n        # Delete any training artifacts\n        s3_delete_path = f\"{self.model_training_path}\"\n        self.log.info(f\"Deleting Inference S3 Objects {s3_delete_path}\")\n        wr.s3.delete_objects(s3_delete_path, boto3_session=self.boto_session)\n\n        # Delete any data in the Cache\n        for key in self.data_storage.list_subkeys(f\"model:{self.uuid}\"):\n            self.log.info(f\"Deleting Cache Key {key}...\")\n            self.data_storage.delete(key)\n\n    def _set_model_type(self, model_type: ModelType):\n        \"\"\"Internal: Set the Model Type for this Model\"\"\"\n        self.model_type = model_type\n        self.upsert_sageworks_meta({\"sageworks_model_type\": self.model_type.value})\n        self.remove_sageworks_health_tag(\"model_type_unknown\")\n\n    def _get_model_type(self) -&gt; ModelType:\n        \"\"\"Internal: Query the SageWorks Metadata to get the model type\n        Returns:\n            ModelType: The ModelType of this Model\n        Notes:\n            This is an internal method that should not be called directly\n            Use the model_type attribute instead\n        \"\"\"\n        model_type = self.sageworks_meta().get(\"sageworks_model_type\")\n        if model_type and model_type != \"unknown\":\n            return ModelType(model_type)\n        else:\n            self.log.warning(f\"Could not determine model type for {self.model_name}!\")\n            return ModelType.UNKNOWN\n\n    def _pull_inference_metadata(self) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"Internal: Retrieve the inference metadata for this model\n        Returns:\n            dict: Dictionary of the inference metadata (might be None)\n        Notes:\n            Basically when the inference was run, name of the dataset, the MD5, etc\n        \"\"\"\n        s3_path = f\"{self.model_inference_path}/inference_meta.json\"\n        try:\n            return wr.s3.read_json(s3_path)\n        except NoFilesFound:\n            self.log.info(f\"Could not find model inference meta at {s3_path}...\")\n            return None\n\n    def _pull_shapley_values(self) -&gt; Union[list[pd.DataFrame], pd.DataFrame, None]:\n        \"\"\"Internal: Retrieve the inference Shapely values for this model\n        Returns:\n            pd.DataFrame: Dataframe of the shapley values for the prediction dataframe\n        \"\"\"\n\n        # Multiple CSV if classifier\n        if self.model_type == ModelType.CLASSIFIER:\n            # CSVs for shap values are indexed by prediction class\n            # Because we don't know how many classes there are, we need to search through\n            # a list of S3 objects in the parent folder\n            s3_paths = wr.s3.list_objects(self.model_inference_path)\n            return [\n                self._pull_s3_model_artifacts(f, embedded_index=False) for f in s3_paths if \"inference_shap_values\" in f\n            ]\n\n        # One CSV if regressor\n        if self.model_type == ModelType.REGRESSOR:\n            s3_path = f\"{self.model_inference_path}/inference_shap_values.csv\"\n            return self._pull_s3_model_artifacts(s3_path, embedded_index=False)\n\n    def _pull_inference_metrics(self) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"Internal: Retrieve the inference model metrics for this model\n        Returns:\n            pd.DataFrame: DataFrame of the inference model metrics (might be None)\n        \"\"\"\n        s3_path = f\"{self.model_inference_path}/inference_metrics.csv\"\n        return self._pull_s3_model_artifacts(s3_path)\n\n    def _pull_inference_cm(self) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"Internal: Retrieve the inference Confusion Matrix for this model\n        Returns:\n            pd.DataFrame: DataFrame of the inference Confusion Matrix (might be None)\n        \"\"\"\n        s3_path = f\"{self.model_inference_path}/inference_cm.csv\"\n        return self._pull_s3_model_artifacts(s3_path, embedded_index=True)\n\n    def _pull_s3_model_artifacts(self, s3_path, embedded_index=False) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"Internal: Helper method to pull Model Artifact data from S3 storage\n        Args:\n            s3_path (str): S3 Path to the Model Artifact\n            embedded_index (bool, optional): Is the index embedded in the CSV? Defaults to False.\n        Returns:\n            pd.DataFrame: DataFrame of the Model Artifact (metrics, CM, regression_preds) (might be None)\n        \"\"\"\n\n        # Pull the CSV file from S3\n        try:\n            if embedded_index:\n                df = wr.s3.read_csv(s3_path, index_col=0)\n            else:\n                df = wr.s3.read_csv(s3_path)\n            return df\n        except NoFilesFound:\n            self.log.info(f\"Could not find model artifact at {s3_path}...\")\n            return None\n\n    def _pull_training_job_metrics(self, force_pull=False):\n        \"\"\"Internal: Grab any captured metrics from the training job for this model\n        Args:\n            force_pull (bool, optional): Force a pull from TrainingJobAnalytics. Defaults to False.\n        \"\"\"\n\n        # First check if we have already computed the various metrics\n        model_metrics = self.sageworks_meta().get(\"sageworks_training_metrics\")\n        if self.model_type == ModelType.REGRESSOR:\n            if model_metrics and not force_pull:\n                return\n\n        # For classifiers, we need to pull the confusion matrix as well\n        cm = self.sageworks_meta().get(\"sageworks_training_cm\")\n        if model_metrics and cm and not force_pull:\n            return\n\n        # We don't have them, so go and grab the training job metrics\n        self.log.info(f\"Pulling training job metrics for {self.training_job_name}...\")\n        try:\n            df = TrainingJobAnalytics(training_job_name=self.training_job_name).dataframe()\n            if self.model_type == ModelType.REGRESSOR:\n                if \"timestamp\" in df.columns:\n                    df = df.drop(columns=[\"timestamp\"])\n\n                # Store and return the metrics in the SageWorks Metadata\n                self.upsert_sageworks_meta({\"sageworks_training_metrics\": df.to_dict(), \"sageworks_training_cm\": None})\n                return\n        except (KeyError, botocore.exceptions.ClientError):\n            self.log.warning(f\"No training job metrics found for {self.training_job_name}\")\n            # Store and return the metrics in the SageWorks Metadata\n            self.upsert_sageworks_meta({\"sageworks_training_metrics\": None, \"sageworks_training_cm\": None})\n            return\n\n        # We need additional processing for classification metrics\n        if self.model_type == ModelType.CLASSIFIER:\n            metrics_df, cm_df = self._process_classification_metrics(df)\n\n            # Store and return the metrics in the SageWorks Metadata\n            self.upsert_sageworks_meta(\n                {\"sageworks_training_metrics\": metrics_df.to_dict(), \"sageworks_training_cm\": cm_df.to_dict()}\n            )\n\n    def _extract_training_job_name(self) -&gt; Union[str, None]:\n        \"\"\"Internal: Extract the training job name from the ModelDataUrl\"\"\"\n        try:\n            model_data_url = self.model_container_info()[\"ModelDataUrl\"]\n            parsed_url = urllib.parse.urlparse(model_data_url)\n            training_job_name = parsed_url.path.lstrip(\"/\").split(\"/\")[0]\n            return training_job_name\n        except KeyError:\n            self.log.warning(f\"Could not extract training job name from {model_data_url}\")\n            return None\n\n    @staticmethod\n    def _process_classification_metrics(df: pd.DataFrame) -&gt; (pd.DataFrame, pd.DataFrame):\n        \"\"\"Internal: Process classification metrics into a more reasonable format\n        Args:\n            df (pd.DataFrame): DataFrame of training metrics\n        Returns:\n            (pd.DataFrame, pd.DataFrame): Tuple of DataFrames. Metrics and confusion matrix\n        \"\"\"\n        # Split into two DataFrames based on 'metric_name'\n        metrics_df = df[df[\"metric_name\"].str.startswith(\"Metrics:\")].copy()\n        cm_df = df[df[\"metric_name\"].str.startswith(\"ConfusionMatrix:\")].copy()\n\n        # Split the 'metric_name' into different parts\n        metrics_df[\"class\"] = metrics_df[\"metric_name\"].str.split(\":\").str[1]\n        metrics_df[\"metric_type\"] = metrics_df[\"metric_name\"].str.split(\":\").str[2]\n\n        # Pivot the DataFrame to get the desired structure\n        metrics_df = metrics_df.pivot(index=\"class\", columns=\"metric_type\", values=\"value\").reset_index()\n        metrics_df = metrics_df.rename_axis(None, axis=1)\n\n        # Now process the confusion matrix\n        cm_df[\"row_class\"] = cm_df[\"metric_name\"].str.split(\":\").str[1]\n        cm_df[\"col_class\"] = cm_df[\"metric_name\"].str.split(\":\").str[2]\n\n        # Pivot the DataFrame to create a form suitable for the heatmap\n        cm_df = cm_df.pivot(index=\"row_class\", columns=\"col_class\", values=\"value\")\n\n        # Convert the values in cm_df to integers\n        cm_df = cm_df.astype(int)\n\n        return metrics_df, cm_df\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#sageworks.core.artifacts.model_core.ModelCore.__init__","title":"<code>__init__(model_uuid, force_refresh=False, model_type=None)</code>","text":"<p>ModelCore Initialization Args:     model_uuid (str): Name of Model in SageWorks.     force_refresh (bool, optional): Force a refresh of the AWS Broker. Defaults to False.     model_type (ModelType, optional): Set this for newly created Models. Defaults to None.</p> Source code in <code>src/sageworks/core/artifacts/model_core.py</code> <pre><code>def __init__(self, model_uuid: str, force_refresh: bool = False, model_type: ModelType = None):\n    \"\"\"ModelCore Initialization\n    Args:\n        model_uuid (str): Name of Model in SageWorks.\n        force_refresh (bool, optional): Force a refresh of the AWS Broker. Defaults to False.\n        model_type (ModelType, optional): Set this for newly created Models. Defaults to None.\n    \"\"\"\n    # Call SuperClass Initialization\n    super().__init__(model_uuid)\n\n    # Grab an AWS Metadata Broker object and pull information for Models\n    self.model_name = model_uuid\n    aws_meta = self.aws_broker.get_metadata(ServiceCategory.MODELS, force_refresh=force_refresh)\n    self.model_meta = aws_meta.get(self.model_name)\n    if self.model_meta is None:\n        self.log.important(f\"Could not find model {self.model_name} within current visibility scope\")\n        self.latest_model = None\n        self.model_type = ModelType.UNKNOWN\n    else:\n        try:\n            self.latest_model = self.model_meta[0]\n            self.description = self.latest_model.get(\"ModelPackageDescription\", \"-\")\n            self.training_job_name = self._extract_training_job_name()\n            if model_type:\n                self._set_model_type(model_type)\n            else:\n                self.model_type = self._get_model_type()\n        except (IndexError, KeyError):\n            self.log.critical(f\"Model {self.model_name} appears to be malformed. Delete and recreate it!\")\n            self.latest_model = None\n            self.model_type = ModelType.UNKNOWN\n\n    # Set the Model Training S3 Paths\n    self.model_training_path = self.models_s3_path + \"/training/\" + self.model_name\n    self.model_inference_path = self.models_s3_path + \"/inference/\" + self.model_name\n\n    # Call SuperClass Post Initialization\n    super().__post_init__()\n\n    # All done\n    self.log.info(f\"Model Initialized: {self.model_name}\")\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#sageworks.core.artifacts.model_core.ModelCore.arn","title":"<code>arn()</code>","text":"<p>AWS ARN (Amazon Resource Name) for the Model Package Group</p> Source code in <code>src/sageworks/core/artifacts/model_core.py</code> <pre><code>def arn(self) -&gt; str:\n    \"\"\"AWS ARN (Amazon Resource Name) for the Model Package Group\"\"\"\n    return self.group_arn()\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#sageworks.core.artifacts.model_core.ModelCore.aws_meta","title":"<code>aws_meta()</code>","text":"<p>Get ALL the AWS metadata for this artifact</p> Source code in <code>src/sageworks/core/artifacts/model_core.py</code> <pre><code>def aws_meta(self) -&gt; dict:\n    \"\"\"Get ALL the AWS metadata for this artifact\"\"\"\n    return self.latest_model\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#sageworks.core.artifacts.model_core.ModelCore.aws_url","title":"<code>aws_url()</code>","text":"<p>The AWS URL for looking at/querying this data source</p> Source code in <code>src/sageworks/core/artifacts/model_core.py</code> <pre><code>def aws_url(self):\n    \"\"\"The AWS URL for looking at/querying this data source\"\"\"\n    return f\"https://{self.aws_region}.console.aws.amazon.com/athena/home\"\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#sageworks.core.artifacts.model_core.ModelCore.confusion_matrix","title":"<code>confusion_matrix()</code>","text":"<p>Retrieve the confusion_matrix for this model Returns:     pd.DataFrame: DataFrame of the Confusion Matrix (might be None)</p> Source code in <code>src/sageworks/core/artifacts/model_core.py</code> <pre><code>def confusion_matrix(self) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"Retrieve the confusion_matrix for this model\n    Returns:\n        pd.DataFrame: DataFrame of the Confusion Matrix (might be None)\n    \"\"\"\n    # Grab the confusion matrix from the SageWorks Metadata\n    cm = self._pull_inference_cm()\n    if cm is not None:\n        return cm\n    cm = self.sageworks_meta().get(\"sageworks_training_cm\")\n    return pd.DataFrame.from_dict(cm) if cm else None\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#sageworks.core.artifacts.model_core.ModelCore.created","title":"<code>created()</code>","text":"<p>Return the datetime when this artifact was created</p> Source code in <code>src/sageworks/core/artifacts/model_core.py</code> <pre><code>def created(self) -&gt; datetime:\n    \"\"\"Return the datetime when this artifact was created\"\"\"\n    return self.latest_model[\"CreationTime\"]\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#sageworks.core.artifacts.model_core.ModelCore.delete","title":"<code>delete()</code>","text":"<p>Delete the Model Packages and the Model Group</p> Source code in <code>src/sageworks/core/artifacts/model_core.py</code> <pre><code>def delete(self):\n    \"\"\"Delete the Model Packages and the Model Group\"\"\"\n\n    # If we don't have meta then the model probably doesn't exist\n    if self.model_meta is None:\n        self.log.info(f\"Model {self.model_name} doesn't appear to exist...\")\n        return\n\n    # First delete the Model Packages within the Model Group\n    for model in self.model_meta:\n        self.log.info(f\"Deleting Model Package {model['ModelPackageArn']}...\")\n        self.sm_client.delete_model_package(ModelPackageName=model[\"ModelPackageArn\"])\n\n    # Delete the Model Package Group\n    self.log.info(f\"Deleting Model Group {self.model_name}...\")\n    self.sm_client.delete_model_package_group(ModelPackageGroupName=self.model_name)\n\n    # Delete any inference artifacts\n    s3_delete_path = f\"{self.model_inference_path}\"\n    self.log.info(f\"Deleting Training S3 Objects {s3_delete_path}\")\n    wr.s3.delete_objects(s3_delete_path, boto3_session=self.boto_session)\n\n    # Delete any training artifacts\n    s3_delete_path = f\"{self.model_training_path}\"\n    self.log.info(f\"Deleting Inference S3 Objects {s3_delete_path}\")\n    wr.s3.delete_objects(s3_delete_path, boto3_session=self.boto_session)\n\n    # Delete any data in the Cache\n    for key in self.data_storage.list_subkeys(f\"model:{self.uuid}\"):\n        self.log.info(f\"Deleting Cache Key {key}...\")\n        self.data_storage.delete(key)\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#sageworks.core.artifacts.model_core.ModelCore.details","title":"<code>details(recompute=False)</code>","text":"<p>Additional Details about this Model Args:     recompute (bool, optional): Recompute the details (default: False) Returns:     dict: Dictionary of details about this Model</p> Source code in <code>src/sageworks/core/artifacts/model_core.py</code> <pre><code>def details(self, recompute=False) -&gt; dict:\n    \"\"\"Additional Details about this Model\n    Args:\n        recompute (bool, optional): Recompute the details (default: False)\n    Returns:\n        dict: Dictionary of details about this Model\n    \"\"\"\n\n    # Check if we have cached version of the Model Details\n    storage_key = f\"model:{self.uuid}:details\"\n    cached_details = self.data_storage.get(storage_key)\n    if cached_details and not recompute:\n        return cached_details\n\n    self.log.info(\"Recomputing Model Details...\")\n    details = self.summary()\n    details[\"model_type\"] = self.model_type.value\n    details[\"model_package_group_arn\"] = self.group_arn()\n    details[\"model_package_arn\"] = self.model_package_arn()\n    aws_meta = self.aws_meta()\n    details[\"description\"] = aws_meta.get(\"ModelPackageDescription\", \"-\")\n    details[\"version\"] = aws_meta[\"ModelPackageVersion\"]\n    details[\"status\"] = aws_meta[\"ModelPackageStatus\"]\n    details[\"approval_status\"] = aws_meta[\"ModelApprovalStatus\"]\n    details[\"image\"] = self.model_image().split(\"/\")[-1]  # Shorten the image uri\n\n    # Grab the inference and container info\n    package_details = aws_meta[\"ModelPackageDetails\"]\n    inference_spec = package_details[\"InferenceSpecification\"]\n    container_info = self.model_container_info()\n    details[\"framework\"] = container_info.get(\"Framework\", \"unknown\")\n    details[\"framework_version\"] = container_info.get(\"FrameworkVersion\", \"unknown\")\n    details[\"inference_types\"] = inference_spec[\"SupportedRealtimeInferenceInstanceTypes\"]\n    details[\"transform_types\"] = inference_spec[\"SupportedTransformInstanceTypes\"]\n    details[\"content_types\"] = inference_spec[\"SupportedContentTypes\"]\n    details[\"response_types\"] = inference_spec[\"SupportedResponseMIMETypes\"]\n    details[\"model_metrics\"] = self.model_metrics()\n    if self.model_type == ModelType.CLASSIFIER:\n        details[\"confusion_matrix\"] = self.confusion_matrix()\n        details[\"regression_predictions\"] = None\n    else:\n        details[\"confusion_matrix\"] = None\n        details[\"regression_predictions\"] = self.regression_predictions()\n\n    # Set Shapley values\n    details[\"shapley_values\"] = self.model_shapley_values()\n\n    # Grab the inference metadata\n    details[\"inference_meta\"] = self._pull_inference_metadata()\n\n    # Cache the details\n    self.data_storage.set(storage_key, details)\n\n    # Return the details\n    return details\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#sageworks.core.artifacts.model_core.ModelCore.exists","title":"<code>exists()</code>","text":"<p>Does the model metadata exist in the AWS Metadata?</p> Source code in <code>src/sageworks/core/artifacts/model_core.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"Does the model metadata exist in the AWS Metadata?\"\"\"\n    if self.model_meta is None:\n        self.log.debug(f\"Model {self.model_name} not found in AWS Metadata!\")\n        return False\n    return True\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#sageworks.core.artifacts.model_core.ModelCore.expected_meta","title":"<code>expected_meta()</code>","text":"<p>Metadata we expect to see for this Model when it's ready Returns:     list[str]: List of expected metadata keys</p> Source code in <code>src/sageworks/core/artifacts/model_core.py</code> <pre><code>def expected_meta(self) -&gt; list[str]:\n    \"\"\"Metadata we expect to see for this Model when it's ready\n    Returns:\n        list[str]: List of expected metadata keys\n    \"\"\"\n    # Our current list of expected metadata, we can add to this as needed\n    return [\"sageworks_status\", \"sageworks_training_metrics\", \"sageworks_training_cm\"]\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#sageworks.core.artifacts.model_core.ModelCore.group_arn","title":"<code>group_arn()</code>","text":"<p>AWS ARN (Amazon Resource Name) for the Model Package Group</p> Source code in <code>src/sageworks/core/artifacts/model_core.py</code> <pre><code>def group_arn(self) -&gt; str:\n    \"\"\"AWS ARN (Amazon Resource Name) for the Model Package Group\"\"\"\n    return self.latest_model[\"ModelPackageGroupArn\"]\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#sageworks.core.artifacts.model_core.ModelCore.health_check","title":"<code>health_check()</code>","text":"<p>Perform a health check on this model Returns:     list[str]: List of health issues</p> Source code in <code>src/sageworks/core/artifacts/model_core.py</code> <pre><code>def health_check(self) -&gt; list[str]:\n    \"\"\"Perform a health check on this model\n    Returns:\n        list[str]: List of health issues\n    \"\"\"\n    # Call the base class health check\n    health_issues = super().health_check()\n\n    # Model Type\n    if self._get_model_type() == ModelType.UNKNOWN:\n        health_issues.append(\"model_type_unknown\")\n    else:\n        self.remove_sageworks_health_tag(\"model_type_unknown\")\n\n    # Model Metrics\n    if self.model_metrics() is None:\n        health_issues.append(\"metrics_needed\")\n    else:\n        self.remove_sageworks_health_tag(\"metrics_needed\")\n    return health_issues\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#sageworks.core.artifacts.model_core.ModelCore.is_model_unknown","title":"<code>is_model_unknown()</code>","text":"<p>Is the Model Type unknown?</p> Source code in <code>src/sageworks/core/artifacts/model_core.py</code> <pre><code>def is_model_unknown(self) -&gt; bool:\n    \"\"\"Is the Model Type unknown?\"\"\"\n    return self.model_type == ModelType.UNKNOWN\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#sageworks.core.artifacts.model_core.ModelCore.latest_model_object","title":"<code>latest_model_object()</code>","text":"<p>Return the latest AWS Sagemaker Model object for this SageWorks Model Returns:    sagemaker.model.Model: AWS Sagemaker Model object</p> Source code in <code>src/sageworks/core/artifacts/model_core.py</code> <pre><code>def latest_model_object(self) -&gt; SagemakerModel:\n    \"\"\"Return the latest AWS Sagemaker Model object for this SageWorks Model\n    Returns:\n       sagemaker.model.Model: AWS Sagemaker Model object\n    \"\"\"\n    return SagemakerModel(\n        model_data=self.model_package_arn(), sagemaker_session=self.sm_session, image_uri=self.model_image()\n    )\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#sageworks.core.artifacts.model_core.ModelCore.make_ready","title":"<code>make_ready()</code>","text":"<p>This is a BLOCKING method that will wait until the Model is ready Returns:     bool: True if the Model is ready, False otherwise</p> Source code in <code>src/sageworks/core/artifacts/model_core.py</code> <pre><code>def make_ready(self) -&gt; bool:\n    \"\"\"This is a BLOCKING method that will wait until the Model is ready\n    Returns:\n        bool: True if the Model is ready, False otherwise\n    \"\"\"\n    self._pull_training_job_metrics(force_pull=True)\n    self.set_status(\"ready\")\n    self.remove_sageworks_health_tag(\"needs_onboard\")\n    time.sleep(1)  # Give the AWS Metadata a chance to update\n    self.health_check()\n    self.refresh_meta()\n    self.details(recompute=True)\n    return True\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#sageworks.core.artifacts.model_core.ModelCore.model_container_info","title":"<code>model_container_info()</code>","text":"<p>Containiner Info for the Latest Model Package</p> Source code in <code>src/sageworks/core/artifacts/model_core.py</code> <pre><code>def model_container_info(self) -&gt; dict:\n    \"\"\"Containiner Info for the Latest Model Package\"\"\"\n    return self.latest_model[\"ModelPackageDetails\"][\"InferenceSpecification\"][\"Containers\"][0]\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#sageworks.core.artifacts.model_core.ModelCore.model_image","title":"<code>model_image()</code>","text":"<p>Containiner Image for the Latest Model Package</p> Source code in <code>src/sageworks/core/artifacts/model_core.py</code> <pre><code>def model_image(self) -&gt; str:\n    \"\"\"Containiner Image for the Latest Model Package\"\"\"\n    return self.model_container_info()[\"Image\"]\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#sageworks.core.artifacts.model_core.ModelCore.model_metrics","title":"<code>model_metrics()</code>","text":"<p>Retrieve the training metrics for this model Returns:     pd.DataFrame: DataFrame of the Model Metrics</p> Source code in <code>src/sageworks/core/artifacts/model_core.py</code> <pre><code>def model_metrics(self) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"Retrieve the training metrics for this model\n    Returns:\n        pd.DataFrame: DataFrame of the Model Metrics\n    \"\"\"\n    # Grab the metrics from the SageWorks Metadata (try inference first, then training)\n    metrics = self._pull_inference_metrics()\n    if metrics is not None:\n        return metrics\n    metrics = self.sageworks_meta().get(\"sageworks_training_metrics\")\n    return pd.DataFrame.from_dict(metrics) if isinstance(metrics, dict) else None\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#sageworks.core.artifacts.model_core.ModelCore.model_package_arn","title":"<code>model_package_arn()</code>","text":"<p>AWS ARN (Amazon Resource Name) for the Model Package (within the Group)</p> Source code in <code>src/sageworks/core/artifacts/model_core.py</code> <pre><code>def model_package_arn(self) -&gt; str:\n    \"\"\"AWS ARN (Amazon Resource Name) for the Model Package (within the Group)\"\"\"\n    return self.latest_model[\"ModelPackageArn\"]\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#sageworks.core.artifacts.model_core.ModelCore.modified","title":"<code>modified()</code>","text":"<p>Return the datetime when this artifact was last modified</p> Source code in <code>src/sageworks/core/artifacts/model_core.py</code> <pre><code>def modified(self) -&gt; datetime:\n    \"\"\"Return the datetime when this artifact was last modified\"\"\"\n    return self.latest_model[\"CreationTime\"]\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#sageworks.core.artifacts.model_core.ModelCore.onboard","title":"<code>onboard()</code>","text":"<p>Onboard this Model into SageWorks Returns:     bool: True if the Model was successfully onboarded, False otherwise</p> Source code in <code>src/sageworks/core/artifacts/model_core.py</code> <pre><code>def onboard(self) -&gt; bool:\n    \"\"\"Onboard this Model into SageWorks\n    Returns:\n        bool: True if the Model was successfully onboarded, False otherwise\n    \"\"\"\n\n    # Determine the Model Type\n    while self.is_model_unknown():\n        self._determine_model_type()\n\n    # Call the superclass onboard\n    super().onboard()\n    return True\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#sageworks.core.artifacts.model_core.ModelCore.refresh_meta","title":"<code>refresh_meta()</code>","text":"<p>Refresh the Artifact's metadata</p> Source code in <code>src/sageworks/core/artifacts/model_core.py</code> <pre><code>def refresh_meta(self):\n    \"\"\"Refresh the Artifact's metadata\"\"\"\n    self.model_meta = self.aws_broker.get_metadata(ServiceCategory.MODELS, force_refresh=True).get(self.model_name)\n    self.latest_model = self.model_meta[0]\n    self.description = self.latest_model.get(\"ModelPackageDescription\", \"-\")\n    self.training_job_name = self._extract_training_job_name()\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#sageworks.core.artifacts.model_core.ModelCore.regression_predictions","title":"<code>regression_predictions()</code>","text":"<p>Retrieve the regression based predictions for this model Returns:     pd.DataFrame: DataFrame of the Regression based Predictions (might be None)</p> Source code in <code>src/sageworks/core/artifacts/model_core.py</code> <pre><code>def regression_predictions(self) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"Retrieve the regression based predictions for this model\n    Returns:\n        pd.DataFrame: DataFrame of the Regression based Predictions (might be None)\n    \"\"\"\n\n    # Pull the regression predictions, try first from inference, then from training\n    s3_path = f\"{self.model_inference_path}/inference_predictions.csv\"\n    df = self._pull_s3_model_artifacts(s3_path)\n    if df is not None:\n        return df\n    else:\n        s3_path = f\"{self.model_training_path}/validation_predictions.csv\"\n        df = self._pull_s3_model_artifacts(s3_path)\n        return df\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#sageworks.core.artifacts.model_core.ModelCore.size","title":"<code>size()</code>","text":"<p>Return the size of this data in MegaBytes</p> Source code in <code>src/sageworks/core/artifacts/model_core.py</code> <pre><code>def size(self) -&gt; float:\n    \"\"\"Return the size of this data in MegaBytes\"\"\"\n    return 0.0\n</code></pre>"},{"location":"core_classes/artifacts/model_core/#sageworks.core.artifacts.model_core.ModelType","title":"<code>ModelType</code>","text":"<p>             Bases: <code>Enum</code></p> <p>Enumerated Types for SageWorks Model Types</p> Source code in <code>src/sageworks/core/artifacts/model_core.py</code> <pre><code>class ModelType(Enum):\n    \"\"\"Enumerated Types for SageWorks Model Types\"\"\"\n\n    CLASSIFIER = \"classifier\"\n    REGRESSOR = \"regressor\"\n    UNSUPERVISED = \"unsupervised\"\n    TRANSFORMER = \"transformer\"\n    UNKNOWN = \"unknown\"\n</code></pre>"},{"location":"core_classes/artifacts/overview/","title":"SageWorks Artifacts","text":"<p>API Classes</p> <p>For most users the API Classes will provide all the general functionality to create a full AWS ML Pipeline</p>"},{"location":"core_classes/artifacts/overview/#welcome-to-the-sageworks-core-artifact-classes","title":"Welcome to the SageWorks Core Artifact Classes","text":"<p>These classes provide low-level APIs for the SageWorks package, they interact more directly with AWS Services and are therefore more complex with a fairly large number of methods. </p> <ul> <li>AthenaSource: Manages AWS Data Catalog and Athena</li> <li>FeatureSetCore: Manages AWS Feature Store and Feature Groups</li> <li>ModelCore: Manages the training and deployment of AWS Model Groups and Packages</li> <li>EndpointCore: Manages the deployment and invocations/inference on AWS Endpoints</li> </ul> <p></p>"},{"location":"core_classes/transforms/data_loaders_heavy/","title":"DataLoaders Heavy","text":"<p>These DataLoader Classes are intended to load larger dataset into AWS. For large data we need to use AWS Glue Jobs/Batch Jobs and in general the process is a bit more complicated and has less features.</p> <p>If you have smaller data please see DataLoaders Light</p> <p>Welcome to the SageWorks DataLoaders Heavy Classes</p> <p>These classes provide low-level APIs for loading larger data into AWS services</p> <ul> <li>S3HeavyToDataSource: Loads large data from S3 into a DataSource</li> </ul>"},{"location":"core_classes/transforms/data_loaders_heavy/#sageworks.core.transforms.data_loaders.heavy.S3HeavyToDataSource","title":"<code>S3HeavyToDataSource</code>","text":"Source code in <code>src/sageworks/core/transforms/data_loaders/heavy/s3_heavy_to_data_source.py</code> <pre><code>class S3HeavyToDataSource:\n    def __init__(self, glue_context: GlueContext, input_uuid: str, output_uuid: str):\n        \"\"\"S3HeavyToDataSource: Class to move HEAVY S3 Files into a SageWorks DataSource\n\n        Args:\n            glue_context: GlueContext, AWS Glue Specific wrapper around SparkContext\n            input_uuid (str): The S3 Path to the files to be loaded\n            output_uuid (str): The UUID of the SageWorks DataSource to be created\n        \"\"\"\n        self.log = glue_context.get_logger()\n\n        # FIXME: Pull these from Parameter Store or Config\n        self.input_uuid = input_uuid\n        self.output_uuid = output_uuid\n        self.output_meta = {\"sageworks_input\": self.input_uuid}\n        sageworks_bucket = \"s3://sandbox-sageworks-artifacts\"\n        self.data_sources_s3_path = sageworks_bucket + \"/data-sources\"\n\n        # Our Spark Context\n        self.glue_context = glue_context\n\n    @staticmethod\n    def resolve_choice_fields(dyf):\n        # Get schema fields\n        schema_fields = dyf.schema().fields\n\n        # Collect choice fields\n        choice_fields = [(field.name, \"cast:long\") for field in schema_fields if field.dataType.typeName() == \"choice\"]\n        print(f\"Choice Fields: {choice_fields}\")\n\n        # If there are choice fields, resolve them\n        if choice_fields:\n            dyf = dyf.resolveChoice(specs=choice_fields)\n\n        return dyf\n\n    def timestamp_conversions(self, dyf: DynamicFrame, time_columns: list = []) -&gt; DynamicFrame:\n        \"\"\"Convert columns in the DynamicFrame to the correct data types\n        Args:\n            dyf (DynamicFrame): The DynamicFrame to convert\n            time_columns (list): A list of column names to convert to timestamp\n        Returns:\n            DynamicFrame: The converted DynamicFrame\n        \"\"\"\n\n        # Convert the timestamp columns to timestamp types\n        spark_df = dyf.toDF()\n        for column in time_columns:\n            spark_df = spark_df.withColumn(column, to_timestamp(col(column)))\n\n        # Convert the Spark DataFrame back to a Glue DynamicFrame and return\n        return DynamicFrame.fromDF(spark_df, self.glue_context, \"output_dyf\")\n\n    @staticmethod\n    def remove_periods_from_column_names(dyf: DynamicFrame) -&gt; DynamicFrame:\n        \"\"\"Remove periods from column names in the DynamicFrame\n        Args:\n            dyf (DynamicFrame): The DynamicFrame to convert\n        Returns:\n            DynamicFrame: The converted DynamicFrame\n        \"\"\"\n        # Extract the column names from the schema\n        old_column_names = [field.name for field in dyf.schema().fields]\n\n        # Create a new list of renamed column names\n        new_column_names = [name.replace(\".\", \"_\") for name in old_column_names]\n        print(old_column_names)\n        print(new_column_names)\n\n        # Create a new DynamicFrame with renamed columns\n        for c_old, c_new in zip(old_column_names, new_column_names):\n            dyf = dyf.rename_field(f\"`{c_old}`\", c_new)\n        return dyf\n\n    def transform(\n        self,\n        input_type: str = \"json\",\n        timestamp_columns: list = None,\n        output_format: str = \"parquet\",\n    ):\n        \"\"\"Convert the CSV or JSON data into Parquet Format in the SageWorks S3 Bucket, and\n        store the information about the data to the AWS Data Catalog sageworks database\n        Args:\n            input_type (str): The type of input files, either 'csv' or 'json'\n            timestamp_columns (list): A list of column names to convert to timestamp\n            output_format (str): The format of the output files, either 'parquet' or 'orc'\n        \"\"\"\n\n        # Add some tags here\n        tags = [\"heavy\"]\n\n        # Create the Output Parquet file S3 Storage Path\n        s3_storage_path = f\"{self.data_sources_s3_path}/{self.output_uuid}\"\n\n        # Read JSONL files from S3 and infer schema dynamically\n        self.log.info(f\"Reading JSONL files from {self.input_uuid}...\")\n        input_dyf = self.glue_context.create_dynamic_frame.from_options(\n            connection_type=\"s3\",\n            connection_options={\n                \"paths\": [self.input_uuid],\n                \"recurse\": True,\n                \"gzip\": True,\n            },\n            format=input_type,\n            # format_options={'jsonPath': 'auto'}, Look into this later\n        )\n        self.log.info(\"Incoming DataFrame...\")\n        input_dyf.show(5)\n        input_dyf.printSchema()\n\n        # Resolve Choice fields\n        resolved_dyf = self.resolve_choice_fields(input_dyf)\n\n        # The next couple of lines of code is for un-nesting any nested JSON\n        # Create a Dynamic Frame Collection (dfc)\n        dfc = Relationalize.apply(resolved_dyf, name=\"root\")\n\n        # Aggregate the collection into a single dynamic frame\n        output_dyf = dfc.select(\"root\")\n\n        print(\"Before TimeStamp Conversions\")\n        output_dyf.printSchema()\n\n        # Convert any timestamp columns\n        output_dyf = self.timestamp_conversions(output_dyf, timestamp_columns)\n\n        # Relationalize will put periods in the column names. This will cause\n        # problems later when we try to create a FeatureSet from this DataSource\n        output_dyf = self.remove_periods_from_column_names(output_dyf)\n\n        print(\"After TimeStamp Conversions and Removing Periods from column names\")\n        output_dyf.printSchema()\n\n        # Write Parquet files to S3\n        self.log.info(f\"Writing Parquet files to {s3_storage_path}...\")\n        self.glue_context.purge_s3_path(s3_storage_path, {\"retentionPeriod\": 0})\n        self.glue_context.write_dynamic_frame.from_options(\n            frame=output_dyf,\n            connection_type=\"s3\",\n            connection_options={\n                \"path\": s3_storage_path\n                # \"partitionKeys\": [\"year\", \"month\", \"day\"],\n            },\n            format=output_format,\n        )\n\n        # Set up our SageWorks metadata (description, tags, etc)\n        description = f\"SageWorks data source: {self.output_uuid}\"\n        sageworks_meta = {\"sageworks_tags\": \":\".join(tags)}\n        for key, value in self.output_meta.items():\n            sageworks_meta[key] = value\n\n        # Create a new table in the AWS Data Catalog\n        self.log.info(f\"Creating Data Catalog Table: {self.output_uuid}...\")\n\n        # Converting the Spark Types to Athena Types\n        def to_athena_type(col):\n            athena_type_map = {\"long\": \"bigint\"}\n            spark_type = col.dataType.typeName()\n            return athena_type_map.get(spark_type, spark_type)\n\n        column_name_types = [{\"Name\": col.name, \"Type\": to_athena_type(col)} for col in output_dyf.schema().fields]\n\n        # Our parameters for the Glue Data Catalog are different for Parquet and ORC\n        if output_format == \"parquet\":\n            glue_input_format = \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\"\n            glue_output_format = \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\"\n            serialization_library = \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\"\n        else:\n            glue_input_format = \"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\"\n            glue_output_format = \"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\"\n            serialization_library = \"org.apache.hadoop.hive.ql.io.orc.OrcSerde\"\n\n        table_input = {\n            \"Name\": self.output_uuid,\n            \"Description\": description,\n            \"Parameters\": sageworks_meta,\n            \"TableType\": \"EXTERNAL_TABLE\",\n            \"StorageDescriptor\": {\n                \"Columns\": column_name_types,\n                \"Location\": s3_storage_path,\n                \"InputFormat\": glue_input_format,\n                \"OutputFormat\": glue_output_format,\n                \"Compressed\": True,\n                \"SerdeInfo\": {\n                    \"SerializationLibrary\": serialization_library,\n                },\n            },\n        }\n\n        # Delete the Data Catalog Table if it already exists\n        glue_client = boto3.client(\"glue\")\n        try:\n            glue_client.delete_table(DatabaseName=\"sageworks\", Name=self.output_uuid)\n            self.log.info(f\"Deleting Data Catalog Table: {self.output_uuid}...\")\n        except ClientError as e:\n            if e.response[\"Error\"][\"Code\"] != \"EntityNotFoundException\":\n                raise e\n\n        self.log.info(f\"Creating Data Catalog Table: {self.output_uuid}...\")\n        glue_client.create_table(DatabaseName=\"sageworks\", TableInput=table_input)\n\n        # All done!\n        self.log.info(f\"{self.input_uuid} --&gt; {self.output_uuid} complete!\")\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_heavy/#sageworks.core.transforms.data_loaders.heavy.S3HeavyToDataSource.__init__","title":"<code>__init__(glue_context, input_uuid, output_uuid)</code>","text":"<p>S3HeavyToDataSource: Class to move HEAVY S3 Files into a SageWorks DataSource</p> <p>Parameters:</p> Name Type Description Default <code>glue_context</code> <code>GlueContext</code> <p>GlueContext, AWS Glue Specific wrapper around SparkContext</p> required <code>input_uuid</code> <code>str</code> <p>The S3 Path to the files to be loaded</p> required <code>output_uuid</code> <code>str</code> <p>The UUID of the SageWorks DataSource to be created</p> required Source code in <code>src/sageworks/core/transforms/data_loaders/heavy/s3_heavy_to_data_source.py</code> <pre><code>def __init__(self, glue_context: GlueContext, input_uuid: str, output_uuid: str):\n    \"\"\"S3HeavyToDataSource: Class to move HEAVY S3 Files into a SageWorks DataSource\n\n    Args:\n        glue_context: GlueContext, AWS Glue Specific wrapper around SparkContext\n        input_uuid (str): The S3 Path to the files to be loaded\n        output_uuid (str): The UUID of the SageWorks DataSource to be created\n    \"\"\"\n    self.log = glue_context.get_logger()\n\n    # FIXME: Pull these from Parameter Store or Config\n    self.input_uuid = input_uuid\n    self.output_uuid = output_uuid\n    self.output_meta = {\"sageworks_input\": self.input_uuid}\n    sageworks_bucket = \"s3://sandbox-sageworks-artifacts\"\n    self.data_sources_s3_path = sageworks_bucket + \"/data-sources\"\n\n    # Our Spark Context\n    self.glue_context = glue_context\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_heavy/#sageworks.core.transforms.data_loaders.heavy.S3HeavyToDataSource.remove_periods_from_column_names","title":"<code>remove_periods_from_column_names(dyf)</code>  <code>staticmethod</code>","text":"<p>Remove periods from column names in the DynamicFrame Args:     dyf (DynamicFrame): The DynamicFrame to convert Returns:     DynamicFrame: The converted DynamicFrame</p> Source code in <code>src/sageworks/core/transforms/data_loaders/heavy/s3_heavy_to_data_source.py</code> <pre><code>@staticmethod\ndef remove_periods_from_column_names(dyf: DynamicFrame) -&gt; DynamicFrame:\n    \"\"\"Remove periods from column names in the DynamicFrame\n    Args:\n        dyf (DynamicFrame): The DynamicFrame to convert\n    Returns:\n        DynamicFrame: The converted DynamicFrame\n    \"\"\"\n    # Extract the column names from the schema\n    old_column_names = [field.name for field in dyf.schema().fields]\n\n    # Create a new list of renamed column names\n    new_column_names = [name.replace(\".\", \"_\") for name in old_column_names]\n    print(old_column_names)\n    print(new_column_names)\n\n    # Create a new DynamicFrame with renamed columns\n    for c_old, c_new in zip(old_column_names, new_column_names):\n        dyf = dyf.rename_field(f\"`{c_old}`\", c_new)\n    return dyf\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_heavy/#sageworks.core.transforms.data_loaders.heavy.S3HeavyToDataSource.timestamp_conversions","title":"<code>timestamp_conversions(dyf, time_columns=[])</code>","text":"<p>Convert columns in the DynamicFrame to the correct data types Args:     dyf (DynamicFrame): The DynamicFrame to convert     time_columns (list): A list of column names to convert to timestamp Returns:     DynamicFrame: The converted DynamicFrame</p> Source code in <code>src/sageworks/core/transforms/data_loaders/heavy/s3_heavy_to_data_source.py</code> <pre><code>def timestamp_conversions(self, dyf: DynamicFrame, time_columns: list = []) -&gt; DynamicFrame:\n    \"\"\"Convert columns in the DynamicFrame to the correct data types\n    Args:\n        dyf (DynamicFrame): The DynamicFrame to convert\n        time_columns (list): A list of column names to convert to timestamp\n    Returns:\n        DynamicFrame: The converted DynamicFrame\n    \"\"\"\n\n    # Convert the timestamp columns to timestamp types\n    spark_df = dyf.toDF()\n    for column in time_columns:\n        spark_df = spark_df.withColumn(column, to_timestamp(col(column)))\n\n    # Convert the Spark DataFrame back to a Glue DynamicFrame and return\n    return DynamicFrame.fromDF(spark_df, self.glue_context, \"output_dyf\")\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_heavy/#sageworks.core.transforms.data_loaders.heavy.S3HeavyToDataSource.transform","title":"<code>transform(input_type='json', timestamp_columns=None, output_format='parquet')</code>","text":"<p>Convert the CSV or JSON data into Parquet Format in the SageWorks S3 Bucket, and store the information about the data to the AWS Data Catalog sageworks database Args:     input_type (str): The type of input files, either 'csv' or 'json'     timestamp_columns (list): A list of column names to convert to timestamp     output_format (str): The format of the output files, either 'parquet' or 'orc'</p> Source code in <code>src/sageworks/core/transforms/data_loaders/heavy/s3_heavy_to_data_source.py</code> <pre><code>def transform(\n    self,\n    input_type: str = \"json\",\n    timestamp_columns: list = None,\n    output_format: str = \"parquet\",\n):\n    \"\"\"Convert the CSV or JSON data into Parquet Format in the SageWorks S3 Bucket, and\n    store the information about the data to the AWS Data Catalog sageworks database\n    Args:\n        input_type (str): The type of input files, either 'csv' or 'json'\n        timestamp_columns (list): A list of column names to convert to timestamp\n        output_format (str): The format of the output files, either 'parquet' or 'orc'\n    \"\"\"\n\n    # Add some tags here\n    tags = [\"heavy\"]\n\n    # Create the Output Parquet file S3 Storage Path\n    s3_storage_path = f\"{self.data_sources_s3_path}/{self.output_uuid}\"\n\n    # Read JSONL files from S3 and infer schema dynamically\n    self.log.info(f\"Reading JSONL files from {self.input_uuid}...\")\n    input_dyf = self.glue_context.create_dynamic_frame.from_options(\n        connection_type=\"s3\",\n        connection_options={\n            \"paths\": [self.input_uuid],\n            \"recurse\": True,\n            \"gzip\": True,\n        },\n        format=input_type,\n        # format_options={'jsonPath': 'auto'}, Look into this later\n    )\n    self.log.info(\"Incoming DataFrame...\")\n    input_dyf.show(5)\n    input_dyf.printSchema()\n\n    # Resolve Choice fields\n    resolved_dyf = self.resolve_choice_fields(input_dyf)\n\n    # The next couple of lines of code is for un-nesting any nested JSON\n    # Create a Dynamic Frame Collection (dfc)\n    dfc = Relationalize.apply(resolved_dyf, name=\"root\")\n\n    # Aggregate the collection into a single dynamic frame\n    output_dyf = dfc.select(\"root\")\n\n    print(\"Before TimeStamp Conversions\")\n    output_dyf.printSchema()\n\n    # Convert any timestamp columns\n    output_dyf = self.timestamp_conversions(output_dyf, timestamp_columns)\n\n    # Relationalize will put periods in the column names. This will cause\n    # problems later when we try to create a FeatureSet from this DataSource\n    output_dyf = self.remove_periods_from_column_names(output_dyf)\n\n    print(\"After TimeStamp Conversions and Removing Periods from column names\")\n    output_dyf.printSchema()\n\n    # Write Parquet files to S3\n    self.log.info(f\"Writing Parquet files to {s3_storage_path}...\")\n    self.glue_context.purge_s3_path(s3_storage_path, {\"retentionPeriod\": 0})\n    self.glue_context.write_dynamic_frame.from_options(\n        frame=output_dyf,\n        connection_type=\"s3\",\n        connection_options={\n            \"path\": s3_storage_path\n            # \"partitionKeys\": [\"year\", \"month\", \"day\"],\n        },\n        format=output_format,\n    )\n\n    # Set up our SageWorks metadata (description, tags, etc)\n    description = f\"SageWorks data source: {self.output_uuid}\"\n    sageworks_meta = {\"sageworks_tags\": \":\".join(tags)}\n    for key, value in self.output_meta.items():\n        sageworks_meta[key] = value\n\n    # Create a new table in the AWS Data Catalog\n    self.log.info(f\"Creating Data Catalog Table: {self.output_uuid}...\")\n\n    # Converting the Spark Types to Athena Types\n    def to_athena_type(col):\n        athena_type_map = {\"long\": \"bigint\"}\n        spark_type = col.dataType.typeName()\n        return athena_type_map.get(spark_type, spark_type)\n\n    column_name_types = [{\"Name\": col.name, \"Type\": to_athena_type(col)} for col in output_dyf.schema().fields]\n\n    # Our parameters for the Glue Data Catalog are different for Parquet and ORC\n    if output_format == \"parquet\":\n        glue_input_format = \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\"\n        glue_output_format = \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\"\n        serialization_library = \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\"\n    else:\n        glue_input_format = \"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\"\n        glue_output_format = \"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\"\n        serialization_library = \"org.apache.hadoop.hive.ql.io.orc.OrcSerde\"\n\n    table_input = {\n        \"Name\": self.output_uuid,\n        \"Description\": description,\n        \"Parameters\": sageworks_meta,\n        \"TableType\": \"EXTERNAL_TABLE\",\n        \"StorageDescriptor\": {\n            \"Columns\": column_name_types,\n            \"Location\": s3_storage_path,\n            \"InputFormat\": glue_input_format,\n            \"OutputFormat\": glue_output_format,\n            \"Compressed\": True,\n            \"SerdeInfo\": {\n                \"SerializationLibrary\": serialization_library,\n            },\n        },\n    }\n\n    # Delete the Data Catalog Table if it already exists\n    glue_client = boto3.client(\"glue\")\n    try:\n        glue_client.delete_table(DatabaseName=\"sageworks\", Name=self.output_uuid)\n        self.log.info(f\"Deleting Data Catalog Table: {self.output_uuid}...\")\n    except ClientError as e:\n        if e.response[\"Error\"][\"Code\"] != \"EntityNotFoundException\":\n            raise e\n\n    self.log.info(f\"Creating Data Catalog Table: {self.output_uuid}...\")\n    glue_client.create_table(DatabaseName=\"sageworks\", TableInput=table_input)\n\n    # All done!\n    self.log.info(f\"{self.input_uuid} --&gt; {self.output_uuid} complete!\")\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_light/","title":"DataLoaders Light","text":"<p>API Classes</p> <p>For most users the API Classes will provide all the general functionality to create a full AWS ML Pipeline</p> <p>These DataLoader Classes are intended to load smaller dataset into AWS. If you have large data please see DataLoaders Heavy</p> <p>Welcome to the SageWorks DataLoaders Light Classes</p> <p>These classes provide low-level APIs for loading smaller data into AWS services</p> <ul> <li>CSVToDataSource: Loads local CSV data into a DataSource</li> <li>JSONToDataSource: Loads local JSON data into a DataSource</li> <li>S3ToDataSourceLight: Loads S3 data into a DataSource</li> </ul>"},{"location":"core_classes/transforms/data_loaders_light/#sageworks.core.transforms.data_loaders.light.CSVToDataSource","title":"<code>CSVToDataSource</code>","text":"<p>             Bases: <code>Transform</code></p> <p>CSVToDataSource: Class to move local CSV Files into a SageWorks DataSource</p> Common Usage <pre><code>csv_to_data = CSVToDataSource(csv_file_path, data_uuid)\ncsv_to_data.set_output_tags([\"abalone\", \"csv\", \"whatever\"])\ncsv_to_data.transform()\n</code></pre> Source code in <code>src/sageworks/core/transforms/data_loaders/light/csv_to_data_source.py</code> <pre><code>class CSVToDataSource(Transform):\n    \"\"\"CSVToDataSource: Class to move local CSV Files into a SageWorks DataSource\n\n    Common Usage:\n        ```\n        csv_to_data = CSVToDataSource(csv_file_path, data_uuid)\n        csv_to_data.set_output_tags([\"abalone\", \"csv\", \"whatever\"])\n        csv_to_data.transform()\n        ```\n    \"\"\"\n\n    def __init__(self, csv_file_path: str, data_uuid: str):\n        \"\"\"CSVToDataSource: Class to move local CSV Files into a SageWorks DataSource\n\n        Args:\n            csv_file_path (str): The path to the CSV file to be transformed\n            data_uuid (str): The UUID of the SageWorks DataSource to be created\n        \"\"\"\n\n        # Call superclass init\n        super().__init__(csv_file_path, data_uuid)\n\n        # Set up all my instance attributes\n        self.input_type = TransformInput.LOCAL_FILE\n        self.output_type = TransformOutput.DATA_SOURCE\n\n    def transform_impl(self, overwrite: bool = True):\n        \"\"\"Convert the local CSV file into Parquet Format in the SageWorks Data Sources Bucket, and\n        store the information about the data to the AWS Data Catalog sageworks database\n        \"\"\"\n\n        # Report the transformation initiation\n        csv_file = os.path.basename(self.input_uuid)\n        self.log.info(f\"Starting {csv_file} --&gt;  DataSource: {self.output_uuid}...\")\n\n        # Read in the Local CSV as a Pandas DataFrame\n        df = pd.read_csv(self.input_uuid, low_memory=False)\n        df = convert_object_columns(df)\n\n        # Use the SageWorks Pandas to Data Source class\n        pandas_to_data = PandasToData(self.output_uuid)\n        pandas_to_data.set_input(df)\n        pandas_to_data.set_output_tags(self.output_tags)\n        pandas_to_data.add_output_meta(self.output_meta)\n        pandas_to_data.transform()\n\n        # Report the transformation results\n        self.log.info(f\"{csv_file} --&gt;  DataSource: {self.output_uuid} Complete!\")\n\n    def post_transform(self, **kwargs):\n        \"\"\"Post-Transform: Calling make_ready() on the DataSource\"\"\"\n        self.log.info(\"Post-Transform: Calling make_ready() on the DataSource...\")\n\n        # Okay, lets wait just a bit for the\n        output_data_source = DataSourceFactory(self.output_uuid, force_refresh=True)\n        output_data_source.set_status(\"initializing\")\n\n        # Call the FeatureSet make_ready method to compute a bunch of EDA stuff\n        output_data_source.make_ready()\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_light/#sageworks.core.transforms.data_loaders.light.CSVToDataSource.__init__","title":"<code>__init__(csv_file_path, data_uuid)</code>","text":"<p>CSVToDataSource: Class to move local CSV Files into a SageWorks DataSource</p> <p>Parameters:</p> Name Type Description Default <code>csv_file_path</code> <code>str</code> <p>The path to the CSV file to be transformed</p> required <code>data_uuid</code> <code>str</code> <p>The UUID of the SageWorks DataSource to be created</p> required Source code in <code>src/sageworks/core/transforms/data_loaders/light/csv_to_data_source.py</code> <pre><code>def __init__(self, csv_file_path: str, data_uuid: str):\n    \"\"\"CSVToDataSource: Class to move local CSV Files into a SageWorks DataSource\n\n    Args:\n        csv_file_path (str): The path to the CSV file to be transformed\n        data_uuid (str): The UUID of the SageWorks DataSource to be created\n    \"\"\"\n\n    # Call superclass init\n    super().__init__(csv_file_path, data_uuid)\n\n    # Set up all my instance attributes\n    self.input_type = TransformInput.LOCAL_FILE\n    self.output_type = TransformOutput.DATA_SOURCE\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_light/#sageworks.core.transforms.data_loaders.light.CSVToDataSource.post_transform","title":"<code>post_transform(**kwargs)</code>","text":"<p>Post-Transform: Calling make_ready() on the DataSource</p> Source code in <code>src/sageworks/core/transforms/data_loaders/light/csv_to_data_source.py</code> <pre><code>def post_transform(self, **kwargs):\n    \"\"\"Post-Transform: Calling make_ready() on the DataSource\"\"\"\n    self.log.info(\"Post-Transform: Calling make_ready() on the DataSource...\")\n\n    # Okay, lets wait just a bit for the\n    output_data_source = DataSourceFactory(self.output_uuid, force_refresh=True)\n    output_data_source.set_status(\"initializing\")\n\n    # Call the FeatureSet make_ready method to compute a bunch of EDA stuff\n    output_data_source.make_ready()\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_light/#sageworks.core.transforms.data_loaders.light.CSVToDataSource.transform_impl","title":"<code>transform_impl(overwrite=True)</code>","text":"<p>Convert the local CSV file into Parquet Format in the SageWorks Data Sources Bucket, and store the information about the data to the AWS Data Catalog sageworks database</p> Source code in <code>src/sageworks/core/transforms/data_loaders/light/csv_to_data_source.py</code> <pre><code>def transform_impl(self, overwrite: bool = True):\n    \"\"\"Convert the local CSV file into Parquet Format in the SageWorks Data Sources Bucket, and\n    store the information about the data to the AWS Data Catalog sageworks database\n    \"\"\"\n\n    # Report the transformation initiation\n    csv_file = os.path.basename(self.input_uuid)\n    self.log.info(f\"Starting {csv_file} --&gt;  DataSource: {self.output_uuid}...\")\n\n    # Read in the Local CSV as a Pandas DataFrame\n    df = pd.read_csv(self.input_uuid, low_memory=False)\n    df = convert_object_columns(df)\n\n    # Use the SageWorks Pandas to Data Source class\n    pandas_to_data = PandasToData(self.output_uuid)\n    pandas_to_data.set_input(df)\n    pandas_to_data.set_output_tags(self.output_tags)\n    pandas_to_data.add_output_meta(self.output_meta)\n    pandas_to_data.transform()\n\n    # Report the transformation results\n    self.log.info(f\"{csv_file} --&gt;  DataSource: {self.output_uuid} Complete!\")\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_light/#sageworks.core.transforms.data_loaders.light.JSONToDataSource","title":"<code>JSONToDataSource</code>","text":"<p>             Bases: <code>Transform</code></p> <p>JSONToDataSource: Class to move local JSON Files into a SageWorks DataSource</p> Common Usage <pre><code>json_to_data = JSONToDataSource(json_file_path, data_uuid)\njson_to_data.set_output_tags([\"abalone\", \"json\", \"whatever\"])\njson_to_data.transform()\n</code></pre> Source code in <code>src/sageworks/core/transforms/data_loaders/light/json_to_data_source.py</code> <pre><code>class JSONToDataSource(Transform):\n    \"\"\"JSONToDataSource: Class to move local JSON Files into a SageWorks DataSource\n\n    Common Usage:\n        ```\n        json_to_data = JSONToDataSource(json_file_path, data_uuid)\n        json_to_data.set_output_tags([\"abalone\", \"json\", \"whatever\"])\n        json_to_data.transform()\n        ```\n    \"\"\"\n\n    def __init__(self, json_file_path: str, data_uuid: str):\n        \"\"\"JSONToDataSource: Class to move local JSON Files into a SageWorks DataSource\n\n        Args:\n            json_file_path (str): The path to the JSON file to be transformed\n            data_uuid (str): The UUID of the SageWorks DataSource to be created\n        \"\"\"\n\n        # Call superclass init\n        super().__init__(json_file_path, data_uuid)\n\n        # Set up all my instance attributes\n        self.input_type = TransformInput.LOCAL_FILE\n        self.output_type = TransformOutput.DATA_SOURCE\n\n    def transform_impl(self, overwrite: bool = True):\n        \"\"\"Convert the local JSON file into Parquet Format in the SageWorks Data Sources Bucket, and\n        store the information about the data to the AWS Data Catalog sageworks database\n        \"\"\"\n\n        # Report the transformation initiation\n        json_file = os.path.basename(self.input_uuid)\n        self.log.info(f\"Starting {json_file} --&gt;  DataSource: {self.output_uuid}...\")\n\n        # Read in the Local JSON as a Pandas DataFrame\n        df = pd.read_json(self.input_uuid, lines=True)\n\n        # Use the SageWorks Pandas to Data Source class\n        pandas_to_data = PandasToData(self.output_uuid)\n        pandas_to_data.set_input(df)\n        pandas_to_data.set_output_tags(self.output_tags)\n        pandas_to_data.add_output_meta(self.output_meta)\n        pandas_to_data.transform()\n\n        # Report the transformation results\n        self.log.info(f\"{json_file} --&gt;  DataSource: {self.output_uuid} Complete!\")\n\n    def post_transform(self, **kwargs):\n        \"\"\"Post-Transform: Calling make_ready() on the DataSource\"\"\"\n        self.log.info(\"Post-Transform: Calling make_ready() on the DataSource...\")\n\n        # Okay, lets wait just a bit for the\n        output_data_source = DataSourceFactory(self.output_uuid, force_refresh=True)\n        output_data_source.set_status(\"initializing\")\n\n        # Call the FeatureSet make_ready method to compute a bunch of EDA stuff\n        output_data_source.make_ready()\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_light/#sageworks.core.transforms.data_loaders.light.JSONToDataSource.__init__","title":"<code>__init__(json_file_path, data_uuid)</code>","text":"<p>JSONToDataSource: Class to move local JSON Files into a SageWorks DataSource</p> <p>Parameters:</p> Name Type Description Default <code>json_file_path</code> <code>str</code> <p>The path to the JSON file to be transformed</p> required <code>data_uuid</code> <code>str</code> <p>The UUID of the SageWorks DataSource to be created</p> required Source code in <code>src/sageworks/core/transforms/data_loaders/light/json_to_data_source.py</code> <pre><code>def __init__(self, json_file_path: str, data_uuid: str):\n    \"\"\"JSONToDataSource: Class to move local JSON Files into a SageWorks DataSource\n\n    Args:\n        json_file_path (str): The path to the JSON file to be transformed\n        data_uuid (str): The UUID of the SageWorks DataSource to be created\n    \"\"\"\n\n    # Call superclass init\n    super().__init__(json_file_path, data_uuid)\n\n    # Set up all my instance attributes\n    self.input_type = TransformInput.LOCAL_FILE\n    self.output_type = TransformOutput.DATA_SOURCE\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_light/#sageworks.core.transforms.data_loaders.light.JSONToDataSource.post_transform","title":"<code>post_transform(**kwargs)</code>","text":"<p>Post-Transform: Calling make_ready() on the DataSource</p> Source code in <code>src/sageworks/core/transforms/data_loaders/light/json_to_data_source.py</code> <pre><code>def post_transform(self, **kwargs):\n    \"\"\"Post-Transform: Calling make_ready() on the DataSource\"\"\"\n    self.log.info(\"Post-Transform: Calling make_ready() on the DataSource...\")\n\n    # Okay, lets wait just a bit for the\n    output_data_source = DataSourceFactory(self.output_uuid, force_refresh=True)\n    output_data_source.set_status(\"initializing\")\n\n    # Call the FeatureSet make_ready method to compute a bunch of EDA stuff\n    output_data_source.make_ready()\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_light/#sageworks.core.transforms.data_loaders.light.JSONToDataSource.transform_impl","title":"<code>transform_impl(overwrite=True)</code>","text":"<p>Convert the local JSON file into Parquet Format in the SageWorks Data Sources Bucket, and store the information about the data to the AWS Data Catalog sageworks database</p> Source code in <code>src/sageworks/core/transforms/data_loaders/light/json_to_data_source.py</code> <pre><code>def transform_impl(self, overwrite: bool = True):\n    \"\"\"Convert the local JSON file into Parquet Format in the SageWorks Data Sources Bucket, and\n    store the information about the data to the AWS Data Catalog sageworks database\n    \"\"\"\n\n    # Report the transformation initiation\n    json_file = os.path.basename(self.input_uuid)\n    self.log.info(f\"Starting {json_file} --&gt;  DataSource: {self.output_uuid}...\")\n\n    # Read in the Local JSON as a Pandas DataFrame\n    df = pd.read_json(self.input_uuid, lines=True)\n\n    # Use the SageWorks Pandas to Data Source class\n    pandas_to_data = PandasToData(self.output_uuid)\n    pandas_to_data.set_input(df)\n    pandas_to_data.set_output_tags(self.output_tags)\n    pandas_to_data.add_output_meta(self.output_meta)\n    pandas_to_data.transform()\n\n    # Report the transformation results\n    self.log.info(f\"{json_file} --&gt;  DataSource: {self.output_uuid} Complete!\")\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_light/#sageworks.core.transforms.data_loaders.light.S3ToDataSourceLight","title":"<code>S3ToDataSourceLight</code>","text":"<p>             Bases: <code>Transform</code></p> <p>S3ToDataSourceLight: Class to move LIGHT S3 Files into a SageWorks DataSource</p> Common Usage <pre><code>s3_to_data = S3ToDataSourceLight(s3_path, data_uuid, datatype=\"csv/json\")\ns3_to_data.set_output_tags([\"abalone\", \"whatever\"])\ns3_to_data.transform()\n</code></pre> Source code in <code>src/sageworks/core/transforms/data_loaders/light/s3_to_data_source_light.py</code> <pre><code>class S3ToDataSourceLight(Transform):\n    \"\"\"S3ToDataSourceLight: Class to move LIGHT S3 Files into a SageWorks DataSource\n\n    Common Usage:\n        ```\n        s3_to_data = S3ToDataSourceLight(s3_path, data_uuid, datatype=\"csv/json\")\n        s3_to_data.set_output_tags([\"abalone\", \"whatever\"])\n        s3_to_data.transform()\n        ```\n    \"\"\"\n\n    def __init__(self, s3_path: str, data_uuid: str, datatype: str = \"csv\"):\n        \"\"\"S3ToDataSourceLight Initialization\n\n        Args:\n            s3_path (str): The S3 Path to the file to be transformed\n            data_uuid (str): The UUID of the SageWorks DataSource to be created\n            datatype (str): The datatype of the file to be transformed (defaults to \"csv\")\n        \"\"\"\n\n        # Call superclass init\n        super().__init__(s3_path, data_uuid)\n\n        # Set up all my instance attributes\n        self.input_type = TransformInput.S3_OBJECT\n        self.output_type = TransformOutput.DATA_SOURCE\n        self.datatype = datatype\n\n    def input_size_mb(self) -&gt; int:\n        \"\"\"Get the size of the input S3 object in MBytes\"\"\"\n        size_in_bytes = wr.s3.size_objects(self.input_uuid, boto3_session=self.boto_session)[self.input_uuid]\n        size_in_mb = round(size_in_bytes / 1_000_000)\n        return size_in_mb\n\n    def transform_impl(self, overwrite: bool = True):\n        \"\"\"Convert the S3 CSV data into Parquet Format in the SageWorks Data Sources Bucket, and\n        store the information about the data to the AWS Data Catalog sageworks database\n        \"\"\"\n\n        # Sanity Check for S3 Object size\n        object_megabytes = self.input_size_mb()\n        if object_megabytes &gt; 100:\n            self.log.error(f\"S3 Object too big ({object_megabytes} MBytes): Use the S3ToDataSourceHeavy class!\")\n            return\n\n        # Read in the S3 CSV as a Pandas DataFrame\n        if self.datatype == \"csv\":\n            df = wr.s3.read_csv(self.input_uuid, low_memory=False, boto3_session=self.boto_session)\n        else:\n            df = wr.s3.read_json(self.input_uuid, lines=True, boto3_session=self.boto_session)\n\n        # Temporary hack to limit the number of columns in the dataframe\n        if len(df.columns) &gt; 40:\n            self.log.warning(f\"{self.input_uuid} Too Many Columns! Talk to SageWorks Support...\")\n\n        # Convert object columns before sending to SageWorks Data Source\n        df = convert_object_columns(df)\n\n        # Use the SageWorks Pandas to Data Source class\n        pandas_to_data = PandasToData(self.output_uuid)\n        pandas_to_data.set_input(df)\n        pandas_to_data.set_output_tags(self.output_tags)\n        pandas_to_data.add_output_meta(self.output_meta)\n        pandas_to_data.transform()\n\n        # Report the transformation results\n        self.log.info(f\"{self.input_uuid} --&gt;  DataSource: {self.output_uuid} Complete!\")\n\n    def post_transform(self, **kwargs):\n        \"\"\"Post-Transform: Calling make_ready() on the DataSource\"\"\"\n        self.log.info(\"Post-Transform: Calling make_ready() on the DataSource...\")\n\n        # Okay, lets wait just a bit for the\n        output_data_source = DataSourceFactory(self.output_uuid, force_refresh=True)\n        output_data_source.set_status(\"initializing\")\n\n        # Call the FeatureSet make_ready method to compute a bunch of EDA stuff\n        output_data_source.make_ready()\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_light/#sageworks.core.transforms.data_loaders.light.S3ToDataSourceLight.__init__","title":"<code>__init__(s3_path, data_uuid, datatype='csv')</code>","text":"<p>S3ToDataSourceLight Initialization</p> <p>Parameters:</p> Name Type Description Default <code>s3_path</code> <code>str</code> <p>The S3 Path to the file to be transformed</p> required <code>data_uuid</code> <code>str</code> <p>The UUID of the SageWorks DataSource to be created</p> required <code>datatype</code> <code>str</code> <p>The datatype of the file to be transformed (defaults to \"csv\")</p> <code>'csv'</code> Source code in <code>src/sageworks/core/transforms/data_loaders/light/s3_to_data_source_light.py</code> <pre><code>def __init__(self, s3_path: str, data_uuid: str, datatype: str = \"csv\"):\n    \"\"\"S3ToDataSourceLight Initialization\n\n    Args:\n        s3_path (str): The S3 Path to the file to be transformed\n        data_uuid (str): The UUID of the SageWorks DataSource to be created\n        datatype (str): The datatype of the file to be transformed (defaults to \"csv\")\n    \"\"\"\n\n    # Call superclass init\n    super().__init__(s3_path, data_uuid)\n\n    # Set up all my instance attributes\n    self.input_type = TransformInput.S3_OBJECT\n    self.output_type = TransformOutput.DATA_SOURCE\n    self.datatype = datatype\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_light/#sageworks.core.transforms.data_loaders.light.S3ToDataSourceLight.input_size_mb","title":"<code>input_size_mb()</code>","text":"<p>Get the size of the input S3 object in MBytes</p> Source code in <code>src/sageworks/core/transforms/data_loaders/light/s3_to_data_source_light.py</code> <pre><code>def input_size_mb(self) -&gt; int:\n    \"\"\"Get the size of the input S3 object in MBytes\"\"\"\n    size_in_bytes = wr.s3.size_objects(self.input_uuid, boto3_session=self.boto_session)[self.input_uuid]\n    size_in_mb = round(size_in_bytes / 1_000_000)\n    return size_in_mb\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_light/#sageworks.core.transforms.data_loaders.light.S3ToDataSourceLight.post_transform","title":"<code>post_transform(**kwargs)</code>","text":"<p>Post-Transform: Calling make_ready() on the DataSource</p> Source code in <code>src/sageworks/core/transforms/data_loaders/light/s3_to_data_source_light.py</code> <pre><code>def post_transform(self, **kwargs):\n    \"\"\"Post-Transform: Calling make_ready() on the DataSource\"\"\"\n    self.log.info(\"Post-Transform: Calling make_ready() on the DataSource...\")\n\n    # Okay, lets wait just a bit for the\n    output_data_source = DataSourceFactory(self.output_uuid, force_refresh=True)\n    output_data_source.set_status(\"initializing\")\n\n    # Call the FeatureSet make_ready method to compute a bunch of EDA stuff\n    output_data_source.make_ready()\n</code></pre>"},{"location":"core_classes/transforms/data_loaders_light/#sageworks.core.transforms.data_loaders.light.S3ToDataSourceLight.transform_impl","title":"<code>transform_impl(overwrite=True)</code>","text":"<p>Convert the S3 CSV data into Parquet Format in the SageWorks Data Sources Bucket, and store the information about the data to the AWS Data Catalog sageworks database</p> Source code in <code>src/sageworks/core/transforms/data_loaders/light/s3_to_data_source_light.py</code> <pre><code>def transform_impl(self, overwrite: bool = True):\n    \"\"\"Convert the S3 CSV data into Parquet Format in the SageWorks Data Sources Bucket, and\n    store the information about the data to the AWS Data Catalog sageworks database\n    \"\"\"\n\n    # Sanity Check for S3 Object size\n    object_megabytes = self.input_size_mb()\n    if object_megabytes &gt; 100:\n        self.log.error(f\"S3 Object too big ({object_megabytes} MBytes): Use the S3ToDataSourceHeavy class!\")\n        return\n\n    # Read in the S3 CSV as a Pandas DataFrame\n    if self.datatype == \"csv\":\n        df = wr.s3.read_csv(self.input_uuid, low_memory=False, boto3_session=self.boto_session)\n    else:\n        df = wr.s3.read_json(self.input_uuid, lines=True, boto3_session=self.boto_session)\n\n    # Temporary hack to limit the number of columns in the dataframe\n    if len(df.columns) &gt; 40:\n        self.log.warning(f\"{self.input_uuid} Too Many Columns! Talk to SageWorks Support...\")\n\n    # Convert object columns before sending to SageWorks Data Source\n    df = convert_object_columns(df)\n\n    # Use the SageWorks Pandas to Data Source class\n    pandas_to_data = PandasToData(self.output_uuid)\n    pandas_to_data.set_input(df)\n    pandas_to_data.set_output_tags(self.output_tags)\n    pandas_to_data.add_output_meta(self.output_meta)\n    pandas_to_data.transform()\n\n    # Report the transformation results\n    self.log.info(f\"{self.input_uuid} --&gt;  DataSource: {self.output_uuid} Complete!\")\n</code></pre>"},{"location":"core_classes/transforms/data_to_features/","title":"Data To Features","text":"<p>API Classes</p> <p>For most users the API Classes will provide all the general functionality to create a full AWS ML Pipeline</p> <p>DataToFeaturesLight: Base Class for Light DataSource to FeatureSet using Pandas</p> <p>RDKitDescriptors: Compute a Feature Set based on RDKit Descriptors</p>"},{"location":"core_classes/transforms/data_to_features/#sageworks.core.transforms.data_to_features.light.data_to_features_light.DataToFeaturesLight","title":"<code>DataToFeaturesLight</code>","text":"<p>             Bases: <code>Transform</code></p> <p>DataToFeaturesLight: Base Class for Light DataSource to FeatureSet using Pandas</p> Common Usage <pre><code>to_features = DataToFeaturesLight(data_uuid, feature_uuid)\nto_features.set_output_tags([\"abalone\", \"public\", \"whatever\"])\nto_features.transform(target_column=\"target\"/None, id_column=\"id\"/None,\n                      event_time_column=\"date\"/None, query=str/None)\n</code></pre> Source code in <code>src/sageworks/core/transforms/data_to_features/light/data_to_features_light.py</code> <pre><code>class DataToFeaturesLight(Transform):\n    \"\"\"DataToFeaturesLight: Base Class for Light DataSource to FeatureSet using Pandas\n\n    Common Usage:\n        ```\n        to_features = DataToFeaturesLight(data_uuid, feature_uuid)\n        to_features.set_output_tags([\"abalone\", \"public\", \"whatever\"])\n        to_features.transform(target_column=\"target\"/None, id_column=\"id\"/None,\n                              event_time_column=\"date\"/None, query=str/None)\n        ```\n    \"\"\"\n\n    def __init__(self, data_uuid: str, feature_uuid: str):\n        \"\"\"DataToFeaturesLight Initialization\n\n        Args:\n            data_uuid (str): The UUID of the SageWorks DataSource to be transformed\n            feature_uuid (str): The UUID of the SageWorks FeatureSet to be created\n        \"\"\"\n\n        # Call superclass init\n        super().__init__(data_uuid, feature_uuid)\n\n        # Set up all my instance attributes\n        self.input_type = TransformInput.DATA_SOURCE\n        self.output_type = TransformOutput.FEATURE_SET\n        self.input_df = None\n        self.output_df = None\n\n    def pre_transform(self, query: str = None, **kwargs):\n        \"\"\"Pull the input DataSource into our Input Pandas DataFrame\n        Args:\n            query(str): Optional query to filter the input DataFrame\n        \"\"\"\n\n        # Grab the Input (Data Source)\n        data_to_pandas = DataToPandas(self.input_uuid)\n        data_to_pandas.transform(query=query)\n        self.input_df = data_to_pandas.get_output()\n\n    def transform_impl(self, **kwargs):\n        \"\"\"Transform the input DataFrame into a Feature Set\"\"\"\n\n        # This is a reference implementation that should be overridden by the subclass\n        self.output_df = self.input_df\n\n    def post_transform(\n        self, target_column=None, id_column=None, event_time_column=None, auto_categorize=True, **kwargs\n    ):\n        \"\"\"At this point the output DataFrame should be populated, so publish it as a Feature Set\n        Args:\n            target_column(str): The name of the target column in the output DataFrame (default: None)\n            id_column(str): The name of the id column in the output DataFrame (default: None)\n            event_time_column(str): The name of the event time column in the output DataFrame (default: None)\n            auto_categorize(bool): Whether to auto categorize the output DataFrame (default: True)\n        \"\"\"\n        # Now publish to the output location\n        output_features = PandasToFeatures(self.output_uuid, auto_categorize=auto_categorize)\n        output_features.set_input(\n            self.output_df, target_column=target_column, id_column=id_column, event_time_column=event_time_column\n        )\n        output_features.set_output_tags(self.output_tags)\n        output_features.add_output_meta(self.output_meta)\n        output_features.transform()\n\n        # Create a default training_view for this FeatureSet\n        fs = FeatureSetCore(self.output_uuid, force_refresh=True)\n        fs.create_default_training_view()\n</code></pre>"},{"location":"core_classes/transforms/data_to_features/#sageworks.core.transforms.data_to_features.light.data_to_features_light.DataToFeaturesLight.__init__","title":"<code>__init__(data_uuid, feature_uuid)</code>","text":"<p>DataToFeaturesLight Initialization</p> <p>Parameters:</p> Name Type Description Default <code>data_uuid</code> <code>str</code> <p>The UUID of the SageWorks DataSource to be transformed</p> required <code>feature_uuid</code> <code>str</code> <p>The UUID of the SageWorks FeatureSet to be created</p> required Source code in <code>src/sageworks/core/transforms/data_to_features/light/data_to_features_light.py</code> <pre><code>def __init__(self, data_uuid: str, feature_uuid: str):\n    \"\"\"DataToFeaturesLight Initialization\n\n    Args:\n        data_uuid (str): The UUID of the SageWorks DataSource to be transformed\n        feature_uuid (str): The UUID of the SageWorks FeatureSet to be created\n    \"\"\"\n\n    # Call superclass init\n    super().__init__(data_uuid, feature_uuid)\n\n    # Set up all my instance attributes\n    self.input_type = TransformInput.DATA_SOURCE\n    self.output_type = TransformOutput.FEATURE_SET\n    self.input_df = None\n    self.output_df = None\n</code></pre>"},{"location":"core_classes/transforms/data_to_features/#sageworks.core.transforms.data_to_features.light.data_to_features_light.DataToFeaturesLight.post_transform","title":"<code>post_transform(target_column=None, id_column=None, event_time_column=None, auto_categorize=True, **kwargs)</code>","text":"<p>At this point the output DataFrame should be populated, so publish it as a Feature Set Args:     target_column(str): The name of the target column in the output DataFrame (default: None)     id_column(str): The name of the id column in the output DataFrame (default: None)     event_time_column(str): The name of the event time column in the output DataFrame (default: None)     auto_categorize(bool): Whether to auto categorize the output DataFrame (default: True)</p> Source code in <code>src/sageworks/core/transforms/data_to_features/light/data_to_features_light.py</code> <pre><code>def post_transform(\n    self, target_column=None, id_column=None, event_time_column=None, auto_categorize=True, **kwargs\n):\n    \"\"\"At this point the output DataFrame should be populated, so publish it as a Feature Set\n    Args:\n        target_column(str): The name of the target column in the output DataFrame (default: None)\n        id_column(str): The name of the id column in the output DataFrame (default: None)\n        event_time_column(str): The name of the event time column in the output DataFrame (default: None)\n        auto_categorize(bool): Whether to auto categorize the output DataFrame (default: True)\n    \"\"\"\n    # Now publish to the output location\n    output_features = PandasToFeatures(self.output_uuid, auto_categorize=auto_categorize)\n    output_features.set_input(\n        self.output_df, target_column=target_column, id_column=id_column, event_time_column=event_time_column\n    )\n    output_features.set_output_tags(self.output_tags)\n    output_features.add_output_meta(self.output_meta)\n    output_features.transform()\n\n    # Create a default training_view for this FeatureSet\n    fs = FeatureSetCore(self.output_uuid, force_refresh=True)\n    fs.create_default_training_view()\n</code></pre>"},{"location":"core_classes/transforms/data_to_features/#sageworks.core.transforms.data_to_features.light.data_to_features_light.DataToFeaturesLight.pre_transform","title":"<code>pre_transform(query=None, **kwargs)</code>","text":"<p>Pull the input DataSource into our Input Pandas DataFrame Args:     query(str): Optional query to filter the input DataFrame</p> Source code in <code>src/sageworks/core/transforms/data_to_features/light/data_to_features_light.py</code> <pre><code>def pre_transform(self, query: str = None, **kwargs):\n    \"\"\"Pull the input DataSource into our Input Pandas DataFrame\n    Args:\n        query(str): Optional query to filter the input DataFrame\n    \"\"\"\n\n    # Grab the Input (Data Source)\n    data_to_pandas = DataToPandas(self.input_uuid)\n    data_to_pandas.transform(query=query)\n    self.input_df = data_to_pandas.get_output()\n</code></pre>"},{"location":"core_classes/transforms/data_to_features/#sageworks.core.transforms.data_to_features.light.data_to_features_light.DataToFeaturesLight.transform_impl","title":"<code>transform_impl(**kwargs)</code>","text":"<p>Transform the input DataFrame into a Feature Set</p> Source code in <code>src/sageworks/core/transforms/data_to_features/light/data_to_features_light.py</code> <pre><code>def transform_impl(self, **kwargs):\n    \"\"\"Transform the input DataFrame into a Feature Set\"\"\"\n\n    # This is a reference implementation that should be overridden by the subclass\n    self.output_df = self.input_df\n</code></pre>"},{"location":"core_classes/transforms/data_to_features/#sageworks.core.transforms.data_to_features.light.rdkit_descriptors.RDKitDescriptors","title":"<code>RDKitDescriptors</code>","text":"<p>             Bases: <code>DataToFeaturesLight</code></p> <p>RDKitDescriptors: Create a FeatureSet (RDKit Descriptors) from a DataSource</p> Common Usage <pre><code>to_features = RDKitDescriptors(data_uuid, feature_uuid)\nto_features.set_output_tags([\"aqsol\", \"rdkit\", \"whatever\"])\nto_features.transform()\n</code></pre> Source code in <code>src/sageworks/core/transforms/data_to_features/light/rdkit_descriptors.py</code> <pre><code>class RDKitDescriptors(DataToFeaturesLight):\n    \"\"\"RDKitDescriptors: Create a FeatureSet (RDKit Descriptors) from a DataSource\n\n    Common Usage:\n        ```\n        to_features = RDKitDescriptors(data_uuid, feature_uuid)\n        to_features.set_output_tags([\"aqsol\", \"rdkit\", \"whatever\"])\n        to_features.transform()\n        ```\n    \"\"\"\n\n    def __init__(self, data_uuid: str, feature_uuid: str):\n        \"\"\"RDKitDescriptors Initialization\n\n        Args:\n            data_uuid (str): The UUID of the SageWorks DataSource to be transformed\n            feature_uuid (str): The UUID of the SageWorks FeatureSet to be created\n        \"\"\"\n\n        # Call superclass init\n        super().__init__(data_uuid, feature_uuid)\n\n        # Turn off warnings for RDKIT (revisit this)\n        RDLogger.DisableLog(\"rdApp.*\")\n\n    def transform_impl(self, **kwargs):\n        \"\"\"Compute a Feature Set based on RDKit Descriptors\"\"\"\n\n        # Check the input DataFrame has the required columns\n        if \"smiles\" not in self.input_df.columns:\n            raise ValueError(\"Input DataFrame must have a 'smiles' column\")\n\n        # Compute/add all the RDKIT Descriptors\n        self.output_df = self.compute_rdkit_descriptors(self.input_df)\n\n        # Drop any NaNs (and INFs)\n        self.output_df = pandas_utils.drop_nans(self.output_df, how=\"all\")\n\n    def compute_rdkit_descriptors(self, process_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Compute and add all the RDKit Descriptors\n        Args:\n            process_df(pd.DataFrame): The DataFrame to process and generate RDKit Descriptors\n        Returns:\n            pd.DataFrame: The input DataFrame with all the RDKit Descriptors added\n        \"\"\"\n        self.log.important(\"Computing RDKit Descriptors...\")\n\n        # Conversion to Molecules\n        molecules = [Chem.MolFromSmiles(smile) for smile in process_df[\"smiles\"]]\n\n        # Now get all the RDKIT Descriptors\n        # all_descriptors = [x[0] for x in Descriptors._descList]\n\n        # There's an overflow issue that happens with the IPC descriptor, so we'll remove it\n        # See: https://github.com/rdkit/rdkit/issues/1527\n        # if \"Ipc\" in all_descriptors:\n        #    all_descriptors.remove(\"Ipc\")\n\n        # Get the descriptors that are most useful for Solubility\n        best_descriptors = [\n            \"MolLogP\",\n            \"MolWt\",\n            \"TPSA\",\n            \"NumHDonors\",\n            \"NumHAcceptors\",\n            \"NumRotatableBonds\",\n            \"NumAromaticRings\",\n            \"NumSaturatedRings\",\n            \"NumAliphaticRings\",\n            \"NumAromaticCarbocycles\",\n        ]\n        best_20_descriptors = best_descriptors + [\n            \"HeavyAtomCount\",\n            \"RingCount\",\n            \"Chi0\",\n            \"Chi1\",\n            \"Kappa1\",\n            \"Kappa2\",\n            \"Kappa3\",\n            \"LabuteASA\",\n            \"FractionCSP3\",\n            \"HallKierAlpha\",\n        ]\n        best_30_descriptors = best_20_descriptors + [\n            \"SMR_VSA1\",\n            \"SlogP_VSA1\",\n            \"EState_VSA1\",\n            \"VSA_EState1\",\n            \"PEOE_VSA1\",\n            \"NumValenceElectrons\",\n            \"NumRadicalElectrons\",\n            \"MaxPartialCharge\",\n            \"MinPartialCharge\",\n            \"MaxAbsPartialCharge\",\n        ]\n        best_40_descriptors = best_30_descriptors + [\n            \"MolMR\",\n            \"ExactMolWt\",\n            \"NOCount\",\n            \"NumHeteroatoms\",\n            \"NumAmideBonds\",\n            \"FpDensityMorgan1\",\n            \"FpDensityMorgan2\",\n            \"FpDensityMorgan3\",\n            \"MaxEStateIndex\",\n            \"MinEStateIndex\",\n        ]\n\n        # Super useful Molecular Descriptor Calculator Class\n        calc = MoleculeDescriptors.MolecularDescriptorCalculator(best_40_descriptors)\n        column_names = calc.GetDescriptorNames()\n\n        descriptor_values = [calc.CalcDescriptors(m) for m in molecules]\n        df_features = pd.DataFrame(descriptor_values, columns=column_names)\n        return pd.concat([process_df, df_features], axis=1)\n</code></pre>"},{"location":"core_classes/transforms/data_to_features/#sageworks.core.transforms.data_to_features.light.rdkit_descriptors.RDKitDescriptors.__init__","title":"<code>__init__(data_uuid, feature_uuid)</code>","text":"<p>RDKitDescriptors Initialization</p> <p>Parameters:</p> Name Type Description Default <code>data_uuid</code> <code>str</code> <p>The UUID of the SageWorks DataSource to be transformed</p> required <code>feature_uuid</code> <code>str</code> <p>The UUID of the SageWorks FeatureSet to be created</p> required Source code in <code>src/sageworks/core/transforms/data_to_features/light/rdkit_descriptors.py</code> <pre><code>def __init__(self, data_uuid: str, feature_uuid: str):\n    \"\"\"RDKitDescriptors Initialization\n\n    Args:\n        data_uuid (str): The UUID of the SageWorks DataSource to be transformed\n        feature_uuid (str): The UUID of the SageWorks FeatureSet to be created\n    \"\"\"\n\n    # Call superclass init\n    super().__init__(data_uuid, feature_uuid)\n\n    # Turn off warnings for RDKIT (revisit this)\n    RDLogger.DisableLog(\"rdApp.*\")\n</code></pre>"},{"location":"core_classes/transforms/data_to_features/#sageworks.core.transforms.data_to_features.light.rdkit_descriptors.RDKitDescriptors.compute_rdkit_descriptors","title":"<code>compute_rdkit_descriptors(process_df)</code>","text":"<p>Compute and add all the RDKit Descriptors Args:     process_df(pd.DataFrame): The DataFrame to process and generate RDKit Descriptors Returns:     pd.DataFrame: The input DataFrame with all the RDKit Descriptors added</p> Source code in <code>src/sageworks/core/transforms/data_to_features/light/rdkit_descriptors.py</code> <pre><code>def compute_rdkit_descriptors(self, process_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Compute and add all the RDKit Descriptors\n    Args:\n        process_df(pd.DataFrame): The DataFrame to process and generate RDKit Descriptors\n    Returns:\n        pd.DataFrame: The input DataFrame with all the RDKit Descriptors added\n    \"\"\"\n    self.log.important(\"Computing RDKit Descriptors...\")\n\n    # Conversion to Molecules\n    molecules = [Chem.MolFromSmiles(smile) for smile in process_df[\"smiles\"]]\n\n    # Now get all the RDKIT Descriptors\n    # all_descriptors = [x[0] for x in Descriptors._descList]\n\n    # There's an overflow issue that happens with the IPC descriptor, so we'll remove it\n    # See: https://github.com/rdkit/rdkit/issues/1527\n    # if \"Ipc\" in all_descriptors:\n    #    all_descriptors.remove(\"Ipc\")\n\n    # Get the descriptors that are most useful for Solubility\n    best_descriptors = [\n        \"MolLogP\",\n        \"MolWt\",\n        \"TPSA\",\n        \"NumHDonors\",\n        \"NumHAcceptors\",\n        \"NumRotatableBonds\",\n        \"NumAromaticRings\",\n        \"NumSaturatedRings\",\n        \"NumAliphaticRings\",\n        \"NumAromaticCarbocycles\",\n    ]\n    best_20_descriptors = best_descriptors + [\n        \"HeavyAtomCount\",\n        \"RingCount\",\n        \"Chi0\",\n        \"Chi1\",\n        \"Kappa1\",\n        \"Kappa2\",\n        \"Kappa3\",\n        \"LabuteASA\",\n        \"FractionCSP3\",\n        \"HallKierAlpha\",\n    ]\n    best_30_descriptors = best_20_descriptors + [\n        \"SMR_VSA1\",\n        \"SlogP_VSA1\",\n        \"EState_VSA1\",\n        \"VSA_EState1\",\n        \"PEOE_VSA1\",\n        \"NumValenceElectrons\",\n        \"NumRadicalElectrons\",\n        \"MaxPartialCharge\",\n        \"MinPartialCharge\",\n        \"MaxAbsPartialCharge\",\n    ]\n    best_40_descriptors = best_30_descriptors + [\n        \"MolMR\",\n        \"ExactMolWt\",\n        \"NOCount\",\n        \"NumHeteroatoms\",\n        \"NumAmideBonds\",\n        \"FpDensityMorgan1\",\n        \"FpDensityMorgan2\",\n        \"FpDensityMorgan3\",\n        \"MaxEStateIndex\",\n        \"MinEStateIndex\",\n    ]\n\n    # Super useful Molecular Descriptor Calculator Class\n    calc = MoleculeDescriptors.MolecularDescriptorCalculator(best_40_descriptors)\n    column_names = calc.GetDescriptorNames()\n\n    descriptor_values = [calc.CalcDescriptors(m) for m in molecules]\n    df_features = pd.DataFrame(descriptor_values, columns=column_names)\n    return pd.concat([process_df, df_features], axis=1)\n</code></pre>"},{"location":"core_classes/transforms/data_to_features/#sageworks.core.transforms.data_to_features.light.rdkit_descriptors.RDKitDescriptors.transform_impl","title":"<code>transform_impl(**kwargs)</code>","text":"<p>Compute a Feature Set based on RDKit Descriptors</p> Source code in <code>src/sageworks/core/transforms/data_to_features/light/rdkit_descriptors.py</code> <pre><code>def transform_impl(self, **kwargs):\n    \"\"\"Compute a Feature Set based on RDKit Descriptors\"\"\"\n\n    # Check the input DataFrame has the required columns\n    if \"smiles\" not in self.input_df.columns:\n        raise ValueError(\"Input DataFrame must have a 'smiles' column\")\n\n    # Compute/add all the RDKIT Descriptors\n    self.output_df = self.compute_rdkit_descriptors(self.input_df)\n\n    # Drop any NaNs (and INFs)\n    self.output_df = pandas_utils.drop_nans(self.output_df, how=\"all\")\n</code></pre>"},{"location":"core_classes/transforms/features_to_model/","title":"Features To Model","text":"<p>API Classes</p> <p>For most users the API Classes will provide all the general functionality to create a full AWS ML Pipeline</p> <p>FeaturesToModel: Train/Create a Model from a Feature Set</p>"},{"location":"core_classes/transforms/features_to_model/#sageworks.core.transforms.features_to_model.features_to_model.FeaturesToModel","title":"<code>FeaturesToModel</code>","text":"<p>             Bases: <code>Transform</code></p> <p>FeaturesToModel: Train/Create a Model from a FeatureSet</p> Common Usage <pre><code>to_model = FeaturesToModel(feature_uuid, model_uuid, model_type=ModelType)\nto_model.set_output_tags([\"abalone\", \"public\", \"whatever\"])\nto_model.transform(target_column=\"class_number_of_rings\",\n                   input_feature_list=[feature_list])\n</code></pre> Source code in <code>src/sageworks/core/transforms/features_to_model/features_to_model.py</code> <pre><code>class FeaturesToModel(Transform):\n    \"\"\"FeaturesToModel: Train/Create a Model from a FeatureSet\n\n    Common Usage:\n        ```\n        to_model = FeaturesToModel(feature_uuid, model_uuid, model_type=ModelType)\n        to_model.set_output_tags([\"abalone\", \"public\", \"whatever\"])\n        to_model.transform(target_column=\"class_number_of_rings\",\n                           input_feature_list=[feature_list])\n        ```\n    \"\"\"\n\n    def __init__(self, feature_uuid: str, model_uuid: str, model_type: ModelType):\n        \"\"\"FeaturesToModel Initialization\n        Args:\n            feature_uuid (str): UUID of the FeatureSet to use as input\n            model_uuid (str): UUID of the Model to create as output\n            model_type (ModelType): ModelType.REGRESSOR or ModelType.CLASSIFIER\n        \"\"\"\n\n        # Call superclass init\n        super().__init__(feature_uuid, model_uuid)\n\n        # Set up all my instance attributes\n        self.input_type = TransformInput.FEATURE_SET\n        self.output_type = TransformOutput.MODEL\n        self.model_type = model_type\n        self.estimator = None\n        self.model_script_dir = None\n        self.model_description = None\n        self.model_training_root = self.models_s3_path + \"/training\"\n        self.model_feature_list = None\n        self.target_column = None\n\n    def generate_model_script(\n        self, target_column: str, feature_list: list[str], model_type: ModelType, train_all_data: bool\n    ) -&gt; str:\n        \"\"\"Fill in the model template with specific target and feature_list\n        Args:\n            target_column (str): Column name of the target variable\n            feature_list (list[str]): A list of columns for the features\n            model_type (ModelType): ModelType.REGRESSOR or ModelType.CLASSIFIER\n            train_all_data (bool): Train on ALL (100%) of the data\n        Returns:\n           str: The name of the generated model script\n        \"\"\"\n\n        # FIXME: Revisit all of this since it's a bit wonky\n        script_name = \"generated_xgb_model.py\"\n        dir_path = Path(__file__).parent.absolute()\n        self.model_script_dir = os.path.join(dir_path, \"light_model_harness\")\n        template_path = os.path.join(self.model_script_dir, \"xgb_model.template\")\n        output_path = os.path.join(self.model_script_dir, script_name)\n        with open(template_path, \"r\") as fp:\n            xgb_template = fp.read()\n\n        # Template replacements\n        xgb_script = xgb_template.replace(\"{{target_column}}\", target_column)\n        feature_list_str = json.dumps(feature_list)\n        xgb_script = xgb_script.replace(\"{{feature_list}}\", feature_list_str)\n        xgb_script = xgb_script.replace(\"{{model_type}}\", model_type)\n        metrics_s3_path = f\"{self.model_training_root}/{self.output_uuid}\"\n        xgb_script = xgb_script.replace(\"{{model_metrics_s3_path}}\", metrics_s3_path)\n        xgb_script = xgb_script.replace(\"{{train_all_data}}\", str(train_all_data))\n\n        # Now write out the generated model script and return the name\n        with open(output_path, \"w\") as fp:\n            fp.write(xgb_script)\n        return script_name\n\n    def transform_impl(\n        self, target_column: str, description: str = None, feature_list: list = None, train_all_data=False\n    ):\n        \"\"\"Generic Features to Model: Note you should create a new class and inherit from\n        this one to include specific logic for your Feature Set/Model\n        Args:\n            target_column (str): Column name of the target variable\n            description (str): Description of the model (optional)\n            feature_list (list[str]): A list of columns for the features (default None, will try to guess)\n            train_all_data (bool): Train on ALL (100%) of the data (default False)\n        \"\"\"\n        # Delete the existing model (if it exists)\n        self.log.important(\"Trying to delete existing model...\")\n        delete_model = ModelCore(self.output_uuid, force_refresh=True)\n        delete_model.delete()\n\n        # Set our model description\n        self.model_description = description if description is not None else f\"Model created from {self.input_uuid}\"\n\n        # Get our Feature Set and create an S3 CSV Training dataset\n        feature_set = FeatureSetCore(self.input_uuid)\n        s3_training_path = feature_set.create_s3_training_data()\n        self.log.info(f\"Created new training data {s3_training_path}...\")\n\n        # Report the target column\n        self.target_column = target_column\n        self.log.info(f\"Target column: {self.target_column}\")\n\n        # Did they specify a feature list?\n        if feature_list:\n            # AWS Feature Groups will also add these implicit columns, so remove them\n            aws_cols = [\"write_time\", \"api_invocation_time\", \"is_deleted\", \"event_time\", \"training\"]\n            feature_list = [c for c in feature_list if c not in aws_cols]\n\n        # If they didn't specify a feature list, try to guess it\n        else:\n            # Try to figure out features with this logic\n            # - Don't include id, event_time, __index_level_0__, or training columns\n            # - Don't include AWS generated columns (e.g. write_time, api_invocation_time, is_deleted)\n            # - Don't include the target columns\n            # - Don't include any columns that are of type string or timestamp\n            # - The rest of the columns are assumed to be features\n            self.log.warning(\"Guessing at the feature list, HIGHLY SUGGESTED to specify an explicit feature list!\")\n            all_columns = feature_set.column_names()\n            filter_list = [\n                \"id\",\n                \"__index_level_0__\",\n                \"write_time\",\n                \"api_invocation_time\",\n                \"is_deleted\",\n                \"event_time\",\n                \"training\",\n            ] + [self.target_column]\n            feature_list = [c for c in all_columns if c not in filter_list]\n\n        # AWS Feature Store has 3 user column types (String, Integral, Fractional)\n        # and two internal types (Timestamp and Boolean). A Feature List for\n        # modeling can only contain Integral and Fractional types.\n        remove_columns = []\n        column_details = feature_set.column_details()\n        for column_name in feature_list:\n            if column_details[column_name] not in [\"Integral\", \"Fractional\"]:\n                self.log.warning(\n                    f\"Removing {column_name} from feature list, improper type {column_details[column_name]}\"\n                )\n                remove_columns.append(column_name)\n\n        # Remove the columns that are not Integral or Fractional\n        self.model_feature_list = [c for c in feature_list if c not in remove_columns]\n        self.log.important(f\"Feature List for Modeling: {self.model_feature_list}\")\n\n        # Generate our model script\n        script_path = self.generate_model_script(self.target_column, self.model_feature_list, self.model_type.value, train_all_data)\n\n        # Metric Definitions for Regression and Classification\n        if self.model_type == ModelType.REGRESSOR:\n            metric_definitions = [\n                {\"Name\": \"RMSE\", \"Regex\": \"RMSE: ([0-9.]+)\"},\n                {\"Name\": \"MAE\", \"Regex\": \"MAE: ([0-9.]+)\"},\n                {\"Name\": \"R2\", \"Regex\": \"R2 Score: ([0-9.]+)\"},\n            ]\n        else:\n            # We need to get creative with the Classification Metrics\n            # Grab all the target column class values\n            table = feature_set.data_source.get_table_name()\n            targets = feature_set.query(f\"select DISTINCT {self.target_column} FROM {table}\")[self.target_column].to_list()\n            metrics = [\"precision\", \"recall\", \"fscore\"]\n\n            # Dynamically create the metric definitions\n            metric_definitions = []\n            for t in targets:\n                for m in metrics:\n                    metric_definitions.append({\"Name\": f\"Metrics:{t}:{m}\", \"Regex\": f\"Metrics:{t}:{m} ([0-9.]+)\"})\n\n            # Add the confusion matrix metrics\n            for row in targets:\n                for col in targets:\n                    metric_definitions.append(\n                        {\"Name\": f\"ConfusionMatrix:{row}:{col}\", \"Regex\": f\"ConfusionMatrix:{row}:{col} ([0-9.]+)\"}\n                    )\n\n        # Create a Sagemaker Model with our script\n        self.estimator = SKLearn(\n            entry_point=script_path,\n            source_dir=self.model_script_dir,\n            role=self.sageworks_role_arn,\n            instance_type=\"ml.m5.large\",\n            sagemaker_session=self.sm_session,\n            framework_version=\"1.2-1\",\n            metric_definitions=metric_definitions,\n        )\n\n        # Train the estimator\n        self.estimator.fit({\"train\": s3_training_path})\n\n        # Now delete the training data\n        self.log.info(f\"Deleting training data {s3_training_path}...\")\n        wr.s3.delete_objects(\n            [s3_training_path, s3_training_path.replace(\".csv\", \".csv.metadata\")],\n            boto3_session=self.boto_session,\n        )\n\n        # Create Model and officially Register\n        self.log.important(f\"Creating new model {self.output_uuid}...\")\n        self.create_and_register_model()\n\n    def post_transform(self, **kwargs):\n        \"\"\"Post-Transform: Calling make_ready() on the Model\"\"\"\n        self.log.info(\"Post-Transform: Calling onboard() on the Model...\")\n\n        # Okay, lets get our output model and set it to initializing\n        output_model = ModelCore(self.output_uuid, model_type=self.model_type, force_refresh=True)\n        output_model.set_status(\"initializing\")\n\n        # Store the model feature_list and target_column in the sageworks_meta\n        output_model.upsert_sageworks_meta({\"sageworks_model_features\": self.model_feature_list})\n        output_model.upsert_sageworks_meta({\"sageworks_model_target\": self.target_column})\n\n        # Call the Model onboard method\n        output_model.onboard()\n\n    def create_and_register_model(self):\n        \"\"\"Create and Register the Model\"\"\"\n\n        # Get the metadata/tags to push into AWS\n        aws_tags = self.get_aws_tags()\n\n        # Create model group (if it doesn't already exist)\n        self.sm_client.create_model_package_group(\n            ModelPackageGroupName=self.output_uuid,\n            ModelPackageGroupDescription=self.model_description,\n            Tags=aws_tags,\n        )\n\n        # Register our model\n        model = self.estimator.create_model(role=self.sageworks_role_arn)\n        model.register(\n            model_package_group_name=self.output_uuid,\n            framework_version=\"1.2.1\",\n            content_types=[\"text/csv\"],\n            response_types=[\"text/csv\"],\n            inference_instances=[\"ml.t2.medium\"],\n            transform_instances=[\"ml.m5.large\"],\n            approval_status=\"Approved\",\n            description=self.model_description,\n        )\n</code></pre>"},{"location":"core_classes/transforms/features_to_model/#sageworks.core.transforms.features_to_model.features_to_model.FeaturesToModel.__init__","title":"<code>__init__(feature_uuid, model_uuid, model_type)</code>","text":"<p>FeaturesToModel Initialization Args:     feature_uuid (str): UUID of the FeatureSet to use as input     model_uuid (str): UUID of the Model to create as output     model_type (ModelType): ModelType.REGRESSOR or ModelType.CLASSIFIER</p> Source code in <code>src/sageworks/core/transforms/features_to_model/features_to_model.py</code> <pre><code>def __init__(self, feature_uuid: str, model_uuid: str, model_type: ModelType):\n    \"\"\"FeaturesToModel Initialization\n    Args:\n        feature_uuid (str): UUID of the FeatureSet to use as input\n        model_uuid (str): UUID of the Model to create as output\n        model_type (ModelType): ModelType.REGRESSOR or ModelType.CLASSIFIER\n    \"\"\"\n\n    # Call superclass init\n    super().__init__(feature_uuid, model_uuid)\n\n    # Set up all my instance attributes\n    self.input_type = TransformInput.FEATURE_SET\n    self.output_type = TransformOutput.MODEL\n    self.model_type = model_type\n    self.estimator = None\n    self.model_script_dir = None\n    self.model_description = None\n    self.model_training_root = self.models_s3_path + \"/training\"\n    self.model_feature_list = None\n    self.target_column = None\n</code></pre>"},{"location":"core_classes/transforms/features_to_model/#sageworks.core.transforms.features_to_model.features_to_model.FeaturesToModel.create_and_register_model","title":"<code>create_and_register_model()</code>","text":"<p>Create and Register the Model</p> Source code in <code>src/sageworks/core/transforms/features_to_model/features_to_model.py</code> <pre><code>def create_and_register_model(self):\n    \"\"\"Create and Register the Model\"\"\"\n\n    # Get the metadata/tags to push into AWS\n    aws_tags = self.get_aws_tags()\n\n    # Create model group (if it doesn't already exist)\n    self.sm_client.create_model_package_group(\n        ModelPackageGroupName=self.output_uuid,\n        ModelPackageGroupDescription=self.model_description,\n        Tags=aws_tags,\n    )\n\n    # Register our model\n    model = self.estimator.create_model(role=self.sageworks_role_arn)\n    model.register(\n        model_package_group_name=self.output_uuid,\n        framework_version=\"1.2.1\",\n        content_types=[\"text/csv\"],\n        response_types=[\"text/csv\"],\n        inference_instances=[\"ml.t2.medium\"],\n        transform_instances=[\"ml.m5.large\"],\n        approval_status=\"Approved\",\n        description=self.model_description,\n    )\n</code></pre>"},{"location":"core_classes/transforms/features_to_model/#sageworks.core.transforms.features_to_model.features_to_model.FeaturesToModel.generate_model_script","title":"<code>generate_model_script(target_column, feature_list, model_type, train_all_data)</code>","text":"<p>Fill in the model template with specific target and feature_list Args:     target_column (str): Column name of the target variable     feature_list (list[str]): A list of columns for the features     model_type (ModelType): ModelType.REGRESSOR or ModelType.CLASSIFIER     train_all_data (bool): Train on ALL (100%) of the data Returns:    str: The name of the generated model script</p> Source code in <code>src/sageworks/core/transforms/features_to_model/features_to_model.py</code> <pre><code>def generate_model_script(\n    self, target_column: str, feature_list: list[str], model_type: ModelType, train_all_data: bool\n) -&gt; str:\n    \"\"\"Fill in the model template with specific target and feature_list\n    Args:\n        target_column (str): Column name of the target variable\n        feature_list (list[str]): A list of columns for the features\n        model_type (ModelType): ModelType.REGRESSOR or ModelType.CLASSIFIER\n        train_all_data (bool): Train on ALL (100%) of the data\n    Returns:\n       str: The name of the generated model script\n    \"\"\"\n\n    # FIXME: Revisit all of this since it's a bit wonky\n    script_name = \"generated_xgb_model.py\"\n    dir_path = Path(__file__).parent.absolute()\n    self.model_script_dir = os.path.join(dir_path, \"light_model_harness\")\n    template_path = os.path.join(self.model_script_dir, \"xgb_model.template\")\n    output_path = os.path.join(self.model_script_dir, script_name)\n    with open(template_path, \"r\") as fp:\n        xgb_template = fp.read()\n\n    # Template replacements\n    xgb_script = xgb_template.replace(\"{{target_column}}\", target_column)\n    feature_list_str = json.dumps(feature_list)\n    xgb_script = xgb_script.replace(\"{{feature_list}}\", feature_list_str)\n    xgb_script = xgb_script.replace(\"{{model_type}}\", model_type)\n    metrics_s3_path = f\"{self.model_training_root}/{self.output_uuid}\"\n    xgb_script = xgb_script.replace(\"{{model_metrics_s3_path}}\", metrics_s3_path)\n    xgb_script = xgb_script.replace(\"{{train_all_data}}\", str(train_all_data))\n\n    # Now write out the generated model script and return the name\n    with open(output_path, \"w\") as fp:\n        fp.write(xgb_script)\n    return script_name\n</code></pre>"},{"location":"core_classes/transforms/features_to_model/#sageworks.core.transforms.features_to_model.features_to_model.FeaturesToModel.post_transform","title":"<code>post_transform(**kwargs)</code>","text":"<p>Post-Transform: Calling make_ready() on the Model</p> Source code in <code>src/sageworks/core/transforms/features_to_model/features_to_model.py</code> <pre><code>def post_transform(self, **kwargs):\n    \"\"\"Post-Transform: Calling make_ready() on the Model\"\"\"\n    self.log.info(\"Post-Transform: Calling onboard() on the Model...\")\n\n    # Okay, lets get our output model and set it to initializing\n    output_model = ModelCore(self.output_uuid, model_type=self.model_type, force_refresh=True)\n    output_model.set_status(\"initializing\")\n\n    # Store the model feature_list and target_column in the sageworks_meta\n    output_model.upsert_sageworks_meta({\"sageworks_model_features\": self.model_feature_list})\n    output_model.upsert_sageworks_meta({\"sageworks_model_target\": self.target_column})\n\n    # Call the Model onboard method\n    output_model.onboard()\n</code></pre>"},{"location":"core_classes/transforms/features_to_model/#sageworks.core.transforms.features_to_model.features_to_model.FeaturesToModel.transform_impl","title":"<code>transform_impl(target_column, description=None, feature_list=None, train_all_data=False)</code>","text":"<p>Generic Features to Model: Note you should create a new class and inherit from this one to include specific logic for your Feature Set/Model Args:     target_column (str): Column name of the target variable     description (str): Description of the model (optional)     feature_list (list[str]): A list of columns for the features (default None, will try to guess)     train_all_data (bool): Train on ALL (100%) of the data (default False)</p> Source code in <code>src/sageworks/core/transforms/features_to_model/features_to_model.py</code> <pre><code>def transform_impl(\n    self, target_column: str, description: str = None, feature_list: list = None, train_all_data=False\n):\n    \"\"\"Generic Features to Model: Note you should create a new class and inherit from\n    this one to include specific logic for your Feature Set/Model\n    Args:\n        target_column (str): Column name of the target variable\n        description (str): Description of the model (optional)\n        feature_list (list[str]): A list of columns for the features (default None, will try to guess)\n        train_all_data (bool): Train on ALL (100%) of the data (default False)\n    \"\"\"\n    # Delete the existing model (if it exists)\n    self.log.important(\"Trying to delete existing model...\")\n    delete_model = ModelCore(self.output_uuid, force_refresh=True)\n    delete_model.delete()\n\n    # Set our model description\n    self.model_description = description if description is not None else f\"Model created from {self.input_uuid}\"\n\n    # Get our Feature Set and create an S3 CSV Training dataset\n    feature_set = FeatureSetCore(self.input_uuid)\n    s3_training_path = feature_set.create_s3_training_data()\n    self.log.info(f\"Created new training data {s3_training_path}...\")\n\n    # Report the target column\n    self.target_column = target_column\n    self.log.info(f\"Target column: {self.target_column}\")\n\n    # Did they specify a feature list?\n    if feature_list:\n        # AWS Feature Groups will also add these implicit columns, so remove them\n        aws_cols = [\"write_time\", \"api_invocation_time\", \"is_deleted\", \"event_time\", \"training\"]\n        feature_list = [c for c in feature_list if c not in aws_cols]\n\n    # If they didn't specify a feature list, try to guess it\n    else:\n        # Try to figure out features with this logic\n        # - Don't include id, event_time, __index_level_0__, or training columns\n        # - Don't include AWS generated columns (e.g. write_time, api_invocation_time, is_deleted)\n        # - Don't include the target columns\n        # - Don't include any columns that are of type string or timestamp\n        # - The rest of the columns are assumed to be features\n        self.log.warning(\"Guessing at the feature list, HIGHLY SUGGESTED to specify an explicit feature list!\")\n        all_columns = feature_set.column_names()\n        filter_list = [\n            \"id\",\n            \"__index_level_0__\",\n            \"write_time\",\n            \"api_invocation_time\",\n            \"is_deleted\",\n            \"event_time\",\n            \"training\",\n        ] + [self.target_column]\n        feature_list = [c for c in all_columns if c not in filter_list]\n\n    # AWS Feature Store has 3 user column types (String, Integral, Fractional)\n    # and two internal types (Timestamp and Boolean). A Feature List for\n    # modeling can only contain Integral and Fractional types.\n    remove_columns = []\n    column_details = feature_set.column_details()\n    for column_name in feature_list:\n        if column_details[column_name] not in [\"Integral\", \"Fractional\"]:\n            self.log.warning(\n                f\"Removing {column_name} from feature list, improper type {column_details[column_name]}\"\n            )\n            remove_columns.append(column_name)\n\n    # Remove the columns that are not Integral or Fractional\n    self.model_feature_list = [c for c in feature_list if c not in remove_columns]\n    self.log.important(f\"Feature List for Modeling: {self.model_feature_list}\")\n\n    # Generate our model script\n    script_path = self.generate_model_script(self.target_column, self.model_feature_list, self.model_type.value, train_all_data)\n\n    # Metric Definitions for Regression and Classification\n    if self.model_type == ModelType.REGRESSOR:\n        metric_definitions = [\n            {\"Name\": \"RMSE\", \"Regex\": \"RMSE: ([0-9.]+)\"},\n            {\"Name\": \"MAE\", \"Regex\": \"MAE: ([0-9.]+)\"},\n            {\"Name\": \"R2\", \"Regex\": \"R2 Score: ([0-9.]+)\"},\n        ]\n    else:\n        # We need to get creative with the Classification Metrics\n        # Grab all the target column class values\n        table = feature_set.data_source.get_table_name()\n        targets = feature_set.query(f\"select DISTINCT {self.target_column} FROM {table}\")[self.target_column].to_list()\n        metrics = [\"precision\", \"recall\", \"fscore\"]\n\n        # Dynamically create the metric definitions\n        metric_definitions = []\n        for t in targets:\n            for m in metrics:\n                metric_definitions.append({\"Name\": f\"Metrics:{t}:{m}\", \"Regex\": f\"Metrics:{t}:{m} ([0-9.]+)\"})\n\n        # Add the confusion matrix metrics\n        for row in targets:\n            for col in targets:\n                metric_definitions.append(\n                    {\"Name\": f\"ConfusionMatrix:{row}:{col}\", \"Regex\": f\"ConfusionMatrix:{row}:{col} ([0-9.]+)\"}\n                )\n\n    # Create a Sagemaker Model with our script\n    self.estimator = SKLearn(\n        entry_point=script_path,\n        source_dir=self.model_script_dir,\n        role=self.sageworks_role_arn,\n        instance_type=\"ml.m5.large\",\n        sagemaker_session=self.sm_session,\n        framework_version=\"1.2-1\",\n        metric_definitions=metric_definitions,\n    )\n\n    # Train the estimator\n    self.estimator.fit({\"train\": s3_training_path})\n\n    # Now delete the training data\n    self.log.info(f\"Deleting training data {s3_training_path}...\")\n    wr.s3.delete_objects(\n        [s3_training_path, s3_training_path.replace(\".csv\", \".csv.metadata\")],\n        boto3_session=self.boto_session,\n    )\n\n    # Create Model and officially Register\n    self.log.important(f\"Creating new model {self.output_uuid}...\")\n    self.create_and_register_model()\n</code></pre>"},{"location":"core_classes/transforms/model_to_endpoint/","title":"Model to Endpoint","text":"<p>API Classes</p> <p>For most users the API Classes will provide all the general functionality to create a full AWS ML Pipeline</p> <p>ModelToEndpoint: Deploy an Endpoint for a Model</p>"},{"location":"core_classes/transforms/model_to_endpoint/#sageworks.core.transforms.model_to_endpoint.model_to_endpoint.ModelToEndpoint","title":"<code>ModelToEndpoint</code>","text":"<p>             Bases: <code>Transform</code></p> <p>ModelToEndpoint: Deploy an Endpoint for a Model</p> Common Usage <pre><code>to_endpoint = ModelToEndpoint(model_uuid, endpoint_uuid)\nto_endpoint.set_output_tags([\"aqsol\", \"public\", \"whatever\"])\nto_endpoint.transform()\n</code></pre> Source code in <code>src/sageworks/core/transforms/model_to_endpoint/model_to_endpoint.py</code> <pre><code>class ModelToEndpoint(Transform):\n    \"\"\"ModelToEndpoint: Deploy an Endpoint for a Model\n\n    Common Usage:\n        ```\n        to_endpoint = ModelToEndpoint(model_uuid, endpoint_uuid)\n        to_endpoint.set_output_tags([\"aqsol\", \"public\", \"whatever\"])\n        to_endpoint.transform()\n        ```\n    \"\"\"\n\n    def __init__(self, model_uuid: str, endpoint_uuid: str, serverless: bool = True):\n        \"\"\"ModelToEndpoint Initialization\n        Args:\n            model_uuid(str): The UUID of the input Model\n            endpoint_uuid(str): The UUID of the output Endpoint\n            serverless(bool): Deploy the Endpoint in serverless mode (default: True)\n        \"\"\"\n\n        # Call superclass init\n        super().__init__(model_uuid, endpoint_uuid)\n\n        # Set up all my instance attributes\n        self.instance_type = \"serverless\" if serverless else \"ml.t2.medium\"\n        self.input_type = TransformInput.MODEL\n        self.output_type = TransformOutput.ENDPOINT\n\n    def transform_impl(self):\n        \"\"\"Compute a Feature Set based on RDKit Descriptors\"\"\"\n\n        # Delete endpoint (if it already exists)\n        existing_endpoint = EndpointCore(self.output_uuid)\n        if existing_endpoint.exists():\n            existing_endpoint.delete()\n\n        # Get the Model Package ARN for our input model\n        model_package_arn = ModelCore(self.input_uuid).model_package_arn()\n\n        # Will this be a Serverless Endpoint?\n        if self.instance_type == \"serverless\":\n            self._serverless_deploy(model_package_arn)\n            return\n\n        # Create a Model Package\n        model_package = ModelPackage(role=self.sageworks_role_arn, model_package_arn=model_package_arn)\n\n        # Get the metadata/tags to push into AWS\n        aws_tags = self.get_aws_tags()\n\n        # Deploy an Endpoint\n        model_package.deploy(\n            initial_instance_count=1,\n            instance_type=self.instance_type,\n            endpoint_name=self.output_uuid,\n            serializer=CSVSerializer(),\n            deserializer=CSVDeserializer(),\n            tags=aws_tags,\n        )\n\n    def _serverless_deploy(self, model_package_arn, mem_size=2048, max_concurrency=5, wait=True):\n        \"\"\"Internal Method: Deploy the Endpoint in serverless mode\n        Args:\n            mem_size(int): Memory size in MB (default: 2048)\n            max_concurrency(int): Max concurrency (default: 5)\n            wait(bool): Wait for the Endpoint to be ready (default: True)\n        \"\"\"\n        model_name = self.input_uuid\n        endpoint_name = self.output_uuid\n        aws_tags = self.get_aws_tags()\n\n        # Create Low Level Model Resource (Endpoint Config below references this Model Resource)\n        # Note: Since model is internal to the endpoint we'll add a timestamp (just like SageMaker does)\n        datetime_str = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S-%f\")[:-3]\n        model_name = f\"{model_name}-{datetime_str}\"\n        self.log.info(f\"Creating Low Level Model: {model_name}...\")\n        self.sm_client.create_model(\n            ModelName=model_name,\n            PrimaryContainer={\n                \"ModelPackageName\": model_package_arn,\n            },\n            ExecutionRoleArn=self.sageworks_role_arn,\n            Tags=aws_tags,\n        )\n\n        # Create Endpoint Config\n        self.log.info(f\"Creating Endpoint Config {endpoint_name}...\")\n        self.sm_client.create_endpoint_config(\n            EndpointConfigName=endpoint_name,\n            ProductionVariants=[\n                {\n                    \"ServerlessConfig\": {\"MemorySizeInMB\": mem_size, \"MaxConcurrency\": max_concurrency},\n                    \"ModelName\": model_name,\n                    \"VariantName\": \"AllTraffic\",\n                }\n            ],\n        )\n\n        # Create Endpoint\n        self.log.info(f\"Creating Serverless Endpoint {endpoint_name}...\")\n        self.sm_client.create_endpoint(\n            EndpointName=endpoint_name, EndpointConfigName=endpoint_name, Tags=self.get_aws_tags()\n        )\n\n        # Wait for Endpoint to be ready\n        if not wait:\n            self.log.important(f\"Endpoint {endpoint_name} is being created...\")\n        else:\n            self.log.important(f\"Waiting for Endpoint {endpoint_name} to be ready...\")\n            describe_endpoint_response = self.sm_client.describe_endpoint(EndpointName=endpoint_name)\n            while describe_endpoint_response[\"EndpointStatus\"] == \"Creating\":\n                time.sleep(30)\n                describe_endpoint_response = self.sm_client.describe_endpoint(EndpointName=endpoint_name)\n                self.log.info(describe_endpoint_response[\"EndpointStatus\"])\n            status = describe_endpoint_response[\"EndpointStatus\"]\n            self.log.important(f\"Endpoint {endpoint_name} is now {status}...\")\n\n    def post_transform(self, **kwargs):\n        \"\"\"Post-Transform: Calling make_ready() on the Model\"\"\"\n        self.log.info(\"Post-Transform: Calling make_ready() on the Endpoint...\")\n\n        # Okay, lets get our output model and set it to initializing\n        output_endpoint = EndpointCore(self.output_uuid, force_refresh=True)\n        output_endpoint.set_status(\"initializing\")\n\n        # Call the Model make_ready method and set status to ready\n        output_endpoint.make_ready()\n</code></pre>"},{"location":"core_classes/transforms/model_to_endpoint/#sageworks.core.transforms.model_to_endpoint.model_to_endpoint.ModelToEndpoint.__init__","title":"<code>__init__(model_uuid, endpoint_uuid, serverless=True)</code>","text":"<p>ModelToEndpoint Initialization Args:     model_uuid(str): The UUID of the input Model     endpoint_uuid(str): The UUID of the output Endpoint     serverless(bool): Deploy the Endpoint in serverless mode (default: True)</p> Source code in <code>src/sageworks/core/transforms/model_to_endpoint/model_to_endpoint.py</code> <pre><code>def __init__(self, model_uuid: str, endpoint_uuid: str, serverless: bool = True):\n    \"\"\"ModelToEndpoint Initialization\n    Args:\n        model_uuid(str): The UUID of the input Model\n        endpoint_uuid(str): The UUID of the output Endpoint\n        serverless(bool): Deploy the Endpoint in serverless mode (default: True)\n    \"\"\"\n\n    # Call superclass init\n    super().__init__(model_uuid, endpoint_uuid)\n\n    # Set up all my instance attributes\n    self.instance_type = \"serverless\" if serverless else \"ml.t2.medium\"\n    self.input_type = TransformInput.MODEL\n    self.output_type = TransformOutput.ENDPOINT\n</code></pre>"},{"location":"core_classes/transforms/model_to_endpoint/#sageworks.core.transforms.model_to_endpoint.model_to_endpoint.ModelToEndpoint.post_transform","title":"<code>post_transform(**kwargs)</code>","text":"<p>Post-Transform: Calling make_ready() on the Model</p> Source code in <code>src/sageworks/core/transforms/model_to_endpoint/model_to_endpoint.py</code> <pre><code>def post_transform(self, **kwargs):\n    \"\"\"Post-Transform: Calling make_ready() on the Model\"\"\"\n    self.log.info(\"Post-Transform: Calling make_ready() on the Endpoint...\")\n\n    # Okay, lets get our output model and set it to initializing\n    output_endpoint = EndpointCore(self.output_uuid, force_refresh=True)\n    output_endpoint.set_status(\"initializing\")\n\n    # Call the Model make_ready method and set status to ready\n    output_endpoint.make_ready()\n</code></pre>"},{"location":"core_classes/transforms/model_to_endpoint/#sageworks.core.transforms.model_to_endpoint.model_to_endpoint.ModelToEndpoint.transform_impl","title":"<code>transform_impl()</code>","text":"<p>Compute a Feature Set based on RDKit Descriptors</p> Source code in <code>src/sageworks/core/transforms/model_to_endpoint/model_to_endpoint.py</code> <pre><code>def transform_impl(self):\n    \"\"\"Compute a Feature Set based on RDKit Descriptors\"\"\"\n\n    # Delete endpoint (if it already exists)\n    existing_endpoint = EndpointCore(self.output_uuid)\n    if existing_endpoint.exists():\n        existing_endpoint.delete()\n\n    # Get the Model Package ARN for our input model\n    model_package_arn = ModelCore(self.input_uuid).model_package_arn()\n\n    # Will this be a Serverless Endpoint?\n    if self.instance_type == \"serverless\":\n        self._serverless_deploy(model_package_arn)\n        return\n\n    # Create a Model Package\n    model_package = ModelPackage(role=self.sageworks_role_arn, model_package_arn=model_package_arn)\n\n    # Get the metadata/tags to push into AWS\n    aws_tags = self.get_aws_tags()\n\n    # Deploy an Endpoint\n    model_package.deploy(\n        initial_instance_count=1,\n        instance_type=self.instance_type,\n        endpoint_name=self.output_uuid,\n        serializer=CSVSerializer(),\n        deserializer=CSVDeserializer(),\n        tags=aws_tags,\n    )\n</code></pre>"},{"location":"core_classes/transforms/overview/","title":"Transforms","text":"<p>API Classes</p> <p>For most users the API Classes will provide all the general functionality to create a full AWS ML Pipeline</p> <p>SageWorks currently has a large set of Transforms that go from one Artifact type to another (e.g. DataSource to FeatureSet). The Transforms will often have light and heavy versions depending on the scale of data that needs to be transformed.</p>"},{"location":"core_classes/transforms/overview/#transform-details","title":"Transform Details","text":"<ul> <li>DataLoaders Light: Loads various light/smaller data into AWS Data Catalog and Athena</li> <li>DataLoaders Heavy: Loads heavy/larger data (via Glue) into AWS Data Catalog and Athena</li> <li>DataToFeatures: Transforms a DataSource into a FeatureSet (AWS Feature Store/Group)</li> <li>FeaturesToModel: Trains and deploys an AWS Model Package/Group from a FeatureSet</li> <li>ModelToEndpoint: Manages the provisioning and deployment of a Model Endpoint</li> <li>PandasTransforms: Pandas DataFrame transforms and helper methods.</li> </ul>"},{"location":"core_classes/transforms/pandas_transforms/","title":"Pandas Transforms","text":"<p>Welcome to the SageWorks Pandas Transform Classes</p> <p>These classes provide low-level APIs for using Pandas DataFrames</p> <ul> <li>DataToPandas: Pull a dataframe from a SageWorks DataSource</li> <li>FeaturesToPandas: Pull a dataframe from a SageWorks FeatureSet</li> <li>PandasToData: Create a SageWorks DataSource using a Pandas DataFrame as the source</li> <li>PandasToFeatures: Create a SageWorks FeatureSet using a Pandas DataFrame as the source</li> <li>PandasToFeaturesChunked: Create a SageWorks FeatureSet using a Chunked/Streaming Pandas DataFrame as the source</li> </ul>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.DataToPandas","title":"<code>DataToPandas</code>","text":"<p>             Bases: <code>Transform</code></p> <p>DataToPandas: Class to transform a Data Source into a Pandas DataFrame</p> Common Usage <pre><code>data_to_df = DataToPandas(data_source_uuid)\ndata_to_df.transform(query=&lt;optional SQL query to filter/process data&gt;)\ndata_to_df.transform(max_rows=&lt;optional max rows to sample&gt;)\nmy_df = data_to_df.get_output()\n\nNote: query is the best way to use this class, so use it :)\n</code></pre> Source code in <code>src/sageworks/core/transforms/pandas_transforms/data_to_pandas.py</code> <pre><code>class DataToPandas(Transform):\n    \"\"\"DataToPandas: Class to transform a Data Source into a Pandas DataFrame\n\n    Common Usage:\n        ```\n        data_to_df = DataToPandas(data_source_uuid)\n        data_to_df.transform(query=&lt;optional SQL query to filter/process data&gt;)\n        data_to_df.transform(max_rows=&lt;optional max rows to sample&gt;)\n        my_df = data_to_df.get_output()\n\n        Note: query is the best way to use this class, so use it :)\n        ```\n    \"\"\"\n\n    def __init__(self, input_uuid: str):\n        \"\"\"DataToPandas Initialization\"\"\"\n\n        # Call superclass init\n        super().__init__(input_uuid, \"DataFrame\")\n\n        # Set up all my instance attributes\n        self.input_type = TransformInput.DATA_SOURCE\n        self.output_type = TransformOutput.PANDAS_DF\n        self.output_df = None\n\n    def transform_impl(self, query: str = None, max_rows=100000):\n        \"\"\"Convert the DataSource into a Pandas DataFrame\n        Args:\n            query(str): The query to run against the DataSource (default: None)\n            max_rows(int): The maximum number of rows to return (default: 100000)\n        \"\"\"\n\n        # Grab the Input (Data Source)\n        input_data = DataSourceFactory(self.input_uuid)\n        if not input_data.exists():\n            self.log.critical(f\"Data Check on {self.input_uuid} failed!\")\n            return\n\n        # If a query is provided, that overrides the queries below\n        if query:\n            self.log.info(f\"Querying {self.input_uuid} with {query}...\")\n            self.output_df = input_data.query(query)\n            return\n\n        # If the data source has more rows than max_rows, do a sample query\n        num_rows = input_data.num_rows()\n        if num_rows &gt; max_rows:\n            percentage = round(max_rows * 100.0 / num_rows)\n            self.log.important(f\"DataSource has {num_rows} rows.. sampling down to {max_rows}...\")\n            query = f\"SELECT * FROM {self.input_uuid} TABLESAMPLE BERNOULLI({percentage})\"\n        else:\n            query = f\"SELECT * FROM {self.input_uuid}\"\n\n        # Mark the transform as complete and set the output DataFrame\n        self.output_df = input_data.query(query)\n\n    def post_transform(self, **kwargs):\n        \"\"\"Post-Transform: Any checks on the Pandas DataFrame that need to be done\"\"\"\n        self.log.info(\"Post-Transform: Checking Pandas DataFrame...\")\n        self.log.info(f\"DataFrame Shape: {self.output_df.shape}\")\n\n    def get_output(self) -&gt; pd.DataFrame:\n        \"\"\"Get the DataFrame Output from this Transform\"\"\"\n        return self.output_df\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.DataToPandas.__init__","title":"<code>__init__(input_uuid)</code>","text":"<p>DataToPandas Initialization</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/data_to_pandas.py</code> <pre><code>def __init__(self, input_uuid: str):\n    \"\"\"DataToPandas Initialization\"\"\"\n\n    # Call superclass init\n    super().__init__(input_uuid, \"DataFrame\")\n\n    # Set up all my instance attributes\n    self.input_type = TransformInput.DATA_SOURCE\n    self.output_type = TransformOutput.PANDAS_DF\n    self.output_df = None\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.DataToPandas.get_output","title":"<code>get_output()</code>","text":"<p>Get the DataFrame Output from this Transform</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/data_to_pandas.py</code> <pre><code>def get_output(self) -&gt; pd.DataFrame:\n    \"\"\"Get the DataFrame Output from this Transform\"\"\"\n    return self.output_df\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.DataToPandas.post_transform","title":"<code>post_transform(**kwargs)</code>","text":"<p>Post-Transform: Any checks on the Pandas DataFrame that need to be done</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/data_to_pandas.py</code> <pre><code>def post_transform(self, **kwargs):\n    \"\"\"Post-Transform: Any checks on the Pandas DataFrame that need to be done\"\"\"\n    self.log.info(\"Post-Transform: Checking Pandas DataFrame...\")\n    self.log.info(f\"DataFrame Shape: {self.output_df.shape}\")\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.DataToPandas.transform_impl","title":"<code>transform_impl(query=None, max_rows=100000)</code>","text":"<p>Convert the DataSource into a Pandas DataFrame Args:     query(str): The query to run against the DataSource (default: None)     max_rows(int): The maximum number of rows to return (default: 100000)</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/data_to_pandas.py</code> <pre><code>def transform_impl(self, query: str = None, max_rows=100000):\n    \"\"\"Convert the DataSource into a Pandas DataFrame\n    Args:\n        query(str): The query to run against the DataSource (default: None)\n        max_rows(int): The maximum number of rows to return (default: 100000)\n    \"\"\"\n\n    # Grab the Input (Data Source)\n    input_data = DataSourceFactory(self.input_uuid)\n    if not input_data.exists():\n        self.log.critical(f\"Data Check on {self.input_uuid} failed!\")\n        return\n\n    # If a query is provided, that overrides the queries below\n    if query:\n        self.log.info(f\"Querying {self.input_uuid} with {query}...\")\n        self.output_df = input_data.query(query)\n        return\n\n    # If the data source has more rows than max_rows, do a sample query\n    num_rows = input_data.num_rows()\n    if num_rows &gt; max_rows:\n        percentage = round(max_rows * 100.0 / num_rows)\n        self.log.important(f\"DataSource has {num_rows} rows.. sampling down to {max_rows}...\")\n        query = f\"SELECT * FROM {self.input_uuid} TABLESAMPLE BERNOULLI({percentage})\"\n    else:\n        query = f\"SELECT * FROM {self.input_uuid}\"\n\n    # Mark the transform as complete and set the output DataFrame\n    self.output_df = input_data.query(query)\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.FeaturesToPandas","title":"<code>FeaturesToPandas</code>","text":"<p>             Bases: <code>Transform</code></p> <p>FeaturesToPandas: Class to transform a FeatureSet into a Pandas DataFrame</p> Common Usage <pre><code>feature_to_df = FeaturesToPandas(feature_set_uuid)\nfeature_to_df.transform(max_rows=&lt;optional max rows to sample&gt;)\nmy_df = feature_to_df.get_output()\n</code></pre> Source code in <code>src/sageworks/core/transforms/pandas_transforms/features_to_pandas.py</code> <pre><code>class FeaturesToPandas(Transform):\n    \"\"\"FeaturesToPandas: Class to transform a FeatureSet into a Pandas DataFrame\n\n    Common Usage:\n        ```\n        feature_to_df = FeaturesToPandas(feature_set_uuid)\n        feature_to_df.transform(max_rows=&lt;optional max rows to sample&gt;)\n        my_df = feature_to_df.get_output()\n        ```\n    \"\"\"\n\n    def __init__(self, feature_set_name: str):\n        \"\"\"FeaturesToPandas Initialization\"\"\"\n\n        # Call superclass init\n        super().__init__(input_uuid=feature_set_name, output_uuid=\"DataFrame\")\n\n        # Set up all my instance attributes\n        self.input_type = TransformInput.FEATURE_SET\n        self.output_type = TransformOutput.PANDAS_DF\n        self.output_df = None\n        self.transform_run = False\n\n    def transform_impl(self, max_rows=100000):\n        \"\"\"Convert the FeatureSet into a Pandas DataFrame\"\"\"\n\n        # Grab the Input (Feature Set)\n        input_data = FeatureSetCore(self.input_uuid)\n        if not input_data.exists():\n            self.log.critical(f\"Feature Set Check on {self.input_uuid} failed!\")\n            return\n\n        # Grab the table for this Feature Set\n        table = input_data.athena_table\n\n        # Get the list of columns (and subtract metadata columns that might get added)\n        columns = input_data.column_names()\n        filter_columns = [\"write_time\", \"api_invocation_time\", \"is_deleted\"]\n        columns = \", \".join([x for x in columns if x not in filter_columns])\n\n        # Get the number of rows in the Feature Set\n        num_rows = input_data.num_rows()\n\n        # If the data source has more rows than max_rows, do a sample query\n        if num_rows &gt; max_rows:\n            percentage = round(max_rows * 100.0 / num_rows)\n            self.log.important(f\"DataSource has {num_rows} rows.. sampling down to {max_rows}...\")\n            query = f'SELECT {columns} FROM \"{table}\" TABLESAMPLE BERNOULLI({percentage})'\n        else:\n            query = f'SELECT {columns} FROM \"{table}\"'\n\n        # Mark the transform as complete and set the output DataFrame\n        self.transform_run = True\n        self.output_df = input_data.query(query)\n\n    def post_transform(self, **kwargs):\n        \"\"\"Post-Transform: Any checks on the Pandas DataFrame that need to be done\"\"\"\n        self.log.info(\"Post-Transform: Checking Pandas DataFrame...\")\n        self.log.info(f\"DataFrame Shape: {self.output_df.shape}\")\n\n    def get_output(self) -&gt; pd.DataFrame:\n        \"\"\"Get the DataFrame Output from this Transform\"\"\"\n        if not self.transform_run:\n            self.transform()\n        return self.output_df\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.FeaturesToPandas.__init__","title":"<code>__init__(feature_set_name)</code>","text":"<p>FeaturesToPandas Initialization</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/features_to_pandas.py</code> <pre><code>def __init__(self, feature_set_name: str):\n    \"\"\"FeaturesToPandas Initialization\"\"\"\n\n    # Call superclass init\n    super().__init__(input_uuid=feature_set_name, output_uuid=\"DataFrame\")\n\n    # Set up all my instance attributes\n    self.input_type = TransformInput.FEATURE_SET\n    self.output_type = TransformOutput.PANDAS_DF\n    self.output_df = None\n    self.transform_run = False\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.FeaturesToPandas.get_output","title":"<code>get_output()</code>","text":"<p>Get the DataFrame Output from this Transform</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/features_to_pandas.py</code> <pre><code>def get_output(self) -&gt; pd.DataFrame:\n    \"\"\"Get the DataFrame Output from this Transform\"\"\"\n    if not self.transform_run:\n        self.transform()\n    return self.output_df\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.FeaturesToPandas.post_transform","title":"<code>post_transform(**kwargs)</code>","text":"<p>Post-Transform: Any checks on the Pandas DataFrame that need to be done</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/features_to_pandas.py</code> <pre><code>def post_transform(self, **kwargs):\n    \"\"\"Post-Transform: Any checks on the Pandas DataFrame that need to be done\"\"\"\n    self.log.info(\"Post-Transform: Checking Pandas DataFrame...\")\n    self.log.info(f\"DataFrame Shape: {self.output_df.shape}\")\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.FeaturesToPandas.transform_impl","title":"<code>transform_impl(max_rows=100000)</code>","text":"<p>Convert the FeatureSet into a Pandas DataFrame</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/features_to_pandas.py</code> <pre><code>def transform_impl(self, max_rows=100000):\n    \"\"\"Convert the FeatureSet into a Pandas DataFrame\"\"\"\n\n    # Grab the Input (Feature Set)\n    input_data = FeatureSetCore(self.input_uuid)\n    if not input_data.exists():\n        self.log.critical(f\"Feature Set Check on {self.input_uuid} failed!\")\n        return\n\n    # Grab the table for this Feature Set\n    table = input_data.athena_table\n\n    # Get the list of columns (and subtract metadata columns that might get added)\n    columns = input_data.column_names()\n    filter_columns = [\"write_time\", \"api_invocation_time\", \"is_deleted\"]\n    columns = \", \".join([x for x in columns if x not in filter_columns])\n\n    # Get the number of rows in the Feature Set\n    num_rows = input_data.num_rows()\n\n    # If the data source has more rows than max_rows, do a sample query\n    if num_rows &gt; max_rows:\n        percentage = round(max_rows * 100.0 / num_rows)\n        self.log.important(f\"DataSource has {num_rows} rows.. sampling down to {max_rows}...\")\n        query = f'SELECT {columns} FROM \"{table}\" TABLESAMPLE BERNOULLI({percentage})'\n    else:\n        query = f'SELECT {columns} FROM \"{table}\"'\n\n    # Mark the transform as complete and set the output DataFrame\n    self.transform_run = True\n    self.output_df = input_data.query(query)\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToData","title":"<code>PandasToData</code>","text":"<p>             Bases: <code>Transform</code></p> <p>PandasToData: Class to publish a Pandas DataFrame as a DataSource</p> Common Usage <pre><code>df_to_data = PandasToData(output_uuid)\ndf_to_data.set_output_tags([\"test\", \"small\"])\ndf_to_data.set_input(test_df)\ndf_to_data.transform()\n</code></pre> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_data.py</code> <pre><code>class PandasToData(Transform):\n    \"\"\"PandasToData: Class to publish a Pandas DataFrame as a DataSource\n\n    Common Usage:\n        ```\n        df_to_data = PandasToData(output_uuid)\n        df_to_data.set_output_tags([\"test\", \"small\"])\n        df_to_data.set_input(test_df)\n        df_to_data.transform()\n        ```\n    \"\"\"\n\n    def __init__(self, output_uuid: str, output_format: str = \"parquet\"):\n        \"\"\"PandasToData Initialization\n        Args:\n            output_uuid (str): The UUID of the DataSource to create\n            output_format (str): The file format to store the S3 object data in (default: \"parquet\")\n        \"\"\"\n\n        # Call superclass init\n        super().__init__(\"DataFrame\", output_uuid)\n\n        # Set up all my instance attributes\n        self.input_type = TransformInput.PANDAS_DF\n        self.output_type = TransformOutput.DATA_SOURCE\n        self.output_df = None\n\n        # Give a message that Parquet is best in most cases\n        if output_format != \"parquet\":\n            self.log.warning(\"Parquet format works the best in most cases please consider using it\")\n        self.output_format = output_format\n\n    def set_input(self, input_df: pd.DataFrame):\n        \"\"\"Set the DataFrame Input for this Transform\"\"\"\n        self.output_df = input_df.copy()\n\n    def convert_object_to_string(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Try to automatically convert object columns to string columns\"\"\"\n        for c in df.columns[df.dtypes == \"object\"]:  # Look at the object columns\n            try:\n                df[c] = df[c].astype(\"string\")\n                df[c] = df[c].str.replace(\"'\", '\"')  # This is for nested JSON\n            except (ParserError, ValueError, TypeError):\n                self.log.info(f\"Column {c} could not be converted to string...\")\n        return df\n\n    def convert_object_to_datetime(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Try to automatically convert object columns to datetime or string columns\"\"\"\n        for c in df.columns[df.dtypes == \"object\"]:  # Look at the object columns\n            try:\n                df[c] = pd.to_datetime(df[c])\n            except (ParserError, ValueError, TypeError):\n                self.log.debug(f\"Column {c} could not be converted to datetime...\")\n        return df\n\n    @staticmethod\n    def convert_datetime_columns(df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Convert datetime columns to ISO-8601 string\"\"\"\n        datetime_type = [\"datetime\", \"datetime64\", \"datetime64[ns]\", \"datetimetz\"]\n        for c in df.select_dtypes(include=datetime_type).columns:\n            df[c] = df[c].map(datetime_to_iso8601)\n            df[c] = df[c].astype(pd.StringDtype())\n        return df\n\n    def transform_impl(self, overwrite: bool = True, **kwargs):\n        \"\"\"Convert the Pandas DataFrame into Parquet Format in the SageWorks S3 Bucket, and\n        store the information about the data to the AWS Data Catalog sageworks database\n\n        Args:\n            overwrite (bool): Overwrite the existing data in the SageWorks S3 Bucket\n        \"\"\"\n        self.log.info(f\"DataFrame to SageWorks DataSource: {self.output_uuid}...\")\n\n        # Set up our metadata storage\n        sageworks_meta = {\"sageworks_tags\": self.output_tags}\n        sageworks_meta.update(self.output_meta)\n\n        # Create the Output Parquet file S3 Storage Path\n        s3_storage_path = f\"{self.data_sources_s3_path}/{self.output_uuid}\"\n\n        # Convert Object Columns to String\n        self.output_df = self.convert_object_to_string(self.output_df)\n\n        # Note: Both of these conversions may not be necessary, so we're leaving them commented out\n        \"\"\"\n        # Convert Object Columns to Datetime\n        self.output_df = self.convert_object_to_datetime(self.output_df)\n\n        # Now convert datetime columns to ISO-8601 string\n        # self.output_df = self.convert_datetime_columns(self.output_df)\n        \"\"\"\n\n        # Write out the DataFrame to AWS Data Catalog in either Parquet or JSONL format\n        description = f\"SageWorks data source: {self.output_uuid}\"\n        glue_table_settings = {\"description\": description, \"parameters\": sageworks_meta}\n        if self.output_format == \"parquet\":\n            wr.s3.to_parquet(\n                self.output_df,\n                path=s3_storage_path,\n                dataset=True,\n                mode=\"overwrite\",\n                database=self.data_catalog_db,\n                table=self.output_uuid,\n                filename_prefix=f\"{self.output_uuid}_\",\n                boto3_session=self.boto_session,\n                partition_cols=None,\n                glue_table_settings=glue_table_settings,\n            )  # FIXME: Have some logic around partition columns\n\n        # Note: In general Parquet works will for most uses cases. We recommend using Parquet\n        #       You can use JSON_EXTRACT on Parquet string field, and it works great.\n        elif self.output_format == \"jsonl\":\n            self.log.warning(\"We recommend using Parquet format for most use cases\")\n            self.log.warning(\"If you have a use case that requires JSONL please contact SageWorks support\")\n            self.log.warning(\"We'd like to understand what functionality JSONL is providing that isn't already\")\n            self.log.warning(\"provided with Parquet and JSON_EXTRACT() for your Athena Queries\")\n            wr.s3.to_json(\n                self.output_df,\n                path=s3_storage_path,\n                orient=\"records\",\n                lines=True,\n                date_format=\"iso\",\n                dataset=True,\n                mode=\"overwrite\",\n                database=self.data_catalog_db,\n                table=self.output_uuid,\n                filename_prefix=f\"{self.output_uuid}_\",\n                boto3_session=self.boto_session,\n                partition_cols=None,\n                glue_table_settings=glue_table_settings,\n            )\n        else:\n            raise ValueError(f\"Unsupported file format: {self.output_format}\")\n\n    def post_transform(self, **kwargs):\n        \"\"\"Post-Transform: Calling make_ready() on the DataSource\"\"\"\n        self.log.info(\"Post-Transform: Calling make_ready() on the DataSource...\")\n\n        # Okay grab the output DataSource\n        output_data_source = DataSourceFactory(self.output_uuid, force_refresh=True)\n        output_data_source.set_status(\"initializing\")\n\n        # Call the DataSource make_ready method to compute a bunch of EDA stuff\n        output_data_source.make_ready()\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToData.__init__","title":"<code>__init__(output_uuid, output_format='parquet')</code>","text":"<p>PandasToData Initialization Args:     output_uuid (str): The UUID of the DataSource to create     output_format (str): The file format to store the S3 object data in (default: \"parquet\")</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_data.py</code> <pre><code>def __init__(self, output_uuid: str, output_format: str = \"parquet\"):\n    \"\"\"PandasToData Initialization\n    Args:\n        output_uuid (str): The UUID of the DataSource to create\n        output_format (str): The file format to store the S3 object data in (default: \"parquet\")\n    \"\"\"\n\n    # Call superclass init\n    super().__init__(\"DataFrame\", output_uuid)\n\n    # Set up all my instance attributes\n    self.input_type = TransformInput.PANDAS_DF\n    self.output_type = TransformOutput.DATA_SOURCE\n    self.output_df = None\n\n    # Give a message that Parquet is best in most cases\n    if output_format != \"parquet\":\n        self.log.warning(\"Parquet format works the best in most cases please consider using it\")\n    self.output_format = output_format\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToData.convert_datetime_columns","title":"<code>convert_datetime_columns(df)</code>  <code>staticmethod</code>","text":"<p>Convert datetime columns to ISO-8601 string</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_data.py</code> <pre><code>@staticmethod\ndef convert_datetime_columns(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Convert datetime columns to ISO-8601 string\"\"\"\n    datetime_type = [\"datetime\", \"datetime64\", \"datetime64[ns]\", \"datetimetz\"]\n    for c in df.select_dtypes(include=datetime_type).columns:\n        df[c] = df[c].map(datetime_to_iso8601)\n        df[c] = df[c].astype(pd.StringDtype())\n    return df\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToData.convert_object_to_datetime","title":"<code>convert_object_to_datetime(df)</code>","text":"<p>Try to automatically convert object columns to datetime or string columns</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_data.py</code> <pre><code>def convert_object_to_datetime(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Try to automatically convert object columns to datetime or string columns\"\"\"\n    for c in df.columns[df.dtypes == \"object\"]:  # Look at the object columns\n        try:\n            df[c] = pd.to_datetime(df[c])\n        except (ParserError, ValueError, TypeError):\n            self.log.debug(f\"Column {c} could not be converted to datetime...\")\n    return df\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToData.convert_object_to_string","title":"<code>convert_object_to_string(df)</code>","text":"<p>Try to automatically convert object columns to string columns</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_data.py</code> <pre><code>def convert_object_to_string(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Try to automatically convert object columns to string columns\"\"\"\n    for c in df.columns[df.dtypes == \"object\"]:  # Look at the object columns\n        try:\n            df[c] = df[c].astype(\"string\")\n            df[c] = df[c].str.replace(\"'\", '\"')  # This is for nested JSON\n        except (ParserError, ValueError, TypeError):\n            self.log.info(f\"Column {c} could not be converted to string...\")\n    return df\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToData.post_transform","title":"<code>post_transform(**kwargs)</code>","text":"<p>Post-Transform: Calling make_ready() on the DataSource</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_data.py</code> <pre><code>def post_transform(self, **kwargs):\n    \"\"\"Post-Transform: Calling make_ready() on the DataSource\"\"\"\n    self.log.info(\"Post-Transform: Calling make_ready() on the DataSource...\")\n\n    # Okay grab the output DataSource\n    output_data_source = DataSourceFactory(self.output_uuid, force_refresh=True)\n    output_data_source.set_status(\"initializing\")\n\n    # Call the DataSource make_ready method to compute a bunch of EDA stuff\n    output_data_source.make_ready()\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToData.set_input","title":"<code>set_input(input_df)</code>","text":"<p>Set the DataFrame Input for this Transform</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_data.py</code> <pre><code>def set_input(self, input_df: pd.DataFrame):\n    \"\"\"Set the DataFrame Input for this Transform\"\"\"\n    self.output_df = input_df.copy()\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToData.transform_impl","title":"<code>transform_impl(overwrite=True, **kwargs)</code>","text":"<p>Convert the Pandas DataFrame into Parquet Format in the SageWorks S3 Bucket, and store the information about the data to the AWS Data Catalog sageworks database</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>Overwrite the existing data in the SageWorks S3 Bucket</p> <code>True</code> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_data.py</code> <pre><code>def transform_impl(self, overwrite: bool = True, **kwargs):\n    \"\"\"Convert the Pandas DataFrame into Parquet Format in the SageWorks S3 Bucket, and\n    store the information about the data to the AWS Data Catalog sageworks database\n\n    Args:\n        overwrite (bool): Overwrite the existing data in the SageWorks S3 Bucket\n    \"\"\"\n    self.log.info(f\"DataFrame to SageWorks DataSource: {self.output_uuid}...\")\n\n    # Set up our metadata storage\n    sageworks_meta = {\"sageworks_tags\": self.output_tags}\n    sageworks_meta.update(self.output_meta)\n\n    # Create the Output Parquet file S3 Storage Path\n    s3_storage_path = f\"{self.data_sources_s3_path}/{self.output_uuid}\"\n\n    # Convert Object Columns to String\n    self.output_df = self.convert_object_to_string(self.output_df)\n\n    # Note: Both of these conversions may not be necessary, so we're leaving them commented out\n    \"\"\"\n    # Convert Object Columns to Datetime\n    self.output_df = self.convert_object_to_datetime(self.output_df)\n\n    # Now convert datetime columns to ISO-8601 string\n    # self.output_df = self.convert_datetime_columns(self.output_df)\n    \"\"\"\n\n    # Write out the DataFrame to AWS Data Catalog in either Parquet or JSONL format\n    description = f\"SageWorks data source: {self.output_uuid}\"\n    glue_table_settings = {\"description\": description, \"parameters\": sageworks_meta}\n    if self.output_format == \"parquet\":\n        wr.s3.to_parquet(\n            self.output_df,\n            path=s3_storage_path,\n            dataset=True,\n            mode=\"overwrite\",\n            database=self.data_catalog_db,\n            table=self.output_uuid,\n            filename_prefix=f\"{self.output_uuid}_\",\n            boto3_session=self.boto_session,\n            partition_cols=None,\n            glue_table_settings=glue_table_settings,\n        )  # FIXME: Have some logic around partition columns\n\n    # Note: In general Parquet works will for most uses cases. We recommend using Parquet\n    #       You can use JSON_EXTRACT on Parquet string field, and it works great.\n    elif self.output_format == \"jsonl\":\n        self.log.warning(\"We recommend using Parquet format for most use cases\")\n        self.log.warning(\"If you have a use case that requires JSONL please contact SageWorks support\")\n        self.log.warning(\"We'd like to understand what functionality JSONL is providing that isn't already\")\n        self.log.warning(\"provided with Parquet and JSON_EXTRACT() for your Athena Queries\")\n        wr.s3.to_json(\n            self.output_df,\n            path=s3_storage_path,\n            orient=\"records\",\n            lines=True,\n            date_format=\"iso\",\n            dataset=True,\n            mode=\"overwrite\",\n            database=self.data_catalog_db,\n            table=self.output_uuid,\n            filename_prefix=f\"{self.output_uuid}_\",\n            boto3_session=self.boto_session,\n            partition_cols=None,\n            glue_table_settings=glue_table_settings,\n        )\n    else:\n        raise ValueError(f\"Unsupported file format: {self.output_format}\")\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToFeatures","title":"<code>PandasToFeatures</code>","text":"<p>             Bases: <code>Transform</code></p> <p>PandasToFeatures: Class to publish a Pandas DataFrame into a FeatureSet</p> Common Usage <pre><code>to_features = PandasToFeatures(output_uuid)\nto_features.set_output_tags([\"abalone\", \"public\", \"whatever\"])\nto_features.set_input(df, id_column=\"id\"/None, event_time_column=\"date\"/None)\nto_features.transform()\n</code></pre> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>class PandasToFeatures(Transform):\n    \"\"\"PandasToFeatures: Class to publish a Pandas DataFrame into a FeatureSet\n\n    Common Usage:\n        ```\n        to_features = PandasToFeatures(output_uuid)\n        to_features.set_output_tags([\"abalone\", \"public\", \"whatever\"])\n        to_features.set_input(df, id_column=\"id\"/None, event_time_column=\"date\"/None)\n        to_features.transform()\n        ```\n    \"\"\"\n\n    def __init__(self, output_uuid: str, auto_categorize=True):\n        \"\"\"PandasToFeatures Initialization\n        Args:\n            output_uuid (str): The UUID of the FeatureSet to create\n            auto_categorize (bool): Should we automatically create categorical columns?\n        \"\"\"\n        # Call superclass init\n        super().__init__(\"DataFrame\", output_uuid)\n\n        # Set up all my instance attributes\n        self.input_type = TransformInput.PANDAS_DF\n        self.output_type = TransformOutput.FEATURE_SET\n        self.target_column = None\n        self.id_column = None\n        self.event_time_column = None\n        self.auto_categorize = auto_categorize\n        self.categorical_dtypes = {}\n        self.output_df = None\n        self.table_format = TableFormatEnum.ICEBERG\n\n        # Delete the existing FeatureSet if it exists\n        self.delete_existing()\n\n        # These will be set in the transform method\n        self.output_feature_group = None\n        self.output_feature_set = None\n        self.expected_rows = 0\n\n    def set_input(self, input_df: pd.DataFrame, target_column=None, id_column=None, event_time_column=None):\n        \"\"\"Set the Input DataFrame for this Transform\n        Args:\n            input_df (pd.DataFrame): The input DataFrame\n            target_column (str): The name of the target column (default: None)\n            id_column (str): The name of the id column (default: None)\n            event_time_column (str): The name of the event_time column (default: None)\n        \"\"\"\n        self.target_column = target_column\n        self.id_column = id_column\n        self.event_time_column = event_time_column\n        self.output_df = input_df.copy()\n\n        # Now Prepare the DataFrame for its journey into an AWS FeatureGroup\n        self.prep_dataframe()\n\n    def delete_existing(self):\n        # Delete the existing FeatureSet if it exists\n        try:\n            delete_fs = FeatureSetCore(self.output_uuid)\n            if delete_fs.exists():\n                self.log.info(f\"Deleting the {self.output_uuid} FeatureSet...\")\n                delete_fs.delete()\n                time.sleep(1)\n        except botocore.exceptions.ClientError as exc:\n            self.log.info(f\"FeatureSet {self.output_uuid} doesn't exist...\")\n            self.log.info(exc)\n\n    def _ensure_id_column(self):\n        \"\"\"Internal: AWS Feature Store requires an Id field for all data store\"\"\"\n        if self.id_column is None or self.id_column not in self.output_df.columns:\n            if \"id\" not in self.output_df.columns:\n                self.log.info(\"Generating an id column before FeatureSet Creation...\")\n                self.output_df[\"id\"] = self.output_df.index\n            self.id_column = \"id\"\n\n    def _ensure_event_time(self):\n        \"\"\"Internal: AWS Feature Store requires an event_time field for all data stored\"\"\"\n        if self.event_time_column is None or self.event_time_column not in self.output_df.columns:\n            self.log.info(\"Generating an event_time column before FeatureSet Creation...\")\n            self.event_time_column = \"event_time\"\n            self.output_df[self.event_time_column] = pd.Timestamp(\"now\", tz=\"UTC\")\n\n        # The event_time_column is defined so we need to make sure it's in ISO-8601 string format\n        # Note: AWS Feature Store only a particular ISO-8601 format not ALL ISO-8601 formats\n        time_column = self.output_df[self.event_time_column]\n\n        # Check if the event_time_column is of type object or string convert it to DateTime\n        if time_column.dtypes == \"object\" or time_column.dtypes.name == \"string\":\n            self.log.info(f\"Converting {self.event_time_column} to DateTime...\")\n            time_column = pd.to_datetime(time_column)\n\n        # Let's make sure it the right type for Feature Store\n        if pd.api.types.is_datetime64_any_dtype(time_column):\n            self.log.info(f\"Converting {self.event_time_column} to ISOFormat Date String before FeatureSet Creation...\")\n\n            # Convert the datetime DType to ISO-8601 string\n            # TableFormat=ICEBERG does not support alternate formats for event_time field, it only supports String type.\n            time_column = time_column.map(datetime_to_iso8601)\n            self.output_df[self.event_time_column] = time_column.astype(\"string\")\n\n    def _convert_objs_to_string(self):\n        \"\"\"Internal: AWS Feature Store doesn't know how to store object dtypes, so convert to String\"\"\"\n        for col in self.output_df:\n            if pd.api.types.is_object_dtype(self.output_df[col].dtype):\n                self.output_df[col] = self.output_df[col].astype(pd.StringDtype())\n\n    def process_column_name(self, column: str, shorten: bool = False) -&gt; str:\n        \"\"\"Call various methods to make sure the column is ready for Feature Store\n        Args:\n            column (str): The column name to process\n            shorten (bool): Should we shorten the column name? (default: False)\n        \"\"\"\n        self.log.debug(f\"Processing column {column}...\")\n\n        # Make sure the column name is valid\n        column = self.sanitize_column_name(column)\n\n        # Make sure the column name isn't too long\n        if shorten:\n            column = self.shorten_column_name(column)\n\n        return column\n\n    def shorten_column_name(self, name, max_length=20):\n        if len(name) &lt;= max_length:\n            return name\n\n        # Start building the new name from the end\n        parts = name.split(\"_\")[::-1]\n        new_name = \"\"\n        for part in parts:\n            if len(new_name) + len(part) + 1 &lt;= max_length:  # +1 for the underscore\n                new_name = f\"{part}_{new_name}\" if new_name else part\n            else:\n                break\n\n        # If new_name is empty, just use the last part of the original name\n        if not new_name:\n            new_name = parts[0]\n\n        self.log.info(f\"Shortening {name} to {new_name}\")\n        return new_name\n\n    def sanitize_column_name(self, name):\n        # Remove all invalid characters\n        sanitized = re.sub(\"[^a-zA-Z0-9-_]\", \"_\", name)\n        sanitized = re.sub(\"_+\", \"_\", sanitized)\n        sanitized = sanitized.strip(\"_\")\n\n        # Log the change if the name was altered\n        if sanitized != name:\n            self.log.info(f\"Sanitizing {name} to {sanitized}\")\n\n        return sanitized\n\n    def one_hot_encoding(self, df, categorical_columns: list) -&gt; pd.DataFrame:\n        \"\"\"One Hot Encoding for Categorical Columns with additional column name management\"\"\"\n\n        # Now convert Categorical Types to One Hot Encoding\n        current_columns = list(df.columns)\n        df = pd.get_dummies(df, columns=categorical_columns)\n\n        # Compute the new columns generated by get_dummies\n        new_columns = list(set(df.columns) - set(current_columns))\n\n        # Convert new columns to int32\n        df[new_columns] = df[new_columns].astype(\"int32\")\n\n        # For the new columns we're going to shorten the names\n        renamed_columns = {col: self.process_column_name(col) for col in new_columns}\n\n        # Rename the columns in the DataFrame\n        df.rename(columns=renamed_columns, inplace=True)\n\n        return df\n\n    # Helper Methods\n    def auto_categorize_converter(self):\n        \"\"\"Convert object and string types to Categorical\"\"\"\n        categorical_columns = []\n        for feature, dtype in self.output_df.dtypes.items():\n            if dtype in [\"object\", \"string\", \"category\"] and feature not in [\n                self.event_time_column,\n                self.id_column,\n                self.target_column,\n            ]:\n                unique_values = self.output_df[feature].nunique()\n                if 1 &lt; unique_values &lt; 6:\n                    self.log.important(f\"Converting column {feature} to categorical (unique {unique_values})\")\n                    self.output_df[feature] = self.output_df[feature].astype(\"category\")\n                    categorical_columns.append(feature)\n\n        # Now convert Categorical Types to One Hot Encoding\n        self.output_df = self.one_hot_encoding(self.output_df, categorical_columns)\n\n    def manual_categorical_converter(self):\n        \"\"\"Convert object and string types to Categorical\"\"\"\n        for column, cat_d_type in self.categorical_dtypes.items():\n            self.output_df[column] = self.output_df[column].astype(cat_d_type)\n\n        # Now convert Categorical Types to One Hot Encoding\n        categorical_columns = list(self.categorical_dtypes.keys())\n        self.output_df = self.one_hot_encoding(self.output_df, categorical_columns)\n\n    @staticmethod\n    def convert_column_types(df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Convert the types of the DataFrame to the correct types for the Feature Store\"\"\"\n        for column in list(df.select_dtypes(include=\"bool\").columns):\n            df[column] = df[column].astype(\"int32\")\n        for column in list(df.select_dtypes(include=\"category\").columns):\n            df[column] = df[column].astype(\"str\")\n\n        # Special case for datetime types\n        for column in df.select_dtypes(include=[\"datetime\"]).columns:\n            df[column] = df[column].map(datetime_to_iso8601).astype(\"string\")\n\n        \"\"\"FIXME Not sure we need these conversions\n        for column in list(df.select_dtypes(include=\"object\").columns):\n            df[column] = df[column].astype(\"string\")\n        for column in list(df.select_dtypes(include=[pd.Int64Dtype]).columns):\n            df[column] = df[column].astype(\"int64\")\n        for column in list(df.select_dtypes(include=[pd.Float64Dtype]).columns):\n            df[column] = df[column].astype(\"float64\")\n        \"\"\"\n        return df\n\n    def prep_dataframe(self):\n        \"\"\"Prep the DataFrame for Feature Store Creation\"\"\"\n        self.log.info(\"Prep the output_df (cat_convert, convert types, lowercase columns, add training column)...\")\n\n        # Make sure we have the required id and event_time columns\n        self._ensure_id_column()\n        self._ensure_event_time()\n\n        # Convert object and string types to Categorical\n        if self.auto_categorize:\n            self.auto_categorize_converter()\n        else:\n            self.manual_categorical_converter()\n\n        # We need to convert some of our column types to the correct types\n        # Feature Store only supports these data types:\n        # - Integral\n        # - Fractional\n        # - String (timestamp/datetime types need to be converted to string)\n        self.output_df = self.convert_column_types(self.output_df)\n\n        # FeatureSet Internal Storage (Athena) will convert columns names to lowercase, so we need\n        # to make sure that the column names are lowercase to match and avoid downstream issues\n        self.output_df.columns = self.output_df.columns.str.lower()\n\n    def create_feature_group(self):\n        \"\"\"Create a Feature Group, load our Feature Definitions, and wait for it to be ready\"\"\"\n\n        # Create a Feature Group and load our Feature Definitions\n        my_feature_group = FeatureGroup(name=self.output_uuid, sagemaker_session=self.sm_session)\n        my_feature_group.load_feature_definitions(data_frame=self.output_df)\n\n        # Create the Output S3 Storage Path for this Feature Set\n        s3_storage_path = f\"{self.feature_sets_s3_path}/{self.output_uuid}\"\n\n        # Get the metadata/tags to push into AWS\n        aws_tags = self.get_aws_tags()\n\n        # Create the Feature Group\n        my_feature_group.create(\n            s3_uri=s3_storage_path,\n            record_identifier_name=self.id_column,\n            event_time_feature_name=self.event_time_column,\n            role_arn=self.sageworks_role_arn,\n            enable_online_store=True,\n            table_format=self.table_format,\n            tags=aws_tags,\n        )\n\n        # Ensure/wait for the feature group to be created\n        self.ensure_feature_group_created(my_feature_group)\n        return my_feature_group\n\n    def pre_transform(self, **kwargs):\n        \"\"\"Pre-Transform: Create the Feature Group\"\"\"\n        self.output_feature_group = self.create_feature_group()\n\n    def transform_impl(self):\n        \"\"\"Transform Implementation: Ingest the data into the Feature Group\"\"\"\n\n        # Now we actually push the data into the Feature Group (called ingestion)\n        self.log.important(\"Ingesting rows into Feature Group...\")\n        try:\n            ingest_manager = self.output_feature_group.ingest(self.output_df, max_processes=16, wait=False)\n            ingest_manager.wait()\n        except IngestionError as exc:\n            self.log.warning(f\"Some rows had an ingesting error: {exc}\")\n\n        # Report on any rows that failed to ingest\n        if ingest_manager.failed_rows:\n            self.log.warning(f\"Number of Failed Rows: {len(ingest_manager.failed_rows)}\")\n\n            # FIXME: This may or may not give us the correct rows\n            # If any index is greater then the number of rows, then the index needs\n            # to be converted to a relative index in our current output_df\n            df_rows = len(self.output_df)\n            relative_indexes = [idx - df_rows if idx &gt;= df_rows else idx for idx in ingest_manager.failed_rows]\n            failed_data = self.output_df.iloc[relative_indexes]\n            for idx, row in failed_data.iterrows():\n                self.log.warning(f\"Failed Row {idx}: {row.to_dict()}\")\n\n        # Keep track of the number of rows we expect to be ingested\n        self.expected_rows += len(self.output_df) - len(ingest_manager.failed_rows)\n        self.log.info(f\"Added rows: {len(self.output_df)}\")\n        self.log.info(f\"Failed rows: {len(ingest_manager.failed_rows)}\")\n        self.log.info(f\"Total rows to be ingested: {self.expected_rows}\")\n\n    def post_transform(self, **kwargs):\n        \"\"\"Post-Transform: Populating Offline Storage and make_ready()\"\"\"\n        self.log.info(\"Post-Transform: Populating Offline Storage and make_ready()...\")\n\n        # Feature Group Ingestion takes a while, so we need to wait for it to finish\n        self.output_feature_set = FeatureSetCore(self.output_uuid, force_refresh=True)\n        self.output_feature_set.set_status(\"initializing\")\n\n        # Wait for offline storage of the Feature Group to be ready\n        self.log.important(\"Waiting for AWS Feature Group Offline storage to be ready...\")\n        self.log.important(\"This will often take 10-20 minutes...go have coffee or lunch :)\")\n        self.wait_for_rows(self.expected_rows)\n\n        # Call the FeatureSet make_ready method to compute a bunch of EDA stuff\n        self.output_feature_set.make_ready()\n\n    def ensure_feature_group_created(self, feature_group):\n        status = feature_group.describe().get(\"FeatureGroupStatus\")\n        while status == \"Creating\":\n            self.log.debug(\"FeatureSet being Created...\")\n            time.sleep(5)\n            status = feature_group.describe().get(\"FeatureGroupStatus\")\n        self.log.info(f\"FeatureSet {feature_group.name} successfully created\")\n\n    def wait_for_rows(self, expected_rows: int):\n        \"\"\"Wait for AWS Feature Group to fully populate the Offline Storage\"\"\"\n        rows = self.output_feature_set.num_rows()\n\n        # Wait for the rows to be populated\n        self.log.info(f\"Waiting for AWS Feature Group {self.output_uuid} Offline Storage...\")\n        not_all_rows_retry = 5\n        while rows &lt; expected_rows and not_all_rows_retry &gt; 0:\n            sleep_time = 5 if rows else 60\n            not_all_rows_retry -= 1 if rows else 0\n            time.sleep(sleep_time)\n            rows = self.output_feature_set.num_rows()\n            self.log.info(f\"Offline Storage {self.output_uuid}: {rows} rows out of {expected_rows}\")\n        if rows == expected_rows:\n            self.log.important(f\"Success: Reached Expected Rows ({rows} rows)...\")\n        else:\n            self.log.warning(\n                f\"Did not reach expected rows ({rows}/{expected_rows}) but we're not sweating the small stuff...\"\n            )\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToFeatures.__init__","title":"<code>__init__(output_uuid, auto_categorize=True)</code>","text":"<p>PandasToFeatures Initialization Args:     output_uuid (str): The UUID of the FeatureSet to create     auto_categorize (bool): Should we automatically create categorical columns?</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>def __init__(self, output_uuid: str, auto_categorize=True):\n    \"\"\"PandasToFeatures Initialization\n    Args:\n        output_uuid (str): The UUID of the FeatureSet to create\n        auto_categorize (bool): Should we automatically create categorical columns?\n    \"\"\"\n    # Call superclass init\n    super().__init__(\"DataFrame\", output_uuid)\n\n    # Set up all my instance attributes\n    self.input_type = TransformInput.PANDAS_DF\n    self.output_type = TransformOutput.FEATURE_SET\n    self.target_column = None\n    self.id_column = None\n    self.event_time_column = None\n    self.auto_categorize = auto_categorize\n    self.categorical_dtypes = {}\n    self.output_df = None\n    self.table_format = TableFormatEnum.ICEBERG\n\n    # Delete the existing FeatureSet if it exists\n    self.delete_existing()\n\n    # These will be set in the transform method\n    self.output_feature_group = None\n    self.output_feature_set = None\n    self.expected_rows = 0\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToFeatures.auto_categorize_converter","title":"<code>auto_categorize_converter()</code>","text":"<p>Convert object and string types to Categorical</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>def auto_categorize_converter(self):\n    \"\"\"Convert object and string types to Categorical\"\"\"\n    categorical_columns = []\n    for feature, dtype in self.output_df.dtypes.items():\n        if dtype in [\"object\", \"string\", \"category\"] and feature not in [\n            self.event_time_column,\n            self.id_column,\n            self.target_column,\n        ]:\n            unique_values = self.output_df[feature].nunique()\n            if 1 &lt; unique_values &lt; 6:\n                self.log.important(f\"Converting column {feature} to categorical (unique {unique_values})\")\n                self.output_df[feature] = self.output_df[feature].astype(\"category\")\n                categorical_columns.append(feature)\n\n    # Now convert Categorical Types to One Hot Encoding\n    self.output_df = self.one_hot_encoding(self.output_df, categorical_columns)\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToFeatures.convert_column_types","title":"<code>convert_column_types(df)</code>  <code>staticmethod</code>","text":"<p>Convert the types of the DataFrame to the correct types for the Feature Store</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>@staticmethod\ndef convert_column_types(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Convert the types of the DataFrame to the correct types for the Feature Store\"\"\"\n    for column in list(df.select_dtypes(include=\"bool\").columns):\n        df[column] = df[column].astype(\"int32\")\n    for column in list(df.select_dtypes(include=\"category\").columns):\n        df[column] = df[column].astype(\"str\")\n\n    # Special case for datetime types\n    for column in df.select_dtypes(include=[\"datetime\"]).columns:\n        df[column] = df[column].map(datetime_to_iso8601).astype(\"string\")\n\n    \"\"\"FIXME Not sure we need these conversions\n    for column in list(df.select_dtypes(include=\"object\").columns):\n        df[column] = df[column].astype(\"string\")\n    for column in list(df.select_dtypes(include=[pd.Int64Dtype]).columns):\n        df[column] = df[column].astype(\"int64\")\n    for column in list(df.select_dtypes(include=[pd.Float64Dtype]).columns):\n        df[column] = df[column].astype(\"float64\")\n    \"\"\"\n    return df\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToFeatures.create_feature_group","title":"<code>create_feature_group()</code>","text":"<p>Create a Feature Group, load our Feature Definitions, and wait for it to be ready</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>def create_feature_group(self):\n    \"\"\"Create a Feature Group, load our Feature Definitions, and wait for it to be ready\"\"\"\n\n    # Create a Feature Group and load our Feature Definitions\n    my_feature_group = FeatureGroup(name=self.output_uuid, sagemaker_session=self.sm_session)\n    my_feature_group.load_feature_definitions(data_frame=self.output_df)\n\n    # Create the Output S3 Storage Path for this Feature Set\n    s3_storage_path = f\"{self.feature_sets_s3_path}/{self.output_uuid}\"\n\n    # Get the metadata/tags to push into AWS\n    aws_tags = self.get_aws_tags()\n\n    # Create the Feature Group\n    my_feature_group.create(\n        s3_uri=s3_storage_path,\n        record_identifier_name=self.id_column,\n        event_time_feature_name=self.event_time_column,\n        role_arn=self.sageworks_role_arn,\n        enable_online_store=True,\n        table_format=self.table_format,\n        tags=aws_tags,\n    )\n\n    # Ensure/wait for the feature group to be created\n    self.ensure_feature_group_created(my_feature_group)\n    return my_feature_group\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToFeatures.manual_categorical_converter","title":"<code>manual_categorical_converter()</code>","text":"<p>Convert object and string types to Categorical</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>def manual_categorical_converter(self):\n    \"\"\"Convert object and string types to Categorical\"\"\"\n    for column, cat_d_type in self.categorical_dtypes.items():\n        self.output_df[column] = self.output_df[column].astype(cat_d_type)\n\n    # Now convert Categorical Types to One Hot Encoding\n    categorical_columns = list(self.categorical_dtypes.keys())\n    self.output_df = self.one_hot_encoding(self.output_df, categorical_columns)\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToFeatures.one_hot_encoding","title":"<code>one_hot_encoding(df, categorical_columns)</code>","text":"<p>One Hot Encoding for Categorical Columns with additional column name management</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>def one_hot_encoding(self, df, categorical_columns: list) -&gt; pd.DataFrame:\n    \"\"\"One Hot Encoding for Categorical Columns with additional column name management\"\"\"\n\n    # Now convert Categorical Types to One Hot Encoding\n    current_columns = list(df.columns)\n    df = pd.get_dummies(df, columns=categorical_columns)\n\n    # Compute the new columns generated by get_dummies\n    new_columns = list(set(df.columns) - set(current_columns))\n\n    # Convert new columns to int32\n    df[new_columns] = df[new_columns].astype(\"int32\")\n\n    # For the new columns we're going to shorten the names\n    renamed_columns = {col: self.process_column_name(col) for col in new_columns}\n\n    # Rename the columns in the DataFrame\n    df.rename(columns=renamed_columns, inplace=True)\n\n    return df\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToFeatures.post_transform","title":"<code>post_transform(**kwargs)</code>","text":"<p>Post-Transform: Populating Offline Storage and make_ready()</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>def post_transform(self, **kwargs):\n    \"\"\"Post-Transform: Populating Offline Storage and make_ready()\"\"\"\n    self.log.info(\"Post-Transform: Populating Offline Storage and make_ready()...\")\n\n    # Feature Group Ingestion takes a while, so we need to wait for it to finish\n    self.output_feature_set = FeatureSetCore(self.output_uuid, force_refresh=True)\n    self.output_feature_set.set_status(\"initializing\")\n\n    # Wait for offline storage of the Feature Group to be ready\n    self.log.important(\"Waiting for AWS Feature Group Offline storage to be ready...\")\n    self.log.important(\"This will often take 10-20 minutes...go have coffee or lunch :)\")\n    self.wait_for_rows(self.expected_rows)\n\n    # Call the FeatureSet make_ready method to compute a bunch of EDA stuff\n    self.output_feature_set.make_ready()\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToFeatures.pre_transform","title":"<code>pre_transform(**kwargs)</code>","text":"<p>Pre-Transform: Create the Feature Group</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>def pre_transform(self, **kwargs):\n    \"\"\"Pre-Transform: Create the Feature Group\"\"\"\n    self.output_feature_group = self.create_feature_group()\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToFeatures.prep_dataframe","title":"<code>prep_dataframe()</code>","text":"<p>Prep the DataFrame for Feature Store Creation</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>def prep_dataframe(self):\n    \"\"\"Prep the DataFrame for Feature Store Creation\"\"\"\n    self.log.info(\"Prep the output_df (cat_convert, convert types, lowercase columns, add training column)...\")\n\n    # Make sure we have the required id and event_time columns\n    self._ensure_id_column()\n    self._ensure_event_time()\n\n    # Convert object and string types to Categorical\n    if self.auto_categorize:\n        self.auto_categorize_converter()\n    else:\n        self.manual_categorical_converter()\n\n    # We need to convert some of our column types to the correct types\n    # Feature Store only supports these data types:\n    # - Integral\n    # - Fractional\n    # - String (timestamp/datetime types need to be converted to string)\n    self.output_df = self.convert_column_types(self.output_df)\n\n    # FeatureSet Internal Storage (Athena) will convert columns names to lowercase, so we need\n    # to make sure that the column names are lowercase to match and avoid downstream issues\n    self.output_df.columns = self.output_df.columns.str.lower()\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToFeatures.process_column_name","title":"<code>process_column_name(column, shorten=False)</code>","text":"<p>Call various methods to make sure the column is ready for Feature Store Args:     column (str): The column name to process     shorten (bool): Should we shorten the column name? (default: False)</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>def process_column_name(self, column: str, shorten: bool = False) -&gt; str:\n    \"\"\"Call various methods to make sure the column is ready for Feature Store\n    Args:\n        column (str): The column name to process\n        shorten (bool): Should we shorten the column name? (default: False)\n    \"\"\"\n    self.log.debug(f\"Processing column {column}...\")\n\n    # Make sure the column name is valid\n    column = self.sanitize_column_name(column)\n\n    # Make sure the column name isn't too long\n    if shorten:\n        column = self.shorten_column_name(column)\n\n    return column\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToFeatures.set_input","title":"<code>set_input(input_df, target_column=None, id_column=None, event_time_column=None)</code>","text":"<p>Set the Input DataFrame for this Transform Args:     input_df (pd.DataFrame): The input DataFrame     target_column (str): The name of the target column (default: None)     id_column (str): The name of the id column (default: None)     event_time_column (str): The name of the event_time column (default: None)</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>def set_input(self, input_df: pd.DataFrame, target_column=None, id_column=None, event_time_column=None):\n    \"\"\"Set the Input DataFrame for this Transform\n    Args:\n        input_df (pd.DataFrame): The input DataFrame\n        target_column (str): The name of the target column (default: None)\n        id_column (str): The name of the id column (default: None)\n        event_time_column (str): The name of the event_time column (default: None)\n    \"\"\"\n    self.target_column = target_column\n    self.id_column = id_column\n    self.event_time_column = event_time_column\n    self.output_df = input_df.copy()\n\n    # Now Prepare the DataFrame for its journey into an AWS FeatureGroup\n    self.prep_dataframe()\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToFeatures.transform_impl","title":"<code>transform_impl()</code>","text":"<p>Transform Implementation: Ingest the data into the Feature Group</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>def transform_impl(self):\n    \"\"\"Transform Implementation: Ingest the data into the Feature Group\"\"\"\n\n    # Now we actually push the data into the Feature Group (called ingestion)\n    self.log.important(\"Ingesting rows into Feature Group...\")\n    try:\n        ingest_manager = self.output_feature_group.ingest(self.output_df, max_processes=16, wait=False)\n        ingest_manager.wait()\n    except IngestionError as exc:\n        self.log.warning(f\"Some rows had an ingesting error: {exc}\")\n\n    # Report on any rows that failed to ingest\n    if ingest_manager.failed_rows:\n        self.log.warning(f\"Number of Failed Rows: {len(ingest_manager.failed_rows)}\")\n\n        # FIXME: This may or may not give us the correct rows\n        # If any index is greater then the number of rows, then the index needs\n        # to be converted to a relative index in our current output_df\n        df_rows = len(self.output_df)\n        relative_indexes = [idx - df_rows if idx &gt;= df_rows else idx for idx in ingest_manager.failed_rows]\n        failed_data = self.output_df.iloc[relative_indexes]\n        for idx, row in failed_data.iterrows():\n            self.log.warning(f\"Failed Row {idx}: {row.to_dict()}\")\n\n    # Keep track of the number of rows we expect to be ingested\n    self.expected_rows += len(self.output_df) - len(ingest_manager.failed_rows)\n    self.log.info(f\"Added rows: {len(self.output_df)}\")\n    self.log.info(f\"Failed rows: {len(ingest_manager.failed_rows)}\")\n    self.log.info(f\"Total rows to be ingested: {self.expected_rows}\")\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToFeatures.wait_for_rows","title":"<code>wait_for_rows(expected_rows)</code>","text":"<p>Wait for AWS Feature Group to fully populate the Offline Storage</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_features.py</code> <pre><code>def wait_for_rows(self, expected_rows: int):\n    \"\"\"Wait for AWS Feature Group to fully populate the Offline Storage\"\"\"\n    rows = self.output_feature_set.num_rows()\n\n    # Wait for the rows to be populated\n    self.log.info(f\"Waiting for AWS Feature Group {self.output_uuid} Offline Storage...\")\n    not_all_rows_retry = 5\n    while rows &lt; expected_rows and not_all_rows_retry &gt; 0:\n        sleep_time = 5 if rows else 60\n        not_all_rows_retry -= 1 if rows else 0\n        time.sleep(sleep_time)\n        rows = self.output_feature_set.num_rows()\n        self.log.info(f\"Offline Storage {self.output_uuid}: {rows} rows out of {expected_rows}\")\n    if rows == expected_rows:\n        self.log.important(f\"Success: Reached Expected Rows ({rows} rows)...\")\n    else:\n        self.log.warning(\n            f\"Did not reach expected rows ({rows}/{expected_rows}) but we're not sweating the small stuff...\"\n        )\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToFeaturesChunked","title":"<code>PandasToFeaturesChunked</code>","text":"<p>             Bases: <code>Transform</code></p> <p>PandasToFeaturesChunked:  Class to manage a bunch of chunked Pandas DataFrames into a FeatureSet</p> Common Usage <pre><code>to_features = PandasToFeaturesChunked(output_uuid, id_column=\"id\"/None, event_time_column=\"date\"/None)\nto_features.set_output_tags([\"abalone\", \"public\", \"whatever\"])\ncat_column_info = {\"sex\": [\"M\", \"F\", \"I\"]}\nto_features.set_categorical_info(cat_column_info)\nto_features.add_chunk(df)\nto_features.add_chunk(df)\n...\nto_features.finalize()\n</code></pre> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_features_chunked.py</code> <pre><code>class PandasToFeaturesChunked(Transform):\n    \"\"\"PandasToFeaturesChunked:  Class to manage a bunch of chunked Pandas DataFrames into a FeatureSet\n\n    Common Usage:\n        ```\n        to_features = PandasToFeaturesChunked(output_uuid, id_column=\"id\"/None, event_time_column=\"date\"/None)\n        to_features.set_output_tags([\"abalone\", \"public\", \"whatever\"])\n        cat_column_info = {\"sex\": [\"M\", \"F\", \"I\"]}\n        to_features.set_categorical_info(cat_column_info)\n        to_features.add_chunk(df)\n        to_features.add_chunk(df)\n        ...\n        to_features.finalize()\n        ```\n    \"\"\"\n\n    def __init__(self, output_uuid: str, id_column=None, event_time_column=None):\n        \"\"\"PandasToFeaturesChunked Initialization\"\"\"\n\n        # Call superclass init\n        super().__init__(\"DataFrame\", output_uuid)\n\n        # Set up all my instance attributes\n        self.id_column = id_column\n        self.event_time_column = event_time_column\n        self.first_chunk = None\n        self.pandas_to_features = PandasToFeatures(output_uuid, auto_categorize=False)\n\n    def set_categorical_info(self, cat_column_info: dict[list[str]]):\n        \"\"\"Set the Categorical Columns\n        Args:\n            cat_column_info (dict[list[str]]): Dictionary of categorical columns and their possible values\n        \"\"\"\n\n        # Create the CategoricalDtypes\n        cat_d_types = {}\n        for col, vals in cat_column_info.items():\n            cat_d_types[col] = CategoricalDtype(categories=vals)\n\n        # Now set the CategoricalDtypes on our underlying PandasToFeatures\n        self.pandas_to_features.categorical_dtypes = cat_d_types\n\n    def add_chunk(self, chunk_df: pd.DataFrame):\n        \"\"\"Add a Chunk of Data to the FeatureSet\"\"\"\n\n        # Is this the first chunk? If so we need to run the pre_transform\n        if self.first_chunk is None:\n            self.log.info(f\"Adding first chunk {chunk_df.shape}...\")\n            self.first_chunk = chunk_df\n            self.pandas_to_features.set_input(chunk_df, self.id_column, self.event_time_column)\n            self.pandas_to_features.pre_transform()\n            self.pandas_to_features.transform_impl()\n        else:\n            self.log.info(f\"Adding chunk {chunk_df.shape}...\")\n            self.pandas_to_features.set_input(chunk_df, self.id_column, self.event_time_column)\n            self.pandas_to_features.transform_impl()\n\n    def pre_transform(self, **kwargs):\n        \"\"\"Pre-Transform: Create the Feature Group with Chunked Data\"\"\"\n\n        # Loading data into a Feature Group takes a while, so set status to loading\n        FeatureSetCore(self.output_uuid).set_status(\"loading\")\n\n    def transform_impl(self):\n        \"\"\"Required implementation of the Transform interface\"\"\"\n        self.log.warning(\"PandasToFeaturesChunked.transform_impl() called.  This is a no-op.\")\n\n    def post_transform(self, **kwargs):\n        \"\"\"Post-Transform: Any Post Transform Steps\"\"\"\n        self.pandas_to_features.post_transform()\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToFeaturesChunked.__init__","title":"<code>__init__(output_uuid, id_column=None, event_time_column=None)</code>","text":"<p>PandasToFeaturesChunked Initialization</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_features_chunked.py</code> <pre><code>def __init__(self, output_uuid: str, id_column=None, event_time_column=None):\n    \"\"\"PandasToFeaturesChunked Initialization\"\"\"\n\n    # Call superclass init\n    super().__init__(\"DataFrame\", output_uuid)\n\n    # Set up all my instance attributes\n    self.id_column = id_column\n    self.event_time_column = event_time_column\n    self.first_chunk = None\n    self.pandas_to_features = PandasToFeatures(output_uuid, auto_categorize=False)\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToFeaturesChunked.add_chunk","title":"<code>add_chunk(chunk_df)</code>","text":"<p>Add a Chunk of Data to the FeatureSet</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_features_chunked.py</code> <pre><code>def add_chunk(self, chunk_df: pd.DataFrame):\n    \"\"\"Add a Chunk of Data to the FeatureSet\"\"\"\n\n    # Is this the first chunk? If so we need to run the pre_transform\n    if self.first_chunk is None:\n        self.log.info(f\"Adding first chunk {chunk_df.shape}...\")\n        self.first_chunk = chunk_df\n        self.pandas_to_features.set_input(chunk_df, self.id_column, self.event_time_column)\n        self.pandas_to_features.pre_transform()\n        self.pandas_to_features.transform_impl()\n    else:\n        self.log.info(f\"Adding chunk {chunk_df.shape}...\")\n        self.pandas_to_features.set_input(chunk_df, self.id_column, self.event_time_column)\n        self.pandas_to_features.transform_impl()\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToFeaturesChunked.post_transform","title":"<code>post_transform(**kwargs)</code>","text":"<p>Post-Transform: Any Post Transform Steps</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_features_chunked.py</code> <pre><code>def post_transform(self, **kwargs):\n    \"\"\"Post-Transform: Any Post Transform Steps\"\"\"\n    self.pandas_to_features.post_transform()\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToFeaturesChunked.pre_transform","title":"<code>pre_transform(**kwargs)</code>","text":"<p>Pre-Transform: Create the Feature Group with Chunked Data</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_features_chunked.py</code> <pre><code>def pre_transform(self, **kwargs):\n    \"\"\"Pre-Transform: Create the Feature Group with Chunked Data\"\"\"\n\n    # Loading data into a Feature Group takes a while, so set status to loading\n    FeatureSetCore(self.output_uuid).set_status(\"loading\")\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToFeaturesChunked.set_categorical_info","title":"<code>set_categorical_info(cat_column_info)</code>","text":"<p>Set the Categorical Columns Args:     cat_column_info (dict[list[str]]): Dictionary of categorical columns and their possible values</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_features_chunked.py</code> <pre><code>def set_categorical_info(self, cat_column_info: dict[list[str]]):\n    \"\"\"Set the Categorical Columns\n    Args:\n        cat_column_info (dict[list[str]]): Dictionary of categorical columns and their possible values\n    \"\"\"\n\n    # Create the CategoricalDtypes\n    cat_d_types = {}\n    for col, vals in cat_column_info.items():\n        cat_d_types[col] = CategoricalDtype(categories=vals)\n\n    # Now set the CategoricalDtypes on our underlying PandasToFeatures\n    self.pandas_to_features.categorical_dtypes = cat_d_types\n</code></pre>"},{"location":"core_classes/transforms/pandas_transforms/#sageworks.core.transforms.pandas_transforms.PandasToFeaturesChunked.transform_impl","title":"<code>transform_impl()</code>","text":"<p>Required implementation of the Transform interface</p> Source code in <code>src/sageworks/core/transforms/pandas_transforms/pandas_to_features_chunked.py</code> <pre><code>def transform_impl(self):\n    \"\"\"Required implementation of the Transform interface\"\"\"\n    self.log.warning(\"PandasToFeaturesChunked.transform_impl() called.  This is a no-op.\")\n</code></pre>"},{"location":"misc/general_info/","title":"General info","text":""},{"location":"misc/general_info/#general-info","title":"General Info","text":""},{"location":"misc/general_info/#sageworks-the-scientists-workbench-powered-by-aws-for-scalability-flexibility-and-security","title":"SageWorks: The scientist's workbench powered by AWS\u00ae for scalability, flexibility, and security.","text":"<p>SageWorks is a medium granularity framework that manages and aggregates AWS\u00ae Services into classes and concepts. When you use SageWorks you think about DataSources, FeatureSets, Models, and Endpoints. Underneath the hood those classes handle all the details around updating and managing a complex set of AWS Services. All the power and none of the pain so that your team can Do Science Faster!</p>"},{"location":"misc/general_info/#sageworks-documentation","title":"SageWorks Documentation","text":"<p>See our Python API and AWS documentation here: SageWorks Documentation</p>"},{"location":"misc/general_info/#full-sageworks-overview","title":"Full SageWorks OverView","text":"<p>SageWorks Architected FrameWork</p>"},{"location":"misc/general_info/#why-sageworks","title":"Why SageWorks?","text":"<ul> <li>The AWS SageMaker\u00ae ecosystem is awesome but has a large number of services with significant complexity</li> <li>SageWorks provides rapid prototyping through easy to use classes and transforms</li> <li>SageWorks provides visibility and transparency into AWS SageMaker\u00ae Pipelines<ul> <li>What S3 data sources are getting pulled?</li> <li>What Features Store/Group is the Model Using?</li> <li>What's the Provenance of a Model in Model Registry?</li> <li>What SageMaker Endpoints are associated with this model?</li> </ul> </li> </ul>"},{"location":"misc/general_info/#single-pane-of-glass","title":"Single Pane of Glass","text":"<p>Visibility into the AWS Services that underpin the SageWorks Classes. We can see that SageWorks automatically tags and tracks the inputs of all artifacts providing 'data provenance' for all steps in the AWS modeling pipeline.</p> <p>Image TBD</p> <p> Clearly illustrated: SageWorks provides intuitive and transparent visibility into the full pipeline of your AWS Sagemaker Deployments.</p>"},{"location":"misc/general_info/#getting-started","title":"Getting Started","text":"<ul> <li>SageWorks Overview Slides that cover and illustrate the SageWorks Modeling Pipeline.</li> <li>SageWorks Docs/Wiki Our general documentation for getting started with SageWorks.</li> <li>SageWorks AWS Onboarding Deploy the SageWorks Stack to your AWS Account. </li> <li>Notebook: Start to Finish AWS ML Pipeline Building an AWS\u00ae ML Pipeline from start to finish.</li> <li>Video: Coding with SageWorks Informal coding + chatting while building a full ML pipeline.</li> <li>Join our Discord for questions and advice on using SageWorks within your organization.</li> </ul>"},{"location":"misc/general_info/#sageworks-zen","title":"SageWorks Zen","text":"<ul> <li>The AWS SageMaker\u00ae set of services is vast and complex.</li> <li>SageWorks Classes encapsulate, organize, and manage sets of AWS\u00ae Services.</li> <li>Heavy transforms typically use AWS Athena or Apache Spark (AWS Glue/EMR Serverless).</li> <li>Light transforms will typically use Pandas.</li> <li>Heavy and Light transforms both update AWS Artifacts (collections of AWS Services).</li> <li>Quick prototypes are typically built with the light path and then flipped to the heavy path as the system matures and usage grows.</li> </ul>"},{"location":"misc/general_info/#classes-and-concepts","title":"Classes and Concepts","text":"<p>The SageWorks Classes are organized to work in concert with AWS Services. For more details on the current classes and class hierarchies see SageWorks Classes and Concepts.</p>"},{"location":"misc/general_info/#contributions","title":"Contributions","text":"<p>If you'd like to contribute to the SageWorks project, you're more than welcome. All contributions will fall under the existing project license. If you are interested in contributing or have questions please feel free to contact us at sageworks@supercowpowers.com.</p>"},{"location":"misc/general_info/#sageworks-alpha-testers-wanted","title":"SageWorks Alpha Testers Wanted","text":"<p>Our experienced team can provide development and consulting services to help you effectively use Amazon\u2019s Machine Learning services within your organization.</p> <p>The popularity of cloud based Machine Learning services is booming. The problem many companies face is how that capability gets effectively used and harnessed to drive real business decisions and provide concrete value for their organization.</p> <p>Using SageWorks will minimize the time and manpower needed to incorporate AWS ML into your organization. If your company would like to be a SageWorks Alpha Tester, contact us at sageworks@supercowpowers.com.</p> <p>\u00ae Amazon Web Services, AWS, the Powered by AWS logo, are trademarks of Amazon.com, Inc. or its affiliates.</p> <p>Readme change</p>"},{"location":"misc/sageworks_classes_concepts/","title":"SageWorks Classes and Concepts","text":"<p>A flexible, rapid, and customizable AWS\u00ae ML Sandbox. Here's some of the classes and concepts we use in the SageWorks system:</p> <p></p> <ul> <li>Artifacts</li> <li>DataLoader</li> <li>DataSource</li> <li>FeatureSet</li> <li>Model</li> <li> <p>Endpoint</p> </li> <li> <p>Transforms</p> </li> <li>DataSource to DataSource<ul> <li>Heavy <ul> <li>AWS Glue Jobs</li> <li>AWS EMR Serverless</li> </ul> </li> <li>Light<ul> <li>Local/Laptop</li> <li>Lambdas</li> <li>StepFunctions</li> </ul> </li> </ul> </li> <li>DataSource to FeatureSet<ul> <li>Heavy/Light (see above breakout)</li> </ul> </li> <li>FeatureSet to FeatureSet<ul> <li>Heavy/Light (see above breakout)</li> </ul> </li> <li>FeatureSet to Model</li> <li>Model to Endpoint</li> </ul>"},{"location":"misc/scp_consulting/","title":"Scp consulting","text":""},{"location":"misc/scp_consulting/#consulting","title":"Consulting","text":""},{"location":"misc/scp_consulting/#sageworks-scp-consulting-awesome","title":"SageWorks + SCP Consulting = Awesome","text":"<p>Our experienced team can provide development and consulting services to help you effectively use Amazon\u2019s Machine Learning services within your organization.</p> <p>The popularity of cloud based Machine Learning services is booming. The problem many companies face is how that capability gets effectively used and harnessed to drive real business decisions and provide concrete value for their organization.</p> <p>Using SageWorks will minimizize the time and manpower needed to incorporate AWS ML into your organization. If your company would like to be a SageWorks Alpha Tester, contact us at sageworks@supercowpowers.com.</p>"},{"location":"misc/scp_consulting/#typical-engagements","title":"Typical Engagements","text":"<p>SageWorks clients typically want a tailored web_interface that helps to drive business decisions and provides value for their organization.</p> <ul> <li>SageWorks components provide a set of classes and transforms the will dramatically reduce time and increase productivity when building AWS ML Systems.</li> <li>SageWorks enables rapid prototyping via it's light paths and provides AWS production workflows on large scale data through it's heavy paths.</li> <li> <p>Rapid Prototyping is typically done via these steps.</p> </li> <li> <p>Quick Construction of Web Interface (tailored)</p> </li> <li>Custom Components (tailored)</li> <li>Demo/Review the Application with the Client</li> <li>Get Feedback/Changes/Improvements</li> <li> <p>Goto Step 1</p> </li> <li> <p>When the client is happy/excited about the ProtoType we then bolt down the system, test the heavy paths, review AWS access, security and ensure 'least privileged' roles and policies.</p> </li> </ul> <p>Contact us for a free initial consultation on how we can accelerate the use of AWS ML at your company sageworks@supercowpowers.com.</p>"}]}