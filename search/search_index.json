{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to SageWorks","text":"<p>SageWorks is a medium granularity framework that manages and aggregates AWS\u00ae Services into classes and concepts. When you use SageWorks you think about DataSources, FeatureSets, Models, and Endpoints. Underneath the hood those classes handle all the details around updating and managing a complex set of AWS Services. </p>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Setting up SageWorks on your AWS Account: AWS Setup</li> <li>Using SageWorks for ML Pipelines: SageWorks API Classes</li> </ul>"},{"location":"#additional-resources","title":"Additional Resources","text":"<ul> <li>SageWorks Core Classes: Core Classes</li> <li>Visit our Wiki Pages: SageWorks Wiki</li> <li>Consulting Available: SuperCowPowers LLC</li> </ul>"},{"location":"admin/docker_push/","title":"Docker Build and Push","text":"<p>Notes and information on how to do the Docker Builds and Push to AWS ECR.</p> <p>The following instructions should work, but things change :)</p>"},{"location":"admin/docker_push/#update-sageworks-version","title":"Update SageWorks Version","text":"<pre><code>cd applications/aws_dashboard\nvi Dockerfile\n# Install Sageworks (changes often)\nRUN pip install --no-cache-dir sageworks==0.2.28 &lt;-- change this\n</code></pre>"},{"location":"admin/docker_push/#build-the-docker-image","title":"Build the Docker Image","text":"<pre><code>docker build -t sageworks_dashboard:v0_1_9_amd64 --platform linux/amd64 .\n</code></pre>"},{"location":"admin/docker_push/#test-the-image-locally","title":"Test the Image Locally","text":"<p>You have a <code>docker_local_dashboard</code> alias in your <code>~/.zshrc</code> :)</p>"},{"location":"admin/docker_push/#login-to-ecr","title":"Login to ECR","text":"<pre><code>aws ecr-public get-login-password --region us-east-1 --profile \\\nscp_sandbox_admin | docker login --username AWS \\\n--password-stdin public.ecr.aws\n</code></pre>"},{"location":"admin/docker_push/#tagpush-the-image-to-aws-ecr","title":"Tag/Push the Image to AWS ECR","text":"<pre><code>docker tag sageworks_dashboard:v0_2_28_amd64 \\\npublic.ecr.aws/m6i5k1r2/sageworks_dashboard:v0_2_28_amd64\n</code></pre> <pre><code>docker push public.ecr.aws/m6i5k1r2/sageworks_dashboard:v0_2_28_amd64\n</code></pre>"},{"location":"admin/docker_push/#update-the-latest-tag","title":"Update the 'latest' tag","text":"<pre><code>docker tag public.ecr.aws/m6i5k1r2/sageworks_dashboard:v0_2_28_amd64 \\\npublic.ecr.aws/m6i5k1r2/sageworks_dashboard:latest\n</code></pre> <pre><code>docker push public.ecr.aws/m6i5k1r2/sageworks_dashboard:latest\n</code></pre>"},{"location":"admin/docker_push/#test-the-ecr-image","title":"Test the ECR Image","text":"<p>You have a <code>docker_ecr_dashboard</code> alias in your <code>~/.zshrc</code> :)</p>"},{"location":"admin/pypi_release/","title":"PyPI Release Notes","text":"<p>Notes and information on how to do the PyPI release for the SageMaker project. For full details on packaging you can reference this page Packaging</p> <p>The following instructions should work, but things change :)</p>"},{"location":"admin/pypi_release/#package-requirements","title":"Package Requirements","text":"<ul> <li>pip install tox</li> <li>pip install --upgrade setuptools wheel</li> <li>pip install twine</li> </ul>"},{"location":"admin/pypi_release/#setup-pypirc","title":"Setup pypirc","text":"<p>The easiest thing to do is setup a \\~/.pypirc file with the following contents</p> <pre><code>[distutils]\nindex-servers =\n  pypi\n  testpypi\n\n[pypi]\nrepository=https://upload.pypi.org/legacy/\nusername=&lt;pypi username&gt;\npassword=&lt;pypi password&gt;\n\n[testpypi]\nrepository=https://test.pypi.org/legacy/\nusername=&lt;pypi username&gt;\npassword=&lt;pypi password&gt;\n</code></pre>"},{"location":"admin/pypi_release/#tox-background","title":"Tox Background","text":"<p>Tox will install the SageMaker Sandbox package into a blank virtualenv and then execute all the tests against the newly installed package. So if everything goes okay, you know the pypi package installed fine and the tests (which puls from the installed <code>sageworks</code> package) also ran okay.</p>"},{"location":"admin/pypi_release/#make-sure-all-tests-pass","title":"Make sure ALL tests pass","text":"<pre><code>$ cd sageworks\n$ tox \n</code></pre> <p>If ALL the test above pass...</p>"},{"location":"admin/pypi_release/#clean-any-previous-distribution-files","title":"Clean any previous distribution files","text":"<pre><code>make clean\n</code></pre>"},{"location":"admin/pypi_release/#tag-the-new-version","title":"Tag the New Version","text":"<pre><code>git tag v0.1.8 (or whatever)\ngit push --tags\n</code></pre>"},{"location":"admin/pypi_release/#create-the-test-pypi-release","title":"Create the TEST PyPI Release","text":"<pre><code>python setup.py sdist bdist_wheel\ntwine upload dist/* -r testpypi\n</code></pre>"},{"location":"admin/pypi_release/#install-the-test-pypi-release","title":"Install the TEST PyPI Release","text":"<pre><code>pip install --index-url https://test.pypi.org/simple sageworks\n</code></pre>"},{"location":"admin/pypi_release/#create-the-real-pypi-release","title":"Create the REAL PyPI Release","text":"<pre><code>twine upload dist/* -r pypi\n</code></pre>"},{"location":"admin/pypi_release/#push-any-possible-changes-to-github","title":"Push any possible changes to Github","text":"<pre><code>git push\n</code></pre>"},{"location":"api_classes/data_source/","title":"DataSource","text":"<p>DataSource: SageWorks DataSource API Class</p>"},{"location":"api_classes/data_source/#sageworks.api.data_source.DataSource","title":"<code>DataSource</code>","text":"<p>             Bases: <code>AthenaSource</code></p> <p>DataSource: SageWorks DataSource API Class</p> <p>Common Usage</p> <pre><code>    my_data = DataSource(name_of_source)\n    my_data.summary()\n    my_data.details()\n    my_features = my_data.to_features()\n</code></pre> Source code in <code>src/sageworks/api/data_source.py</code> <pre><code>class DataSource(AthenaSource):\n    \"\"\"DataSource: SageWorks DataSource API Class\n\n    **Common Usage**\n    ```\n        my_data = DataSource(name_of_source)\n        my_data.summary()\n        my_data.details()\n        my_features = my_data.to_features()\n    ```\n    \"\"\"\n\n    def __init__(self, source, name: str = None, tags: list = None):\n        \"\"\"\n        Initializes a new DataSource object.\n\n        Args:\n            source (str): The source of the data. This can be an S3 bucket, file path,\n                          DataFrame object, or an existing DataSource object.\n            name (str): The name of the data source. If not specified, an automatic name will be generated.\n            tags (list[str]): A list of tags associated with the data source. If not specified automatic tags will be generated.\n        \"\"\"\n        self.log = logging.getLogger(\"sageworks\")\n\n        # Load the source (S3, File, or Existing DataSource)\n        ds_name = extract_data_source_basename(source) if name is None else name\n        if ds_name == \"dataframe\":\n            msg = \"Set the 'name' argument in the constructor: DataSource(df, name='my_data')\"\n            self.log.critical(msg)\n            raise ValueError(msg)\n        tags = [ds_name] if tags is None else tags\n        self._load_source(source, ds_name, tags)\n\n        # Call superclass init\n        super().__init__(ds_name)\n\n    def to_features(\n        self, name: str = None, tags: list = None, id_column: str = None, event_time_column: str = None\n    ) -&gt; FeatureSet:\n        \"\"\"\n        Convert the DataSource to a FeatureSet\n\n        Args:\n            name (str): Set the name for feature set. If not specified, an automatic name will be generated\n            tags (list): Set the tags for the feature set. If not specified automatic tags will be generated.\n            id_column (str): Set the id column for the feature set. If not specified one will be generated.\n            event_time_column (str): Set the event time column for the feature set. If not specified one will be generated.\n\n        Returns:\n            FeatureSet: The FeatureSet created from the DataSource\n        \"\"\"\n\n        # Create the FeatureSet Name and Tags\n        fs_name = self.uuid.replace(\"_data\", \"\") + \"_features\" if name is None else name\n        tags = [fs_name] if tags is None else tags\n\n        # Transform the DataSource to a FeatureSet\n        data_to_features = DataToFeaturesLight(self.uuid, fs_name)\n        data_to_features.set_output_tags(tags)\n        data_to_features.transform(id_column=id_column, event_time_column=event_time_column)\n\n        # Return the FeatureSet (which will now be up-to-date)\n        return FeatureSet(fs_name)\n\n    def _load_source(self, source: str, name: str, tags: list):\n        \"\"\"Load the source of the data\"\"\"\n        self.log.info(f\"Loading source: {source}...\")\n\n        # Pandas DataFrame Source\n        if isinstance(source, pd.DataFrame):\n            my_loader = PandasToData(name)\n            my_loader.set_input(source)\n            my_loader.set_output_tags(tags)\n            my_loader.transform()\n\n        # S3 Source\n        source = source if isinstance(source, str) else str(source)\n        if source.startswith(\"s3://\"):\n            my_loader = S3ToDataSourceLight(source, name)\n            my_loader.set_output_tags(tags)\n            my_loader.transform()\n\n        # File Source\n        elif os.path.isfile(source):\n            my_loader = CSVToDataSource(source, name)\n            my_loader.set_output_tags(tags)\n            my_loader.transform()\n</code></pre>"},{"location":"api_classes/data_source/#sageworks.api.data_source.DataSource.__init__","title":"<code>__init__(source, name=None, tags=None)</code>","text":"<p>Initializes a new DataSource object.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source of the data. This can be an S3 bucket, file path,           DataFrame object, or an existing DataSource object.</p> required <code>name</code> <code>str</code> <p>The name of the data source. If not specified, an automatic name will be generated.</p> <code>None</code> <code>tags</code> <code>list[str]</code> <p>A list of tags associated with the data source. If not specified automatic tags will be generated.</p> <code>None</code> Source code in <code>src/sageworks/api/data_source.py</code> <pre><code>def __init__(self, source, name: str = None, tags: list = None):\n    \"\"\"\n    Initializes a new DataSource object.\n\n    Args:\n        source (str): The source of the data. This can be an S3 bucket, file path,\n                      DataFrame object, or an existing DataSource object.\n        name (str): The name of the data source. If not specified, an automatic name will be generated.\n        tags (list[str]): A list of tags associated with the data source. If not specified automatic tags will be generated.\n    \"\"\"\n    self.log = logging.getLogger(\"sageworks\")\n\n    # Load the source (S3, File, or Existing DataSource)\n    ds_name = extract_data_source_basename(source) if name is None else name\n    if ds_name == \"dataframe\":\n        msg = \"Set the 'name' argument in the constructor: DataSource(df, name='my_data')\"\n        self.log.critical(msg)\n        raise ValueError(msg)\n    tags = [ds_name] if tags is None else tags\n    self._load_source(source, ds_name, tags)\n\n    # Call superclass init\n    super().__init__(ds_name)\n</code></pre>"},{"location":"api_classes/data_source/#sageworks.api.data_source.DataSource.to_features","title":"<code>to_features(name=None, tags=None, id_column=None, event_time_column=None)</code>","text":"<p>Convert the DataSource to a FeatureSet</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Set the name for feature set. If not specified, an automatic name will be generated</p> <code>None</code> <code>tags</code> <code>list</code> <p>Set the tags for the feature set. If not specified automatic tags will be generated.</p> <code>None</code> <code>id_column</code> <code>str</code> <p>Set the id column for the feature set. If not specified one will be generated.</p> <code>None</code> <code>event_time_column</code> <code>str</code> <p>Set the event time column for the feature set. If not specified one will be generated.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>FeatureSet</code> <code>FeatureSet</code> <p>The FeatureSet created from the DataSource</p> Source code in <code>src/sageworks/api/data_source.py</code> <pre><code>def to_features(\n    self, name: str = None, tags: list = None, id_column: str = None, event_time_column: str = None\n) -&gt; FeatureSet:\n    \"\"\"\n    Convert the DataSource to a FeatureSet\n\n    Args:\n        name (str): Set the name for feature set. If not specified, an automatic name will be generated\n        tags (list): Set the tags for the feature set. If not specified automatic tags will be generated.\n        id_column (str): Set the id column for the feature set. If not specified one will be generated.\n        event_time_column (str): Set the event time column for the feature set. If not specified one will be generated.\n\n    Returns:\n        FeatureSet: The FeatureSet created from the DataSource\n    \"\"\"\n\n    # Create the FeatureSet Name and Tags\n    fs_name = self.uuid.replace(\"_data\", \"\") + \"_features\" if name is None else name\n    tags = [fs_name] if tags is None else tags\n\n    # Transform the DataSource to a FeatureSet\n    data_to_features = DataToFeaturesLight(self.uuid, fs_name)\n    data_to_features.set_output_tags(tags)\n    data_to_features.transform(id_column=id_column, event_time_column=event_time_column)\n\n    # Return the FeatureSet (which will now be up-to-date)\n    return FeatureSet(fs_name)\n</code></pre>"},{"location":"api_classes/data_source/#examples","title":"Examples","text":"<p>Example Description 1</p> <pre><code>example code 1\n</code></pre> <p>Example Description 2</p> <pre><code>example code 2\n</code></pre> <p>Not Finding a particular method?</p> <p>The SageWorks API Classes use 'Core' Classes Internally, so you can lookup all the methods in SageWorks Core Classes</p>"},{"location":"api_classes/endpoint/","title":"Endpoint","text":"<p>Endpoint: SageWorks Endpoint API Class</p>"},{"location":"api_classes/endpoint/#sageworks.api.endpoint.Endpoint","title":"<code>Endpoint</code>","text":"<p>             Bases: <code>EndpointCore</code></p> <p>Endpoint: SageWorks Endpoint API Class</p> <p>Common Usage</p> <pre><code>    my_endpoint = Endpoint(name)\n    my_endpoint.summary()\n    my_endpoint.details()\n    my_endpoint.predict(df)\n</code></pre> Source code in <code>src/sageworks/api/endpoint.py</code> <pre><code>class Endpoint(EndpointCore):\n    \"\"\"Endpoint: SageWorks Endpoint API Class\n\n    **Common Usage**\n    ```\n        my_endpoint = Endpoint(name)\n        my_endpoint.summary()\n        my_endpoint.details()\n        my_endpoint.predict(df)\n    ```\n    \"\"\"\n\n    def __init__(self, name):\n        \"\"\"Endpoint Initialization\n\n        Args:\n            name (str): The name of the Endpoint\n        \"\"\"\n        # Call superclass init\n        super().__init__(name)\n</code></pre>"},{"location":"api_classes/endpoint/#sageworks.api.endpoint.Endpoint.__init__","title":"<code>__init__(name)</code>","text":"<p>Endpoint Initialization</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the Endpoint</p> required Source code in <code>src/sageworks/api/endpoint.py</code> <pre><code>def __init__(self, name):\n    \"\"\"Endpoint Initialization\n\n    Args:\n        name (str): The name of the Endpoint\n    \"\"\"\n    # Call superclass init\n    super().__init__(name)\n</code></pre>"},{"location":"api_classes/endpoint/#examples","title":"Examples","text":"<p>Example Description 1</p> <pre><code>example code 1\n</code></pre> <p>Example Description 2</p> <pre><code>example code 2\n</code></pre> <p>Not Finding a particular method?</p> <p>The SageWorks API Classes use 'Core' Classes Internally, so you can lookup all the methods in</p> <p>SageWorks Core Classes</p>"},{"location":"api_classes/feature_set/","title":"FeatureSet","text":"<p>FeatureSet: SageWorks FeatureSet API Class</p>"},{"location":"api_classes/feature_set/#sageworks.api.feature_set.FeatureSet","title":"<code>FeatureSet</code>","text":"<p>             Bases: <code>FeatureSetCore</code></p> <p>FeatureSet: SageWorks FeatureSet API Class</p> <p>Common Usage</p> <pre><code>    my_features = FeatureSet(name)\n    my_features.summary()\n    my_features.details()\n    my_features.to_model()\n</code></pre> Source code in <code>src/sageworks/api/feature_set.py</code> <pre><code>class FeatureSet(FeatureSetCore):\n    \"\"\"FeatureSet: SageWorks FeatureSet API Class\n\n    **Common Usage**\n    ```\n        my_features = FeatureSet(name)\n        my_features.summary()\n        my_features.details()\n        my_features.to_model()\n    ```\n    \"\"\"\n\n    def __init__(self, name):\n        \"\"\"FeatureSet Initialization\n\n        Args:\n            name (str): The name of the FeatureSet\n        \"\"\"\n        # Call superclass init\n        super().__init__(name)\n\n    def to_model(\n        self,\n        model_type: ModelType,\n        target_column: str,\n        name: str = None,\n        tags: list = None,\n        description: str = None,\n    ) -&gt; Model:\n        \"\"\"Create a Model from the FeatureSet\n\n        Args:\n            model_type (ModelType): The type of model to create (See sageworks.model.ModelType)\n            target_column (str): The target column for the model (use None for unsupervised model)\n            name (str): Set the name for the model. If not specified, an automatic name will be generated\n            tags (list): Set the tags for the model.  If not specified automatic tags will be generated.\n            description (str): Set the description for the model. If not specified an automatic description will be generated.\n\n        Returns:\n            Model: The Model created from the FeatureSet\n        \"\"\"\n\n        # Create the Model Name and Tags\n        model_name = self.uuid.replace(\"_features\", \"\").replace(\"_\", \"-\") + \"-model\" if name is None else name\n        tags = [model_name] if tags is None else tags\n\n        # Transform the FeatureSet into a Model\n        features_to_model = FeaturesToModel(self.uuid, model_name, model_type=model_type)\n        features_to_model.set_output_tags(tags)\n        features_to_model.transform(target_column=target_column, description=description)\n\n        # Return the Model\n        return Model(model_name)\n</code></pre>"},{"location":"api_classes/feature_set/#sageworks.api.feature_set.FeatureSet.__init__","title":"<code>__init__(name)</code>","text":"<p>FeatureSet Initialization</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the FeatureSet</p> required Source code in <code>src/sageworks/api/feature_set.py</code> <pre><code>def __init__(self, name):\n    \"\"\"FeatureSet Initialization\n\n    Args:\n        name (str): The name of the FeatureSet\n    \"\"\"\n    # Call superclass init\n    super().__init__(name)\n</code></pre>"},{"location":"api_classes/feature_set/#sageworks.api.feature_set.FeatureSet.to_model","title":"<code>to_model(model_type, target_column, name=None, tags=None, description=None)</code>","text":"<p>Create a Model from the FeatureSet</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>ModelType</code> <p>The type of model to create (See sageworks.model.ModelType)</p> required <code>target_column</code> <code>str</code> <p>The target column for the model (use None for unsupervised model)</p> required <code>name</code> <code>str</code> <p>Set the name for the model. If not specified, an automatic name will be generated</p> <code>None</code> <code>tags</code> <code>list</code> <p>Set the tags for the model.  If not specified automatic tags will be generated.</p> <code>None</code> <code>description</code> <code>str</code> <p>Set the description for the model. If not specified an automatic description will be generated.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>The Model created from the FeatureSet</p> Source code in <code>src/sageworks/api/feature_set.py</code> <pre><code>def to_model(\n    self,\n    model_type: ModelType,\n    target_column: str,\n    name: str = None,\n    tags: list = None,\n    description: str = None,\n) -&gt; Model:\n    \"\"\"Create a Model from the FeatureSet\n\n    Args:\n        model_type (ModelType): The type of model to create (See sageworks.model.ModelType)\n        target_column (str): The target column for the model (use None for unsupervised model)\n        name (str): Set the name for the model. If not specified, an automatic name will be generated\n        tags (list): Set the tags for the model.  If not specified automatic tags will be generated.\n        description (str): Set the description for the model. If not specified an automatic description will be generated.\n\n    Returns:\n        Model: The Model created from the FeatureSet\n    \"\"\"\n\n    # Create the Model Name and Tags\n    model_name = self.uuid.replace(\"_features\", \"\").replace(\"_\", \"-\") + \"-model\" if name is None else name\n    tags = [model_name] if tags is None else tags\n\n    # Transform the FeatureSet into a Model\n    features_to_model = FeaturesToModel(self.uuid, model_name, model_type=model_type)\n    features_to_model.set_output_tags(tags)\n    features_to_model.transform(target_column=target_column, description=description)\n\n    # Return the Model\n    return Model(model_name)\n</code></pre>"},{"location":"api_classes/feature_set/#examples","title":"Examples","text":"<p>Example Description 1</p> <pre><code>example code 1\n</code></pre> <p>Example Description 2</p> <pre><code>example code 2\n</code></pre> <p>Not Finding a particular method?</p> <p>The SageWorks API Classes use 'Core' Classes Internally, so you can lookup all the methods in SageWorks Core Classes</p>"},{"location":"api_classes/model/","title":"Model","text":"<p>Model: SageWorks Model API Class</p>"},{"location":"api_classes/model/#sageworks.api.model.Model","title":"<code>Model</code>","text":"<p>             Bases: <code>ModelCore</code></p> <p>Model: SageWorks Model API Class</p> <p>Common Usage</p> <pre><code>    my_features = Model(name)\n    my_features.summary()\n    my_features.details()\n    my_features.to_endpoint()\n</code></pre> Source code in <code>src/sageworks/api/model.py</code> <pre><code>class Model(ModelCore):\n    \"\"\"Model: SageWorks Model API Class\n\n    **Common Usage**\n    ```\n        my_features = Model(name)\n        my_features.summary()\n        my_features.details()\n        my_features.to_endpoint()\n    ```\n    \"\"\"\n\n    def __init__(self, name):\n        \"\"\"Model Initialization\n        Args:\n            name (str): The name of the Model\n        \"\"\"\n        # Call superclass init\n        super().__init__(name)\n\n    def to_endpoint(self, name: str = None, tags: list = None, serverless: bool = True) -&gt; Endpoint:\n        \"\"\"Create an Endpoint from the Model\n\n        Args:\n            name (str): Set the name for the endpoint. If not specified, an automatic name will be generated\n            tags (list): Set the tags for the endpoint. If not specified automatic tags will be generated.\n            serverless (bool): Set the endpoint to be serverless (default: True)\n\n        Returns:\n            Endpoint: The Endpoint created from the Model\n        \"\"\"\n\n        # Create the Endpoint Name and Tags\n        endpoint_name = self.uuid.replace(\"-model\", \"\") + \"-end\" if name is None else name\n        tags = [endpoint_name] if tags is None else tags\n\n        # Create an Endpoint from the Model\n        model_to_endpoint = ModelToEndpoint(self.uuid, endpoint_name, serverless=serverless)\n        model_to_endpoint.set_output_tags(tags)\n        model_to_endpoint.transform()\n\n        # Return the Endpoint\n        return Endpoint(endpoint_name)\n</code></pre>"},{"location":"api_classes/model/#sageworks.api.model.Model.__init__","title":"<code>__init__(name)</code>","text":"<p>Model Initialization Args:     name (str): The name of the Model</p> Source code in <code>src/sageworks/api/model.py</code> <pre><code>def __init__(self, name):\n    \"\"\"Model Initialization\n    Args:\n        name (str): The name of the Model\n    \"\"\"\n    # Call superclass init\n    super().__init__(name)\n</code></pre>"},{"location":"api_classes/model/#sageworks.api.model.Model.to_endpoint","title":"<code>to_endpoint(name=None, tags=None, serverless=True)</code>","text":"<p>Create an Endpoint from the Model</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Set the name for the endpoint. If not specified, an automatic name will be generated</p> <code>None</code> <code>tags</code> <code>list</code> <p>Set the tags for the endpoint. If not specified automatic tags will be generated.</p> <code>None</code> <code>serverless</code> <code>bool</code> <p>Set the endpoint to be serverless (default: True)</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Endpoint</code> <code>Endpoint</code> <p>The Endpoint created from the Model</p> Source code in <code>src/sageworks/api/model.py</code> <pre><code>def to_endpoint(self, name: str = None, tags: list = None, serverless: bool = True) -&gt; Endpoint:\n    \"\"\"Create an Endpoint from the Model\n\n    Args:\n        name (str): Set the name for the endpoint. If not specified, an automatic name will be generated\n        tags (list): Set the tags for the endpoint. If not specified automatic tags will be generated.\n        serverless (bool): Set the endpoint to be serverless (default: True)\n\n    Returns:\n        Endpoint: The Endpoint created from the Model\n    \"\"\"\n\n    # Create the Endpoint Name and Tags\n    endpoint_name = self.uuid.replace(\"-model\", \"\") + \"-end\" if name is None else name\n    tags = [endpoint_name] if tags is None else tags\n\n    # Create an Endpoint from the Model\n    model_to_endpoint = ModelToEndpoint(self.uuid, endpoint_name, serverless=serverless)\n    model_to_endpoint.set_output_tags(tags)\n    model_to_endpoint.transform()\n\n    # Return the Endpoint\n    return Endpoint(endpoint_name)\n</code></pre>"},{"location":"api_classes/model/#examples","title":"Examples","text":"<p>Example Description 1</p> <pre><code>example code 1\n</code></pre> <p>Example Description 2</p> <pre><code>example code 2\n</code></pre> <p>Not Finding a particular method?</p> <p>The SageWorks API Classes use 'Core' Classes Internally, so you can lookup all the methods in SageWorks Core Classes</p>"},{"location":"api_classes/overview/","title":"Overview","text":"<p>Just Getting Started?</p> <p>You're in the right place, the SageWorks API Classes are the best way to get started with SageWorks!</p> <p>Welcome to the SageWorks API Classes</p> <p>These class provide high-level APIs for the SageWorks package, offering easy access to its core classes:</p> <ul> <li>DataSource: Manages AWS Data Catalog and Athena</li> <li>FeatureSet: Manages AWS Feature Store and Feature Groups</li> <li>Model: Manages the training and deployment of AWS Model Groups and Packages</li> <li>Endpoint: Manages the deployment and invocations/inference on AWS Endpoints</li> </ul> <p>Not Finding a particular method?</p> <p>The SageWorks API Classes use 'Core' Classes Internally, so you can lookup all the methods in SageWorks Core Classes</p>"},{"location":"aws_setup/initial_setup/","title":"Initial AWS Setup","text":"<p>AWS Setup can be a bit complex</p> <p>Setting up SageWorks with AWS can be a bit complex, but you only have to do it once and SageWorks tries to make it straight forward. If you have any troubles at all feel free to contact us a email:sageworks@supercowpowers.com or on Discord and we'll help you get started.</p>"},{"location":"core_classes/overview/","title":"Core Classes","text":"<p>The SageWorks Core Classes provide more control but are more complex</p> <p>These classes interact with many of the AWS service details. They try to balance usablility while maintaining a level of control and refinement over the AWS ML Pipline. For user oriented classes please see SageWorks API Classes</p> <p>Welcome to the SageWorks Core Classes</p> <p>These class provide low-level APIs for the SageWorks package, these classes directly interface with the AWS Sagemaker Pipeline interfaces and have a large number of methods with some complexity.</p>"},{"location":"core_classes/overview/#artifacts","title":"Artifacts","text":"<p>SageWorks Artifact Classes</p>"},{"location":"core_classes/overview/#transforms","title":"Transforms","text":"<p>SageWorks Artifact Classes</p>"},{"location":"core_classes/artifacts/artifacts/","title":"SageWorks Artifacts","text":""},{"location":"core_classes/transforms/data_loaders/","title":"DataLoaders","text":"<p>S3ToDataSourceLight: Class to move LIGHT S3 Files into a SageWorks DataSource</p> <p>CSVToDataSource: Class to move local CSV Files into a SageWorks DataSource</p> <p>JSONToDataSource: Class to move local JSON Files into a SageWorks DataSource</p> <p>S3HeavyToDataSource: Class to move HEAVY S3 Files into a SageWorks DataSource</p>"},{"location":"core_classes/transforms/data_loaders/#sageworks.core.transforms.data_loaders.light.s3_to_data_source_light.S3ToDataSourceLight","title":"<code>S3ToDataSourceLight</code>","text":"<p>             Bases: <code>Transform</code></p> <p>S3ToDataSourceLight: Class to move LIGHT S3 Files into a SageWorks DataSource</p> Common Usage <p>s3_to_data = S3ToDataSourceLight(s3_path, data_uuid, datatype=\"csv/json\") s3_to_data.set_output_tags([\"abalone\", \"whatever\"]) s3_to_data.transform()</p> Source code in <code>src/sageworks/core/transforms/data_loaders/light/s3_to_data_source_light.py</code> <pre><code>class S3ToDataSourceLight(Transform):\n    \"\"\"S3ToDataSourceLight: Class to move LIGHT S3 Files into a SageWorks DataSource\n\n    Common Usage:\n        s3_to_data = S3ToDataSourceLight(s3_path, data_uuid, datatype=\"csv/json\")\n        s3_to_data.set_output_tags([\"abalone\", \"whatever\"])\n        s3_to_data.transform()\n    \"\"\"\n\n    def __init__(self, s3_path: str, data_uuid: str, datatype: str = \"csv\"):\n        \"\"\"S3ToDataSourceLight Initialization\n        Args:\n            s3_path (str): The S3 Path to the file to be transformed\n            data_uuid (str): The UUID of the SageWorks DataSource to be created\n            datatype (str): The datatype of the file to be transformed (defaults to \"csv\")\n        \"\"\"\n\n        # Call superclass init\n        super().__init__(s3_path, data_uuid)\n\n        # Set up all my instance attributes\n        self.input_type = TransformInput.S3_OBJECT\n        self.output_type = TransformOutput.DATA_SOURCE\n        self.datatype = datatype\n\n    def input_size_mb(self) -&gt; int:\n        \"\"\"Get the size of the input S3 object in MBytes\"\"\"\n        size_in_bytes = wr.s3.size_objects(self.input_uuid, boto3_session=self.boto_session)[self.input_uuid]\n        size_in_mb = round(size_in_bytes / 1_000_000)\n        return size_in_mb\n\n    def transform_impl(self, overwrite: bool = True):\n        \"\"\"Convert the S3 CSV data into Parquet Format in the SageWorks Data Sources Bucket, and\n        store the information about the data to the AWS Data Catalog sageworks database\n        \"\"\"\n\n        # Sanity Check for S3 Object size\n        object_megabytes = self.input_size_mb()\n        if object_megabytes &gt; 100:\n            self.log.error(f\"S3 Object too big ({object_megabytes} MBytes): Use the S3ToDataSourceHeavy class!\")\n            return\n\n        # Read in the S3 CSV as a Pandas DataFrame\n        if self.datatype == \"csv\":\n            df = wr.s3.read_csv(self.input_uuid, low_memory=False, boto3_session=self.boto_session)\n        else:\n            df = wr.s3.read_json(self.input_uuid, lines=True, boto3_session=self.boto_session)\n\n        # Temporary hack to limit the number of columns in the dataframe\n        if len(df.columns) &gt; 40:\n            self.log.warning(f\"{self.input_uuid} Too Many Columns! Talk to SageWorks Support...\")\n\n        # Convert object columns before sending to SageWorks Data Source\n        df = convert_object_columns(df)\n\n        # Use the SageWorks Pandas to Data Source class\n        pandas_to_data = PandasToData(self.output_uuid)\n        pandas_to_data.set_input(df)\n        pandas_to_data.set_output_tags(self.output_tags)\n        pandas_to_data.add_output_meta(self.output_meta)\n        pandas_to_data.transform()\n\n        # Report the transformation results\n        self.log.info(f\"{self.input_uuid} --&gt;  DataSource: {self.output_uuid} Complete!\")\n\n    def post_transform(self, **kwargs):\n        \"\"\"Post-Transform: Calling make_ready() on the DataSource\"\"\"\n        self.log.info(\"Post-Transform: Calling make_ready() on the DataSource...\")\n\n        # Okay, lets wait just a bit for the\n        output_data_source = DataSourceFactory(self.output_uuid, force_refresh=True)\n        output_data_source.set_status(\"initializing\")\n\n        # Call the FeatureSet make_ready method to compute a bunch of EDA stuff\n        output_data_source.make_ready()\n</code></pre>"},{"location":"core_classes/transforms/data_loaders/#sageworks.core.transforms.data_loaders.light.s3_to_data_source_light.S3ToDataSourceLight.__init__","title":"<code>__init__(s3_path, data_uuid, datatype='csv')</code>","text":"<p>S3ToDataSourceLight Initialization Args:     s3_path (str): The S3 Path to the file to be transformed     data_uuid (str): The UUID of the SageWorks DataSource to be created     datatype (str): The datatype of the file to be transformed (defaults to \"csv\")</p> Source code in <code>src/sageworks/core/transforms/data_loaders/light/s3_to_data_source_light.py</code> <pre><code>def __init__(self, s3_path: str, data_uuid: str, datatype: str = \"csv\"):\n    \"\"\"S3ToDataSourceLight Initialization\n    Args:\n        s3_path (str): The S3 Path to the file to be transformed\n        data_uuid (str): The UUID of the SageWorks DataSource to be created\n        datatype (str): The datatype of the file to be transformed (defaults to \"csv\")\n    \"\"\"\n\n    # Call superclass init\n    super().__init__(s3_path, data_uuid)\n\n    # Set up all my instance attributes\n    self.input_type = TransformInput.S3_OBJECT\n    self.output_type = TransformOutput.DATA_SOURCE\n    self.datatype = datatype\n</code></pre>"},{"location":"core_classes/transforms/data_loaders/#sageworks.core.transforms.data_loaders.light.s3_to_data_source_light.S3ToDataSourceLight.input_size_mb","title":"<code>input_size_mb()</code>","text":"<p>Get the size of the input S3 object in MBytes</p> Source code in <code>src/sageworks/core/transforms/data_loaders/light/s3_to_data_source_light.py</code> <pre><code>def input_size_mb(self) -&gt; int:\n    \"\"\"Get the size of the input S3 object in MBytes\"\"\"\n    size_in_bytes = wr.s3.size_objects(self.input_uuid, boto3_session=self.boto_session)[self.input_uuid]\n    size_in_mb = round(size_in_bytes / 1_000_000)\n    return size_in_mb\n</code></pre>"},{"location":"core_classes/transforms/data_loaders/#sageworks.core.transforms.data_loaders.light.s3_to_data_source_light.S3ToDataSourceLight.post_transform","title":"<code>post_transform(**kwargs)</code>","text":"<p>Post-Transform: Calling make_ready() on the DataSource</p> Source code in <code>src/sageworks/core/transforms/data_loaders/light/s3_to_data_source_light.py</code> <pre><code>def post_transform(self, **kwargs):\n    \"\"\"Post-Transform: Calling make_ready() on the DataSource\"\"\"\n    self.log.info(\"Post-Transform: Calling make_ready() on the DataSource...\")\n\n    # Okay, lets wait just a bit for the\n    output_data_source = DataSourceFactory(self.output_uuid, force_refresh=True)\n    output_data_source.set_status(\"initializing\")\n\n    # Call the FeatureSet make_ready method to compute a bunch of EDA stuff\n    output_data_source.make_ready()\n</code></pre>"},{"location":"core_classes/transforms/data_loaders/#sageworks.core.transforms.data_loaders.light.s3_to_data_source_light.S3ToDataSourceLight.transform_impl","title":"<code>transform_impl(overwrite=True)</code>","text":"<p>Convert the S3 CSV data into Parquet Format in the SageWorks Data Sources Bucket, and store the information about the data to the AWS Data Catalog sageworks database</p> Source code in <code>src/sageworks/core/transforms/data_loaders/light/s3_to_data_source_light.py</code> <pre><code>def transform_impl(self, overwrite: bool = True):\n    \"\"\"Convert the S3 CSV data into Parquet Format in the SageWorks Data Sources Bucket, and\n    store the information about the data to the AWS Data Catalog sageworks database\n    \"\"\"\n\n    # Sanity Check for S3 Object size\n    object_megabytes = self.input_size_mb()\n    if object_megabytes &gt; 100:\n        self.log.error(f\"S3 Object too big ({object_megabytes} MBytes): Use the S3ToDataSourceHeavy class!\")\n        return\n\n    # Read in the S3 CSV as a Pandas DataFrame\n    if self.datatype == \"csv\":\n        df = wr.s3.read_csv(self.input_uuid, low_memory=False, boto3_session=self.boto_session)\n    else:\n        df = wr.s3.read_json(self.input_uuid, lines=True, boto3_session=self.boto_session)\n\n    # Temporary hack to limit the number of columns in the dataframe\n    if len(df.columns) &gt; 40:\n        self.log.warning(f\"{self.input_uuid} Too Many Columns! Talk to SageWorks Support...\")\n\n    # Convert object columns before sending to SageWorks Data Source\n    df = convert_object_columns(df)\n\n    # Use the SageWorks Pandas to Data Source class\n    pandas_to_data = PandasToData(self.output_uuid)\n    pandas_to_data.set_input(df)\n    pandas_to_data.set_output_tags(self.output_tags)\n    pandas_to_data.add_output_meta(self.output_meta)\n    pandas_to_data.transform()\n\n    # Report the transformation results\n    self.log.info(f\"{self.input_uuid} --&gt;  DataSource: {self.output_uuid} Complete!\")\n</code></pre>"},{"location":"core_classes/transforms/data_loaders/#sageworks.core.transforms.data_loaders.light.csv_to_data_source.CSVToDataSource","title":"<code>CSVToDataSource</code>","text":"<p>             Bases: <code>Transform</code></p> <p>CSVToDataSource: Class to move local CSV Files into a SageWorks DataSource</p> Common Usage <p>csv_to_data = CSVToDataSource(csv_file_path, data_uuid) csv_to_data.set_output_tags([\"abalone\", \"csv\", \"whatever\"]) csv_to_data.transform()</p> Source code in <code>src/sageworks/core/transforms/data_loaders/light/csv_to_data_source.py</code> <pre><code>class CSVToDataSource(Transform):\n    \"\"\"CSVToDataSource: Class to move local CSV Files into a SageWorks DataSource\n\n    Common Usage:\n        csv_to_data = CSVToDataSource(csv_file_path, data_uuid)\n        csv_to_data.set_output_tags([\"abalone\", \"csv\", \"whatever\"])\n        csv_to_data.transform()\n    \"\"\"\n\n    def __init__(self, csv_file_path: str, data_uuid: str):\n        \"\"\"CSVToDataSource: Class to move local CSV Files into a SageWorks DataSource\"\"\"\n\n        # Call superclass init\n        super().__init__(csv_file_path, data_uuid)\n\n        # Set up all my instance attributes\n        self.input_type = TransformInput.LOCAL_FILE\n        self.output_type = TransformOutput.DATA_SOURCE\n\n    def transform_impl(self, overwrite: bool = True):\n        \"\"\"Convert the local CSV file into Parquet Format in the SageWorks Data Sources Bucket, and\n        store the information about the data to the AWS Data Catalog sageworks database\n        \"\"\"\n\n        # Report the transformation initiation\n        csv_file = os.path.basename(self.input_uuid)\n        self.log.info(f\"Starting {csv_file} --&gt;  DataSource: {self.output_uuid}...\")\n\n        # Read in the Local CSV as a Pandas DataFrame\n        df = pd.read_csv(self.input_uuid, low_memory=False)\n        df = convert_object_columns(df)\n\n        # Use the SageWorks Pandas to Data Source class\n        pandas_to_data = PandasToData(self.output_uuid)\n        pandas_to_data.set_input(df)\n        pandas_to_data.set_output_tags(self.output_tags)\n        pandas_to_data.add_output_meta(self.output_meta)\n        pandas_to_data.transform()\n\n        # Report the transformation results\n        self.log.info(f\"{csv_file} --&gt;  DataSource: {self.output_uuid} Complete!\")\n\n    def post_transform(self, **kwargs):\n        \"\"\"Post-Transform: Calling make_ready() on the DataSource\"\"\"\n        self.log.info(\"Post-Transform: Calling make_ready() on the DataSource...\")\n\n        # Okay, lets wait just a bit for the\n        output_data_source = DataSourceFactory(self.output_uuid, force_refresh=True)\n        output_data_source.set_status(\"initializing\")\n\n        # Call the FeatureSet make_ready method to compute a bunch of EDA stuff\n        output_data_source.make_ready()\n</code></pre>"},{"location":"core_classes/transforms/data_loaders/#sageworks.core.transforms.data_loaders.light.csv_to_data_source.CSVToDataSource.__init__","title":"<code>__init__(csv_file_path, data_uuid)</code>","text":"<p>CSVToDataSource: Class to move local CSV Files into a SageWorks DataSource</p> Source code in <code>src/sageworks/core/transforms/data_loaders/light/csv_to_data_source.py</code> <pre><code>def __init__(self, csv_file_path: str, data_uuid: str):\n    \"\"\"CSVToDataSource: Class to move local CSV Files into a SageWorks DataSource\"\"\"\n\n    # Call superclass init\n    super().__init__(csv_file_path, data_uuid)\n\n    # Set up all my instance attributes\n    self.input_type = TransformInput.LOCAL_FILE\n    self.output_type = TransformOutput.DATA_SOURCE\n</code></pre>"},{"location":"core_classes/transforms/data_loaders/#sageworks.core.transforms.data_loaders.light.csv_to_data_source.CSVToDataSource.post_transform","title":"<code>post_transform(**kwargs)</code>","text":"<p>Post-Transform: Calling make_ready() on the DataSource</p> Source code in <code>src/sageworks/core/transforms/data_loaders/light/csv_to_data_source.py</code> <pre><code>def post_transform(self, **kwargs):\n    \"\"\"Post-Transform: Calling make_ready() on the DataSource\"\"\"\n    self.log.info(\"Post-Transform: Calling make_ready() on the DataSource...\")\n\n    # Okay, lets wait just a bit for the\n    output_data_source = DataSourceFactory(self.output_uuid, force_refresh=True)\n    output_data_source.set_status(\"initializing\")\n\n    # Call the FeatureSet make_ready method to compute a bunch of EDA stuff\n    output_data_source.make_ready()\n</code></pre>"},{"location":"core_classes/transforms/data_loaders/#sageworks.core.transforms.data_loaders.light.csv_to_data_source.CSVToDataSource.transform_impl","title":"<code>transform_impl(overwrite=True)</code>","text":"<p>Convert the local CSV file into Parquet Format in the SageWorks Data Sources Bucket, and store the information about the data to the AWS Data Catalog sageworks database</p> Source code in <code>src/sageworks/core/transforms/data_loaders/light/csv_to_data_source.py</code> <pre><code>def transform_impl(self, overwrite: bool = True):\n    \"\"\"Convert the local CSV file into Parquet Format in the SageWorks Data Sources Bucket, and\n    store the information about the data to the AWS Data Catalog sageworks database\n    \"\"\"\n\n    # Report the transformation initiation\n    csv_file = os.path.basename(self.input_uuid)\n    self.log.info(f\"Starting {csv_file} --&gt;  DataSource: {self.output_uuid}...\")\n\n    # Read in the Local CSV as a Pandas DataFrame\n    df = pd.read_csv(self.input_uuid, low_memory=False)\n    df = convert_object_columns(df)\n\n    # Use the SageWorks Pandas to Data Source class\n    pandas_to_data = PandasToData(self.output_uuid)\n    pandas_to_data.set_input(df)\n    pandas_to_data.set_output_tags(self.output_tags)\n    pandas_to_data.add_output_meta(self.output_meta)\n    pandas_to_data.transform()\n\n    # Report the transformation results\n    self.log.info(f\"{csv_file} --&gt;  DataSource: {self.output_uuid} Complete!\")\n</code></pre>"},{"location":"core_classes/transforms/data_loaders/#sageworks.core.transforms.data_loaders.light.json_to_data_source.JSONToDataSource","title":"<code>JSONToDataSource</code>","text":"<p>             Bases: <code>Transform</code></p> <p>JSONToDataSource: Class to move local JSON Files into a SageWorks DataSource</p> Common Usage <p>json_to_data = JSONToDataSource(json_file_path, data_uuid) json_to_data.set_output_tags([\"abalone\", \"json\", \"whatever\"]) json_to_data.transform()</p> Source code in <code>src/sageworks/core/transforms/data_loaders/light/json_to_data_source.py</code> <pre><code>class JSONToDataSource(Transform):\n    \"\"\"JSONToDataSource: Class to move local JSON Files into a SageWorks DataSource\n\n    Common Usage:\n        json_to_data = JSONToDataSource(json_file_path, data_uuid)\n        json_to_data.set_output_tags([\"abalone\", \"json\", \"whatever\"])\n        json_to_data.transform()\n    \"\"\"\n\n    def __init__(self, json_file_path: str, data_uuid: str):\n        \"\"\"JSONToDataSource: Class to move local JSON Files into a SageWorks DataSource\"\"\"\n\n        # Call superclass init\n        super().__init__(json_file_path, data_uuid)\n\n        # Set up all my instance attributes\n        self.input_type = TransformInput.LOCAL_FILE\n        self.output_type = TransformOutput.DATA_SOURCE\n\n    def transform_impl(self, overwrite: bool = True):\n        \"\"\"Convert the local JSON file into Parquet Format in the SageWorks Data Sources Bucket, and\n        store the information about the data to the AWS Data Catalog sageworks database\n        \"\"\"\n\n        # Report the transformation initiation\n        json_file = os.path.basename(self.input_uuid)\n        self.log.info(f\"Starting {json_file} --&gt;  DataSource: {self.output_uuid}...\")\n\n        # Read in the Local JSON as a Pandas DataFrame\n        df = pd.read_json(self.input_uuid, lines=True)\n\n        # Use the SageWorks Pandas to Data Source class\n        pandas_to_data = PandasToData(self.output_uuid)\n        pandas_to_data.set_input(df)\n        pandas_to_data.set_output_tags(self.output_tags)\n        pandas_to_data.add_output_meta(self.output_meta)\n        pandas_to_data.transform()\n\n        # Report the transformation results\n        self.log.info(f\"{json_file} --&gt;  DataSource: {self.output_uuid} Complete!\")\n\n    def post_transform(self, **kwargs):\n        \"\"\"Post-Transform: Calling make_ready() on the DataSource\"\"\"\n        self.log.info(\"Post-Transform: Calling make_ready() on the DataSource...\")\n\n        # Okay, lets wait just a bit for the\n        output_data_source = DataSourceFactory(self.output_uuid, force_refresh=True)\n        output_data_source.set_status(\"initializing\")\n\n        # Call the FeatureSet make_ready method to compute a bunch of EDA stuff\n        output_data_source.make_ready()\n</code></pre>"},{"location":"core_classes/transforms/data_loaders/#sageworks.core.transforms.data_loaders.light.json_to_data_source.JSONToDataSource.__init__","title":"<code>__init__(json_file_path, data_uuid)</code>","text":"<p>JSONToDataSource: Class to move local JSON Files into a SageWorks DataSource</p> Source code in <code>src/sageworks/core/transforms/data_loaders/light/json_to_data_source.py</code> <pre><code>def __init__(self, json_file_path: str, data_uuid: str):\n    \"\"\"JSONToDataSource: Class to move local JSON Files into a SageWorks DataSource\"\"\"\n\n    # Call superclass init\n    super().__init__(json_file_path, data_uuid)\n\n    # Set up all my instance attributes\n    self.input_type = TransformInput.LOCAL_FILE\n    self.output_type = TransformOutput.DATA_SOURCE\n</code></pre>"},{"location":"core_classes/transforms/data_loaders/#sageworks.core.transforms.data_loaders.light.json_to_data_source.JSONToDataSource.post_transform","title":"<code>post_transform(**kwargs)</code>","text":"<p>Post-Transform: Calling make_ready() on the DataSource</p> Source code in <code>src/sageworks/core/transforms/data_loaders/light/json_to_data_source.py</code> <pre><code>def post_transform(self, **kwargs):\n    \"\"\"Post-Transform: Calling make_ready() on the DataSource\"\"\"\n    self.log.info(\"Post-Transform: Calling make_ready() on the DataSource...\")\n\n    # Okay, lets wait just a bit for the\n    output_data_source = DataSourceFactory(self.output_uuid, force_refresh=True)\n    output_data_source.set_status(\"initializing\")\n\n    # Call the FeatureSet make_ready method to compute a bunch of EDA stuff\n    output_data_source.make_ready()\n</code></pre>"},{"location":"core_classes/transforms/data_loaders/#sageworks.core.transforms.data_loaders.light.json_to_data_source.JSONToDataSource.transform_impl","title":"<code>transform_impl(overwrite=True)</code>","text":"<p>Convert the local JSON file into Parquet Format in the SageWorks Data Sources Bucket, and store the information about the data to the AWS Data Catalog sageworks database</p> Source code in <code>src/sageworks/core/transforms/data_loaders/light/json_to_data_source.py</code> <pre><code>def transform_impl(self, overwrite: bool = True):\n    \"\"\"Convert the local JSON file into Parquet Format in the SageWorks Data Sources Bucket, and\n    store the information about the data to the AWS Data Catalog sageworks database\n    \"\"\"\n\n    # Report the transformation initiation\n    json_file = os.path.basename(self.input_uuid)\n    self.log.info(f\"Starting {json_file} --&gt;  DataSource: {self.output_uuid}...\")\n\n    # Read in the Local JSON as a Pandas DataFrame\n    df = pd.read_json(self.input_uuid, lines=True)\n\n    # Use the SageWorks Pandas to Data Source class\n    pandas_to_data = PandasToData(self.output_uuid)\n    pandas_to_data.set_input(df)\n    pandas_to_data.set_output_tags(self.output_tags)\n    pandas_to_data.add_output_meta(self.output_meta)\n    pandas_to_data.transform()\n\n    # Report the transformation results\n    self.log.info(f\"{json_file} --&gt;  DataSource: {self.output_uuid} Complete!\")\n</code></pre>"},{"location":"core_classes/transforms/data_loaders/#sageworks.core.transforms.data_loaders.heavy.s3_heavy_to_data_source.S3HeavyToDataSource","title":"<code>S3HeavyToDataSource</code>","text":"Source code in <code>src/sageworks/core/transforms/data_loaders/heavy/s3_heavy_to_data_source.py</code> <pre><code>class S3HeavyToDataSource:\n    def __init__(self, glue_context: GlueContext, input_uuid: str, output_uuid: str):\n        \"\"\"S3HeavyToDataSource: Class to move HEAVY S3 Files into a SageWorks DataSource\n        Args:\n            glue_context: GlueContext, AWS Glue Specific wrapper around SparkContext\n            input_uuid (str): The S3 Path to the files to be loaded\n            output_uuid (str): The UUID of the SageWorks DataSource to be created\n        \"\"\"\n        self.log = glue_context.get_logger()\n\n        # FIXME: Pull these from Parameter Store or Config\n        self.input_uuid = input_uuid\n        self.output_uuid = output_uuid\n        self.output_meta = {\"sageworks_input\": self.input_uuid}\n        sageworks_bucket = \"s3://sandbox-sageworks-artifacts\"\n        self.data_sources_s3_path = sageworks_bucket + \"/data-sources\"\n\n        # Our Spark Context\n        self.glue_context = glue_context\n\n    @staticmethod\n    def resolve_choice_fields(dyf):\n        # Get schema fields\n        schema_fields = dyf.schema().fields\n\n        # Collect choice fields\n        choice_fields = [(field.name, \"cast:long\") for field in schema_fields if field.dataType.typeName() == \"choice\"]\n        print(f\"Choice Fields: {choice_fields}\")\n\n        # If there are choice fields, resolve them\n        if choice_fields:\n            dyf = dyf.resolveChoice(specs=choice_fields)\n\n        return dyf\n\n    def timestamp_conversions(self, dyf: DynamicFrame, time_columns: list = []) -&gt; DynamicFrame:\n        \"\"\"Convert columns in the DynamicFrame to the correct data types\n        Args:\n            dyf (DynamicFrame): The DynamicFrame to convert\n            time_columns (list): A list of column names to convert to timestamp\n        Returns:\n            DynamicFrame: The converted DynamicFrame\n        \"\"\"\n\n        # Convert the timestamp columns to timestamp types\n        spark_df = dyf.toDF()\n        for column in time_columns:\n            spark_df = spark_df.withColumn(column, to_timestamp(col(column)))\n\n        # Convert the Spark DataFrame back to a Glue DynamicFrame and return\n        return DynamicFrame.fromDF(spark_df, self.glue_context, \"output_dyf\")\n\n    @staticmethod\n    def remove_periods_from_column_names(dyf: DynamicFrame) -&gt; DynamicFrame:\n        \"\"\"Remove periods from column names in the DynamicFrame\n        Args:\n            dyf (DynamicFrame): The DynamicFrame to convert\n        Returns:\n            DynamicFrame: The converted DynamicFrame\n        \"\"\"\n        # Extract the column names from the schema\n        old_column_names = [field.name for field in dyf.schema().fields]\n\n        # Create a new list of renamed column names\n        new_column_names = [name.replace(\".\", \"_\") for name in old_column_names]\n        print(old_column_names)\n        print(new_column_names)\n\n        # Create a new DynamicFrame with renamed columns\n        for c_old, c_new in zip(old_column_names, new_column_names):\n            dyf = dyf.rename_field(f\"`{c_old}`\", c_new)\n        return dyf\n\n    def transform(\n        self,\n        input_type: str = \"json\",\n        timestamp_columns: list = None,\n        output_format: str = \"parquet\",\n    ):\n        \"\"\"Convert the CSV or JSON data into Parquet Format in the SageWorks S3 Bucket, and\n        store the information about the data to the AWS Data Catalog sageworks database\n        Args:\n            input_type (str): The type of input files, either 'csv' or 'json'\n            timestamp_columns (list): A list of column names to convert to timestamp\n            output_format (str): The format of the output files, either 'parquet' or 'orc'\n        \"\"\"\n\n        # Add some tags here\n        tags = [\"heavy\"]\n\n        # Create the Output Parquet file S3 Storage Path\n        s3_storage_path = f\"{self.data_sources_s3_path}/{self.output_uuid}\"\n\n        # Read JSONL files from S3 and infer schema dynamically\n        self.log.info(f\"Reading JSONL files from {self.input_uuid}...\")\n        input_dyf = self.glue_context.create_dynamic_frame.from_options(\n            connection_type=\"s3\",\n            connection_options={\n                \"paths\": [self.input_uuid],\n                \"recurse\": True,\n                \"gzip\": True,\n            },\n            format=input_type,\n            # format_options={'jsonPath': 'auto'}, Look into this later\n        )\n        self.log.info(\"Incoming DataFrame...\")\n        input_dyf.show(5)\n        input_dyf.printSchema()\n\n        # Resolve Choice fields\n        resolved_dyf = self.resolve_choice_fields(input_dyf)\n\n        # The next couple of lines of code is for un-nesting any nested JSON\n        # Create a Dynamic Frame Collection (dfc)\n        dfc = Relationalize.apply(resolved_dyf, name=\"root\")\n\n        # Aggregate the collection into a single dynamic frame\n        output_dyf = dfc.select(\"root\")\n\n        print(\"Before TimeStamp Conversions\")\n        output_dyf.printSchema()\n\n        # Convert any timestamp columns\n        output_dyf = self.timestamp_conversions(output_dyf, timestamp_columns)\n\n        # Relationalize will put periods in the column names. This will cause\n        # problems later when we try to create a FeatureSet from this DataSource\n        output_dyf = self.remove_periods_from_column_names(output_dyf)\n\n        print(\"After TimeStamp Conversions and Removing Periods from column names\")\n        output_dyf.printSchema()\n\n        # Write Parquet files to S3\n        self.log.info(f\"Writing Parquet files to {s3_storage_path}...\")\n        self.glue_context.purge_s3_path(s3_storage_path, {\"retentionPeriod\": 0})\n        self.glue_context.write_dynamic_frame.from_options(\n            frame=output_dyf,\n            connection_type=\"s3\",\n            connection_options={\n                \"path\": s3_storage_path\n                # \"partitionKeys\": [\"year\", \"month\", \"day\"],\n            },\n            format=output_format,\n        )\n\n        # Set up our SageWorks metadata (description, tags, etc)\n        description = f\"SageWorks data source: {self.output_uuid}\"\n        sageworks_meta = {\"sageworks_tags\": \":\".join(tags)}\n        for key, value in self.output_meta.items():\n            sageworks_meta[key] = value\n\n        # Create a new table in the AWS Data Catalog\n        self.log.info(f\"Creating Data Catalog Table: {self.output_uuid}...\")\n\n        # Converting the Spark Types to Athena Types\n        def to_athena_type(col):\n            athena_type_map = {\"long\": \"bigint\"}\n            spark_type = col.dataType.typeName()\n            return athena_type_map.get(spark_type, spark_type)\n\n        column_name_types = [{\"Name\": col.name, \"Type\": to_athena_type(col)} for col in output_dyf.schema().fields]\n\n        # Our parameters for the Glue Data Catalog are different for Parquet and ORC\n        if output_format == \"parquet\":\n            glue_input_format = \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\"\n            glue_output_format = \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\"\n            serialization_library = \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\"\n        else:\n            glue_input_format = \"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\"\n            glue_output_format = \"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\"\n            serialization_library = \"org.apache.hadoop.hive.ql.io.orc.OrcSerde\"\n\n        table_input = {\n            \"Name\": self.output_uuid,\n            \"Description\": description,\n            \"Parameters\": sageworks_meta,\n            \"TableType\": \"EXTERNAL_TABLE\",\n            \"StorageDescriptor\": {\n                \"Columns\": column_name_types,\n                \"Location\": s3_storage_path,\n                \"InputFormat\": glue_input_format,\n                \"OutputFormat\": glue_output_format,\n                \"Compressed\": True,\n                \"SerdeInfo\": {\n                    \"SerializationLibrary\": serialization_library,\n                },\n            },\n        }\n\n        # Delete the Data Catalog Table if it already exists\n        glue_client = boto3.client(\"glue\")\n        try:\n            glue_client.delete_table(DatabaseName=\"sageworks\", Name=self.output_uuid)\n            self.log.info(f\"Deleting Data Catalog Table: {self.output_uuid}...\")\n        except ClientError as e:\n            if e.response[\"Error\"][\"Code\"] != \"EntityNotFoundException\":\n                raise e\n\n        self.log.info(f\"Creating Data Catalog Table: {self.output_uuid}...\")\n        glue_client.create_table(DatabaseName=\"sageworks\", TableInput=table_input)\n\n        # All done!\n        self.log.info(f\"{self.input_uuid} --&gt; {self.output_uuid} complete!\")\n</code></pre>"},{"location":"core_classes/transforms/data_loaders/#sageworks.core.transforms.data_loaders.heavy.s3_heavy_to_data_source.S3HeavyToDataSource.__init__","title":"<code>__init__(glue_context, input_uuid, output_uuid)</code>","text":"<p>S3HeavyToDataSource: Class to move HEAVY S3 Files into a SageWorks DataSource Args:     glue_context: GlueContext, AWS Glue Specific wrapper around SparkContext     input_uuid (str): The S3 Path to the files to be loaded     output_uuid (str): The UUID of the SageWorks DataSource to be created</p> Source code in <code>src/sageworks/core/transforms/data_loaders/heavy/s3_heavy_to_data_source.py</code> <pre><code>def __init__(self, glue_context: GlueContext, input_uuid: str, output_uuid: str):\n    \"\"\"S3HeavyToDataSource: Class to move HEAVY S3 Files into a SageWorks DataSource\n    Args:\n        glue_context: GlueContext, AWS Glue Specific wrapper around SparkContext\n        input_uuid (str): The S3 Path to the files to be loaded\n        output_uuid (str): The UUID of the SageWorks DataSource to be created\n    \"\"\"\n    self.log = glue_context.get_logger()\n\n    # FIXME: Pull these from Parameter Store or Config\n    self.input_uuid = input_uuid\n    self.output_uuid = output_uuid\n    self.output_meta = {\"sageworks_input\": self.input_uuid}\n    sageworks_bucket = \"s3://sandbox-sageworks-artifacts\"\n    self.data_sources_s3_path = sageworks_bucket + \"/data-sources\"\n\n    # Our Spark Context\n    self.glue_context = glue_context\n</code></pre>"},{"location":"core_classes/transforms/data_loaders/#sageworks.core.transforms.data_loaders.heavy.s3_heavy_to_data_source.S3HeavyToDataSource.remove_periods_from_column_names","title":"<code>remove_periods_from_column_names(dyf)</code>  <code>staticmethod</code>","text":"<p>Remove periods from column names in the DynamicFrame Args:     dyf (DynamicFrame): The DynamicFrame to convert Returns:     DynamicFrame: The converted DynamicFrame</p> Source code in <code>src/sageworks/core/transforms/data_loaders/heavy/s3_heavy_to_data_source.py</code> <pre><code>@staticmethod\ndef remove_periods_from_column_names(dyf: DynamicFrame) -&gt; DynamicFrame:\n    \"\"\"Remove periods from column names in the DynamicFrame\n    Args:\n        dyf (DynamicFrame): The DynamicFrame to convert\n    Returns:\n        DynamicFrame: The converted DynamicFrame\n    \"\"\"\n    # Extract the column names from the schema\n    old_column_names = [field.name for field in dyf.schema().fields]\n\n    # Create a new list of renamed column names\n    new_column_names = [name.replace(\".\", \"_\") for name in old_column_names]\n    print(old_column_names)\n    print(new_column_names)\n\n    # Create a new DynamicFrame with renamed columns\n    for c_old, c_new in zip(old_column_names, new_column_names):\n        dyf = dyf.rename_field(f\"`{c_old}`\", c_new)\n    return dyf\n</code></pre>"},{"location":"core_classes/transforms/data_loaders/#sageworks.core.transforms.data_loaders.heavy.s3_heavy_to_data_source.S3HeavyToDataSource.timestamp_conversions","title":"<code>timestamp_conversions(dyf, time_columns=[])</code>","text":"<p>Convert columns in the DynamicFrame to the correct data types Args:     dyf (DynamicFrame): The DynamicFrame to convert     time_columns (list): A list of column names to convert to timestamp Returns:     DynamicFrame: The converted DynamicFrame</p> Source code in <code>src/sageworks/core/transforms/data_loaders/heavy/s3_heavy_to_data_source.py</code> <pre><code>def timestamp_conversions(self, dyf: DynamicFrame, time_columns: list = []) -&gt; DynamicFrame:\n    \"\"\"Convert columns in the DynamicFrame to the correct data types\n    Args:\n        dyf (DynamicFrame): The DynamicFrame to convert\n        time_columns (list): A list of column names to convert to timestamp\n    Returns:\n        DynamicFrame: The converted DynamicFrame\n    \"\"\"\n\n    # Convert the timestamp columns to timestamp types\n    spark_df = dyf.toDF()\n    for column in time_columns:\n        spark_df = spark_df.withColumn(column, to_timestamp(col(column)))\n\n    # Convert the Spark DataFrame back to a Glue DynamicFrame and return\n    return DynamicFrame.fromDF(spark_df, self.glue_context, \"output_dyf\")\n</code></pre>"},{"location":"core_classes/transforms/data_loaders/#sageworks.core.transforms.data_loaders.heavy.s3_heavy_to_data_source.S3HeavyToDataSource.transform","title":"<code>transform(input_type='json', timestamp_columns=None, output_format='parquet')</code>","text":"<p>Convert the CSV or JSON data into Parquet Format in the SageWorks S3 Bucket, and store the information about the data to the AWS Data Catalog sageworks database Args:     input_type (str): The type of input files, either 'csv' or 'json'     timestamp_columns (list): A list of column names to convert to timestamp     output_format (str): The format of the output files, either 'parquet' or 'orc'</p> Source code in <code>src/sageworks/core/transforms/data_loaders/heavy/s3_heavy_to_data_source.py</code> <pre><code>def transform(\n    self,\n    input_type: str = \"json\",\n    timestamp_columns: list = None,\n    output_format: str = \"parquet\",\n):\n    \"\"\"Convert the CSV or JSON data into Parquet Format in the SageWorks S3 Bucket, and\n    store the information about the data to the AWS Data Catalog sageworks database\n    Args:\n        input_type (str): The type of input files, either 'csv' or 'json'\n        timestamp_columns (list): A list of column names to convert to timestamp\n        output_format (str): The format of the output files, either 'parquet' or 'orc'\n    \"\"\"\n\n    # Add some tags here\n    tags = [\"heavy\"]\n\n    # Create the Output Parquet file S3 Storage Path\n    s3_storage_path = f\"{self.data_sources_s3_path}/{self.output_uuid}\"\n\n    # Read JSONL files from S3 and infer schema dynamically\n    self.log.info(f\"Reading JSONL files from {self.input_uuid}...\")\n    input_dyf = self.glue_context.create_dynamic_frame.from_options(\n        connection_type=\"s3\",\n        connection_options={\n            \"paths\": [self.input_uuid],\n            \"recurse\": True,\n            \"gzip\": True,\n        },\n        format=input_type,\n        # format_options={'jsonPath': 'auto'}, Look into this later\n    )\n    self.log.info(\"Incoming DataFrame...\")\n    input_dyf.show(5)\n    input_dyf.printSchema()\n\n    # Resolve Choice fields\n    resolved_dyf = self.resolve_choice_fields(input_dyf)\n\n    # The next couple of lines of code is for un-nesting any nested JSON\n    # Create a Dynamic Frame Collection (dfc)\n    dfc = Relationalize.apply(resolved_dyf, name=\"root\")\n\n    # Aggregate the collection into a single dynamic frame\n    output_dyf = dfc.select(\"root\")\n\n    print(\"Before TimeStamp Conversions\")\n    output_dyf.printSchema()\n\n    # Convert any timestamp columns\n    output_dyf = self.timestamp_conversions(output_dyf, timestamp_columns)\n\n    # Relationalize will put periods in the column names. This will cause\n    # problems later when we try to create a FeatureSet from this DataSource\n    output_dyf = self.remove_periods_from_column_names(output_dyf)\n\n    print(\"After TimeStamp Conversions and Removing Periods from column names\")\n    output_dyf.printSchema()\n\n    # Write Parquet files to S3\n    self.log.info(f\"Writing Parquet files to {s3_storage_path}...\")\n    self.glue_context.purge_s3_path(s3_storage_path, {\"retentionPeriod\": 0})\n    self.glue_context.write_dynamic_frame.from_options(\n        frame=output_dyf,\n        connection_type=\"s3\",\n        connection_options={\n            \"path\": s3_storage_path\n            # \"partitionKeys\": [\"year\", \"month\", \"day\"],\n        },\n        format=output_format,\n    )\n\n    # Set up our SageWorks metadata (description, tags, etc)\n    description = f\"SageWorks data source: {self.output_uuid}\"\n    sageworks_meta = {\"sageworks_tags\": \":\".join(tags)}\n    for key, value in self.output_meta.items():\n        sageworks_meta[key] = value\n\n    # Create a new table in the AWS Data Catalog\n    self.log.info(f\"Creating Data Catalog Table: {self.output_uuid}...\")\n\n    # Converting the Spark Types to Athena Types\n    def to_athena_type(col):\n        athena_type_map = {\"long\": \"bigint\"}\n        spark_type = col.dataType.typeName()\n        return athena_type_map.get(spark_type, spark_type)\n\n    column_name_types = [{\"Name\": col.name, \"Type\": to_athena_type(col)} for col in output_dyf.schema().fields]\n\n    # Our parameters for the Glue Data Catalog are different for Parquet and ORC\n    if output_format == \"parquet\":\n        glue_input_format = \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\"\n        glue_output_format = \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\"\n        serialization_library = \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\"\n    else:\n        glue_input_format = \"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\"\n        glue_output_format = \"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\"\n        serialization_library = \"org.apache.hadoop.hive.ql.io.orc.OrcSerde\"\n\n    table_input = {\n        \"Name\": self.output_uuid,\n        \"Description\": description,\n        \"Parameters\": sageworks_meta,\n        \"TableType\": \"EXTERNAL_TABLE\",\n        \"StorageDescriptor\": {\n            \"Columns\": column_name_types,\n            \"Location\": s3_storage_path,\n            \"InputFormat\": glue_input_format,\n            \"OutputFormat\": glue_output_format,\n            \"Compressed\": True,\n            \"SerdeInfo\": {\n                \"SerializationLibrary\": serialization_library,\n            },\n        },\n    }\n\n    # Delete the Data Catalog Table if it already exists\n    glue_client = boto3.client(\"glue\")\n    try:\n        glue_client.delete_table(DatabaseName=\"sageworks\", Name=self.output_uuid)\n        self.log.info(f\"Deleting Data Catalog Table: {self.output_uuid}...\")\n    except ClientError as e:\n        if e.response[\"Error\"][\"Code\"] != \"EntityNotFoundException\":\n            raise e\n\n    self.log.info(f\"Creating Data Catalog Table: {self.output_uuid}...\")\n    glue_client.create_table(DatabaseName=\"sageworks\", TableInput=table_input)\n\n    # All done!\n    self.log.info(f\"{self.input_uuid} --&gt; {self.output_uuid} complete!\")\n</code></pre>"},{"location":"core_classes/transforms/data_to_features/","title":"Data To Features","text":"<p>DataToFeaturesLight: Base Class for Light DataSource to FeatureSet using Pandas</p> <p>RDKitDescriptors: Compute a Feature Set based on RDKit Descriptors</p>"},{"location":"core_classes/transforms/data_to_features/#sageworks.core.transforms.data_to_features.light.data_to_features_light.DataToFeaturesLight","title":"<code>DataToFeaturesLight</code>","text":"<p>             Bases: <code>Transform</code></p> <p>DataToFeaturesLight: Base Class for Light DataSource to FeatureSet using Pandas</p> Common Usage <p>to_features = DataToFeaturesLight(data_uuid, feature_uuid) to_features.set_output_tags([\"abalone\", \"public\", \"whatever\"]) to_features.transform(target_column=\"target\"/None, id_column=\"id\"/None,                       event_time_column=\"date\"/None, query=str/None)</p> Source code in <code>src/sageworks/core/transforms/data_to_features/light/data_to_features_light.py</code> <pre><code>class DataToFeaturesLight(Transform):\n    \"\"\"DataToFeaturesLight: Base Class for Light DataSource to FeatureSet using Pandas\n\n    Common Usage:\n        to_features = DataToFeaturesLight(data_uuid, feature_uuid)\n        to_features.set_output_tags([\"abalone\", \"public\", \"whatever\"])\n        to_features.transform(target_column=\"target\"/None, id_column=\"id\"/None,\n                              event_time_column=\"date\"/None, query=str/None)\n    \"\"\"\n\n    def __init__(self, data_uuid: str, feature_uuid: str):\n        \"\"\"DataToFeaturesLight Initialization\"\"\"\n\n        # Call superclass init\n        super().__init__(data_uuid, feature_uuid)\n\n        # Set up all my instance attributes\n        self.input_type = TransformInput.DATA_SOURCE\n        self.output_type = TransformOutput.FEATURE_SET\n        self.input_df = None\n        self.output_df = None\n\n    def pre_transform(self, query: str = None, **kwargs):\n        \"\"\"Pull the input DataSource into our Input Pandas DataFrame\n        Args:\n            query(str): Optional query to filter the input DataFrame\n        \"\"\"\n\n        # Grab the Input (Data Source)\n        data_to_pandas = DataToPandas(self.input_uuid)\n        data_to_pandas.transform(query=query)\n        self.input_df = data_to_pandas.get_output()\n\n    def transform_impl(self, **kwargs):\n        \"\"\"Transform the input DataFrame into a Feature Set\"\"\"\n\n        # This is a reference implementation that should be overridden by the subclass\n        self.output_df = self.input_df\n\n    def post_transform(\n        self, target_column=None, id_column=None, event_time_column=None, auto_categorize=True, **kwargs\n    ):\n        \"\"\"At this point the output DataFrame should be populated, so publish it as a Feature Set\n        Args:\n            target_column(str): The name of the target column in the output DataFrame (default: None)\n            id_column(str): The name of the id column in the output DataFrame (default: None)\n            event_time_column(str): The name of the event time column in the output DataFrame (default: None)\n            auto_categorize(bool): Whether to auto categorize the output DataFrame (default: True)\n        \"\"\"\n        # Now publish to the output location\n        output_features = PandasToFeatures(self.output_uuid, auto_categorize=auto_categorize)\n        output_features.set_input(\n            self.output_df, target_column=target_column, id_column=id_column, event_time_column=event_time_column\n        )\n        output_features.set_output_tags(self.output_tags)\n        output_features.add_output_meta(self.output_meta)\n        output_features.transform()\n\n        # Create a default training_view for this FeatureSet\n        fs = FeatureSetCore(self.output_uuid, force_refresh=True)\n        fs.create_default_training_view()\n</code></pre>"},{"location":"core_classes/transforms/data_to_features/#sageworks.core.transforms.data_to_features.light.data_to_features_light.DataToFeaturesLight.__init__","title":"<code>__init__(data_uuid, feature_uuid)</code>","text":"<p>DataToFeaturesLight Initialization</p> Source code in <code>src/sageworks/core/transforms/data_to_features/light/data_to_features_light.py</code> <pre><code>def __init__(self, data_uuid: str, feature_uuid: str):\n    \"\"\"DataToFeaturesLight Initialization\"\"\"\n\n    # Call superclass init\n    super().__init__(data_uuid, feature_uuid)\n\n    # Set up all my instance attributes\n    self.input_type = TransformInput.DATA_SOURCE\n    self.output_type = TransformOutput.FEATURE_SET\n    self.input_df = None\n    self.output_df = None\n</code></pre>"},{"location":"core_classes/transforms/data_to_features/#sageworks.core.transforms.data_to_features.light.data_to_features_light.DataToFeaturesLight.post_transform","title":"<code>post_transform(target_column=None, id_column=None, event_time_column=None, auto_categorize=True, **kwargs)</code>","text":"<p>At this point the output DataFrame should be populated, so publish it as a Feature Set Args:     target_column(str): The name of the target column in the output DataFrame (default: None)     id_column(str): The name of the id column in the output DataFrame (default: None)     event_time_column(str): The name of the event time column in the output DataFrame (default: None)     auto_categorize(bool): Whether to auto categorize the output DataFrame (default: True)</p> Source code in <code>src/sageworks/core/transforms/data_to_features/light/data_to_features_light.py</code> <pre><code>def post_transform(\n    self, target_column=None, id_column=None, event_time_column=None, auto_categorize=True, **kwargs\n):\n    \"\"\"At this point the output DataFrame should be populated, so publish it as a Feature Set\n    Args:\n        target_column(str): The name of the target column in the output DataFrame (default: None)\n        id_column(str): The name of the id column in the output DataFrame (default: None)\n        event_time_column(str): The name of the event time column in the output DataFrame (default: None)\n        auto_categorize(bool): Whether to auto categorize the output DataFrame (default: True)\n    \"\"\"\n    # Now publish to the output location\n    output_features = PandasToFeatures(self.output_uuid, auto_categorize=auto_categorize)\n    output_features.set_input(\n        self.output_df, target_column=target_column, id_column=id_column, event_time_column=event_time_column\n    )\n    output_features.set_output_tags(self.output_tags)\n    output_features.add_output_meta(self.output_meta)\n    output_features.transform()\n\n    # Create a default training_view for this FeatureSet\n    fs = FeatureSetCore(self.output_uuid, force_refresh=True)\n    fs.create_default_training_view()\n</code></pre>"},{"location":"core_classes/transforms/data_to_features/#sageworks.core.transforms.data_to_features.light.data_to_features_light.DataToFeaturesLight.pre_transform","title":"<code>pre_transform(query=None, **kwargs)</code>","text":"<p>Pull the input DataSource into our Input Pandas DataFrame Args:     query(str): Optional query to filter the input DataFrame</p> Source code in <code>src/sageworks/core/transforms/data_to_features/light/data_to_features_light.py</code> <pre><code>def pre_transform(self, query: str = None, **kwargs):\n    \"\"\"Pull the input DataSource into our Input Pandas DataFrame\n    Args:\n        query(str): Optional query to filter the input DataFrame\n    \"\"\"\n\n    # Grab the Input (Data Source)\n    data_to_pandas = DataToPandas(self.input_uuid)\n    data_to_pandas.transform(query=query)\n    self.input_df = data_to_pandas.get_output()\n</code></pre>"},{"location":"core_classes/transforms/data_to_features/#sageworks.core.transforms.data_to_features.light.data_to_features_light.DataToFeaturesLight.transform_impl","title":"<code>transform_impl(**kwargs)</code>","text":"<p>Transform the input DataFrame into a Feature Set</p> Source code in <code>src/sageworks/core/transforms/data_to_features/light/data_to_features_light.py</code> <pre><code>def transform_impl(self, **kwargs):\n    \"\"\"Transform the input DataFrame into a Feature Set\"\"\"\n\n    # This is a reference implementation that should be overridden by the subclass\n    self.output_df = self.input_df\n</code></pre>"},{"location":"core_classes/transforms/data_to_features/#sageworks.core.transforms.data_to_features.light.rdkit_descriptors.RDKitDescriptors","title":"<code>RDKitDescriptors</code>","text":"<p>             Bases: <code>DataToFeaturesLight</code></p> <p>RDKitDescriptors: Create a FeatureSet (RDKit Descriptors) from a DataSource</p> Common Usage <p>to_features = RDKitDescriptors(data_uuid, feature_uuid) to_features.set_output_tags([\"aqsol\", \"rdkit\", \"whatever\"]) to_features.transform()</p> Source code in <code>src/sageworks/core/transforms/data_to_features/light/rdkit_descriptors.py</code> <pre><code>class RDKitDescriptors(DataToFeaturesLight):\n    \"\"\"RDKitDescriptors: Create a FeatureSet (RDKit Descriptors) from a DataSource\n\n    Common Usage:\n        to_features = RDKitDescriptors(data_uuid, feature_uuid)\n        to_features.set_output_tags([\"aqsol\", \"rdkit\", \"whatever\"])\n        to_features.transform()\n    \"\"\"\n\n    def __init__(self, data_uuid: str, feature_uuid: str):\n        \"\"\"RDKitDescriptors Initialization\"\"\"\n\n        # Call superclass init\n        super().__init__(data_uuid, feature_uuid)\n\n        # Turn off warnings for RDKIT (revisit this)\n        RDLogger.DisableLog(\"rdApp.*\")\n\n    def transform_impl(self, **kwargs):\n        \"\"\"Compute a Feature Set based on RDKit Descriptors\"\"\"\n\n        # Check the input DataFrame has the required columns\n        if \"smiles\" not in self.input_df.columns:\n            raise ValueError(\"Input DataFrame must have a 'smiles' column\")\n\n        # Compute/add all the RDKIT Descriptors\n        self.output_df = self.compute_rdkit_descriptors(self.input_df)\n\n        # Drop any NaNs (and INFs)\n        self.output_df = pandas_utils.drop_nans(self.output_df, how=\"all\")\n\n    def compute_rdkit_descriptors(self, process_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Compute and add all the RDKit Descriptors\n        Args:\n            process_df(pd.DataFrame): The DataFrame to process and generate RDKit Descriptors\n        Returns:\n            pd.DataFrame: The input DataFrame with all the RDKit Descriptors added\n        \"\"\"\n        self.log.important(\"Computing RDKit Descriptors...\")\n\n        # Conversion to Molecules\n        molecules = [Chem.MolFromSmiles(smile) for smile in process_df[\"smiles\"]]\n\n        # Now get all the RDKIT Descriptors\n        # all_descriptors = [x[0] for x in Descriptors._descList]\n\n        # There's an overflow issue that happens with the IPC descriptor, so we'll remove it\n        # See: https://github.com/rdkit/rdkit/issues/1527\n        # if \"Ipc\" in all_descriptors:\n        #    all_descriptors.remove(\"Ipc\")\n\n        # Get the descriptors that are most useful for Solubility\n        best_descriptors = [\n            \"MolLogP\",\n            \"MolWt\",\n            \"TPSA\",\n            \"NumHDonors\",\n            \"NumHAcceptors\",\n            \"NumRotatableBonds\",\n            \"NumAromaticRings\",\n            \"NumSaturatedRings\",\n            \"NumAliphaticRings\",\n            \"NumAromaticCarbocycles\",\n        ]\n        best_20_descriptors = best_descriptors + [\n            \"HeavyAtomCount\",\n            \"RingCount\",\n            \"Chi0\",\n            \"Chi1\",\n            \"Kappa1\",\n            \"Kappa2\",\n            \"Kappa3\",\n            \"LabuteASA\",\n            \"FractionCSP3\",\n            \"HallKierAlpha\",\n        ]\n        best_30_descriptors = best_20_descriptors + [\n            \"SMR_VSA1\",\n            \"SlogP_VSA1\",\n            \"EState_VSA1\",\n            \"VSA_EState1\",\n            \"PEOE_VSA1\",\n            \"NumValenceElectrons\",\n            \"NumRadicalElectrons\",\n            \"MaxPartialCharge\",\n            \"MinPartialCharge\",\n            \"MaxAbsPartialCharge\",\n        ]\n        best_40_descriptors = best_30_descriptors + [\n            \"MolMR\",\n            \"ExactMolWt\",\n            \"NOCount\",\n            \"NumHeteroatoms\",\n            \"NumAmideBonds\",\n            \"FpDensityMorgan1\",\n            \"FpDensityMorgan2\",\n            \"FpDensityMorgan3\",\n            \"MaxEStateIndex\",\n            \"MinEStateIndex\",\n        ]\n\n        # Super useful Molecular Descriptor Calculator Class\n        calc = MoleculeDescriptors.MolecularDescriptorCalculator(best_40_descriptors)\n        column_names = calc.GetDescriptorNames()\n\n        descriptor_values = [calc.CalcDescriptors(m) for m in molecules]\n        df_features = pd.DataFrame(descriptor_values, columns=column_names)\n        return pd.concat([process_df, df_features], axis=1)\n</code></pre>"},{"location":"core_classes/transforms/data_to_features/#sageworks.core.transforms.data_to_features.light.rdkit_descriptors.RDKitDescriptors.__init__","title":"<code>__init__(data_uuid, feature_uuid)</code>","text":"<p>RDKitDescriptors Initialization</p> Source code in <code>src/sageworks/core/transforms/data_to_features/light/rdkit_descriptors.py</code> <pre><code>def __init__(self, data_uuid: str, feature_uuid: str):\n    \"\"\"RDKitDescriptors Initialization\"\"\"\n\n    # Call superclass init\n    super().__init__(data_uuid, feature_uuid)\n\n    # Turn off warnings for RDKIT (revisit this)\n    RDLogger.DisableLog(\"rdApp.*\")\n</code></pre>"},{"location":"core_classes/transforms/data_to_features/#sageworks.core.transforms.data_to_features.light.rdkit_descriptors.RDKitDescriptors.compute_rdkit_descriptors","title":"<code>compute_rdkit_descriptors(process_df)</code>","text":"<p>Compute and add all the RDKit Descriptors Args:     process_df(pd.DataFrame): The DataFrame to process and generate RDKit Descriptors Returns:     pd.DataFrame: The input DataFrame with all the RDKit Descriptors added</p> Source code in <code>src/sageworks/core/transforms/data_to_features/light/rdkit_descriptors.py</code> <pre><code>def compute_rdkit_descriptors(self, process_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Compute and add all the RDKit Descriptors\n    Args:\n        process_df(pd.DataFrame): The DataFrame to process and generate RDKit Descriptors\n    Returns:\n        pd.DataFrame: The input DataFrame with all the RDKit Descriptors added\n    \"\"\"\n    self.log.important(\"Computing RDKit Descriptors...\")\n\n    # Conversion to Molecules\n    molecules = [Chem.MolFromSmiles(smile) for smile in process_df[\"smiles\"]]\n\n    # Now get all the RDKIT Descriptors\n    # all_descriptors = [x[0] for x in Descriptors._descList]\n\n    # There's an overflow issue that happens with the IPC descriptor, so we'll remove it\n    # See: https://github.com/rdkit/rdkit/issues/1527\n    # if \"Ipc\" in all_descriptors:\n    #    all_descriptors.remove(\"Ipc\")\n\n    # Get the descriptors that are most useful for Solubility\n    best_descriptors = [\n        \"MolLogP\",\n        \"MolWt\",\n        \"TPSA\",\n        \"NumHDonors\",\n        \"NumHAcceptors\",\n        \"NumRotatableBonds\",\n        \"NumAromaticRings\",\n        \"NumSaturatedRings\",\n        \"NumAliphaticRings\",\n        \"NumAromaticCarbocycles\",\n    ]\n    best_20_descriptors = best_descriptors + [\n        \"HeavyAtomCount\",\n        \"RingCount\",\n        \"Chi0\",\n        \"Chi1\",\n        \"Kappa1\",\n        \"Kappa2\",\n        \"Kappa3\",\n        \"LabuteASA\",\n        \"FractionCSP3\",\n        \"HallKierAlpha\",\n    ]\n    best_30_descriptors = best_20_descriptors + [\n        \"SMR_VSA1\",\n        \"SlogP_VSA1\",\n        \"EState_VSA1\",\n        \"VSA_EState1\",\n        \"PEOE_VSA1\",\n        \"NumValenceElectrons\",\n        \"NumRadicalElectrons\",\n        \"MaxPartialCharge\",\n        \"MinPartialCharge\",\n        \"MaxAbsPartialCharge\",\n    ]\n    best_40_descriptors = best_30_descriptors + [\n        \"MolMR\",\n        \"ExactMolWt\",\n        \"NOCount\",\n        \"NumHeteroatoms\",\n        \"NumAmideBonds\",\n        \"FpDensityMorgan1\",\n        \"FpDensityMorgan2\",\n        \"FpDensityMorgan3\",\n        \"MaxEStateIndex\",\n        \"MinEStateIndex\",\n    ]\n\n    # Super useful Molecular Descriptor Calculator Class\n    calc = MoleculeDescriptors.MolecularDescriptorCalculator(best_40_descriptors)\n    column_names = calc.GetDescriptorNames()\n\n    descriptor_values = [calc.CalcDescriptors(m) for m in molecules]\n    df_features = pd.DataFrame(descriptor_values, columns=column_names)\n    return pd.concat([process_df, df_features], axis=1)\n</code></pre>"},{"location":"core_classes/transforms/data_to_features/#sageworks.core.transforms.data_to_features.light.rdkit_descriptors.RDKitDescriptors.transform_impl","title":"<code>transform_impl(**kwargs)</code>","text":"<p>Compute a Feature Set based on RDKit Descriptors</p> Source code in <code>src/sageworks/core/transforms/data_to_features/light/rdkit_descriptors.py</code> <pre><code>def transform_impl(self, **kwargs):\n    \"\"\"Compute a Feature Set based on RDKit Descriptors\"\"\"\n\n    # Check the input DataFrame has the required columns\n    if \"smiles\" not in self.input_df.columns:\n        raise ValueError(\"Input DataFrame must have a 'smiles' column\")\n\n    # Compute/add all the RDKIT Descriptors\n    self.output_df = self.compute_rdkit_descriptors(self.input_df)\n\n    # Drop any NaNs (and INFs)\n    self.output_df = pandas_utils.drop_nans(self.output_df, how=\"all\")\n</code></pre>"},{"location":"core_classes/transforms/features_to_model/","title":"Features To Model","text":""},{"location":"core_classes/transforms/model_to_endpoint/","title":"Model to Endpoint","text":""},{"location":"core_classes/transforms/overview/","title":"Transforms","text":"<p>SageWorks currently has a large set of Transforms that go from one Artifact type to another (e.g. DataSource to FeatureSet). The Transforms will often have light and heavy versions depending on the scale of data that needs to be transformed.</p>"},{"location":"core_classes/transforms/overview/#transform-types","title":"Transform Types","text":"<ul> <li>Data Loaders<ul> <li>Light</li> <li>Heavy</li> </ul> </li> <li>Data to Features<ul> <li>Light</li> <li>Heavy</li> </ul> </li> <li>Features to Model</li> <li>Model to Endpoint</li> </ul>"},{"location":"core_classes/transforms/overview/#pandas-transforms","title":"Pandas Transforms","text":"<p>There's also a large set of Transforms that will use Pandas Dataframes. These are obviously light transforms but they do come in quite handle when working with smaller data.</p> <p>Want to do some custom processing? use Pandas Transforms!</p> <p>At any point you can grab a DataSource or FeatureSet and pull the data as a dataframe. Now run some processing and then save it back to SageWorks</p>"},{"location":"core_classes/transforms/pandas_transforms/","title":"Pandas Transforms","text":""},{"location":"misc/sageworks_classes_concepts/","title":"SageWorks Classes and Concepts","text":"<p>A flexible, rapid, and customizable AWS\u00ae ML Sandbox. Here's some of the classes and concepts we use in the SageWorks system:</p> <p></p> <ul> <li>Artifacts</li> <li>DataLoader</li> <li>DataSource</li> <li>FeatureSet</li> <li>Model</li> <li> <p>Endpoint</p> </li> <li> <p>Transforms</p> </li> <li>DataSource to DataSource<ul> <li>Heavy <ul> <li>AWS Glue Jobs</li> <li>AWS EMR Serverless</li> </ul> </li> <li>Light<ul> <li>Local/Laptop</li> <li>Lambdas</li> <li>StepFunctions</li> </ul> </li> </ul> </li> <li>DataSource to FeatureSet<ul> <li>Heavy/Light (see above breakout)</li> </ul> </li> <li>FeatureSet to FeatureSet<ul> <li>Heavy/Light (see above breakout)</li> </ul> </li> <li>FeatureSet to Model</li> <li>Model to Endpoint</li> </ul>"},{"location":"misc/scp_consulting/","title":"Scp consulting","text":""},{"location":"misc/scp_consulting/#consulting","title":"Consulting","text":""},{"location":"misc/scp_consulting/#sageworks-scp-consulting-awesome","title":"SageWorks + SCP Consulting = Awesome","text":"<p>Our experienced team can provide development and consulting services to help you effectively use Amazon\u2019s Machine Learning services within your organization.</p> <p>The popularity of cloud based Machine Learning services is booming. The problem many companies face is how that capability gets effectively used and harnessed to drive real business decisions and provide concrete value for their organization.</p> <p>Using SageWorks will minimizize the time and manpower needed to incorporate AWS ML into your organization. If your company would like to be a SageWorks Alpha Tester, contact us at sageworks@supercowpowers.com.</p>"},{"location":"misc/scp_consulting/#typical-engagements","title":"Typical Engagements","text":"<p>SageWorks clients typically want a tailored web_interface that helps to drive business decisions and provides value for their organization.</p> <ul> <li>SageWorks components provide a set of classes and transforms the will dramatically reduce time and increase productivity when building AWS ML Systems.</li> <li>SageWorks enables rapid prototyping via it's light paths and provides AWS production workflows on large scale data through it's heavy paths.</li> <li> <p>Rapid Prototyping is typically done via these steps.</p> </li> <li> <p>Quick Construction of Web Interface (tailored)</p> </li> <li>Custom Components (tailored)</li> <li>Demo/Review the Application with the Client</li> <li>Get Feedback/Changes/Improvements</li> <li> <p>Goto Step 1</p> </li> <li> <p>When the client is happy/excited about the ProtoType we then bolt down the system, test the heavy paths, review AWS access, security and ensure 'least privileged' roles and policies.</p> </li> </ul> <p>Contact us for a free initial consultation on how we can accelerate the use of AWS ML at your company sageworks@supercowpowers.com.</p>"}]}